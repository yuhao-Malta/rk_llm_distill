# src/train_distill_amp.py
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast  # ğŸ‘ˆ AMP æ ¸å¿ƒæ¨¡å—
from tqdm import tqdm
import time

# å‡è®¾ TinyTransformer å·²å®šä¹‰åœ¨ models/tiny_transformer.py
from models.tiny_transformer import TinyTransformer


# -----------------------------
# 1. è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼ˆåŠ è½½æ•™å¸ˆè½¯æ ‡ç­¾ï¼‰
# -----------------------------
class TranslationDataset(Dataset):
    def __init__(self, file_path, max_samples=None):
        self.data = []
        with open(file_path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                item = json.loads(line)
                self.data.append(item)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            "id": item["id"],
            "src": item["src"],
            "ref": item["ref"],
            "hyp": item["hyp"],  # æ•™å¸ˆæ¨¡å‹çš„è½¯æ ‡ç­¾ï¼ˆæ–‡æœ¬ï¼‰
            "task_id": item["task_id"],  # 0=ä¸­â†’è‹±, 1=è‹±â†’ä¸­
            "timestamp": item["timestamp"]
        }


# -----------------------------
# 2. KL æ•£åº¦æŸå¤±å‡½æ•°ï¼ˆç”¨äºçŸ¥è¯†è’¸é¦ï¼‰
# -----------------------------
def kl_div_loss(student_logits, teacher_logits, temperature=2.0):
    """
    çŸ¥è¯†è’¸é¦ KL æ•£åº¦æŸå¤±
    :param student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º (batch_size, seq_len, vocab_size)
    :param teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º (batch_size, seq_len, vocab_size)
    :param temperature: æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è½¯æ ‡ç­¾å¹³æ»‘åº¦
    :return: KL æ•£åº¦æŸå¤±
    """

    seq_len_s = student_logits.size(1)
    seq_len_t = teacher_logits.size(1)

    # âœ… åŒæ­¥é•¿åº¦ï¼šæˆªæ–­åˆ°è¾ƒçŸ­çš„åºåˆ—é•¿åº¦
    min_len = min(seq_len_s, seq_len_t)
    if seq_len_s != seq_len_t:
        print(f"âš ï¸  åºåˆ—é•¿åº¦ä¸åŒ¹é… ({seq_len_s} vs {seq_len_t})ï¼Œè‡ªåŠ¨åŒæ­¥åˆ° {min_len}")
        student_logits = student_logits[:, :min_len, :]
        teacher_logits = teacher_logits[:, :min_len, :]

    # è®¡ç®— KL æ•£åº¦
    s_dist = nn.functional.log_softmax(student_logits / temperature, dim=-1)
    t_dist = nn.functional.softmax(teacher_logits / temperature, dim=-1)
    return nn.functional.kl_div(s_dist, t_dist, reduction='batchmean') * (temperature ** 2)


# -----------------------------
# 3. è’¸é¦è®­ç»ƒä¸»å‡½æ•°ï¼ˆæ”¯æŒ AMP + å¤§ Batch + æ¢¯åº¦ç´¯ç§¯ï¼‰
# -----------------------------
def train_distill_amp(
        teacher_logits_dir="data/teacher_logits",
        output_model_dir="outputs/models",
        epochs=3,
        batch_size=8,  # ğŸ‘ˆ å¯è°ƒæ•´ï¼ˆé«˜ç«¯PCå¯è®¾ä¸º64/128ï¼‰
        gradient_accumulation_steps=4,  # ğŸ‘ˆ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿå¤§ Batchï¼‰
        learning_rate=3e-4,
        max_samples_per_task=None,
        device=None  # ğŸ‘ˆ è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
):
    """
    å¤šä»»åŠ¡è’¸é¦è®­ç»ƒï¼ˆä¸­â†’è‹± + è‹±â†’ä¸­ï¼‰ï¼Œæ”¯æŒ AMP + å¤§ Batch + æ¢¯åº¦ç´¯ç§¯
    """
    print("ğŸš€ å¼€å§‹å¤šä»»åŠ¡è’¸é¦è®­ç»ƒ (AMP + å¤§ Batch)...")

    # 1. è®¾ç½®è®¾å¤‡
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“¦ ä½¿ç”¨è®¾å¤‡: {device}")

    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_model_dir, exist_ok=True)

    # 3. åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹
    model = TinyTransformer().to(device)
    tokenizer = model.tokenizer  # å¤ç”¨ Qwen Tokenizer

    # 4. åŠ è½½æ•°æ®é›†
    zh_to_en_path = os.path.join(teacher_logits_dir, "zh_to_en.jsonl")
    en_to_zh_path = os.path.join(teacher_logits_dir, "en_to_zh.jsonl")

    if not os.path.exists(zh_to_en_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ä¸­â†’è‹±æ•°æ®: {zh_to_en_path}")
    if not os.path.exists(en_to_zh_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°è‹±â†’ä¸­æ•°æ®: {en_to_zh_path}")

    dataset_zh_en = TranslationDataset(zh_to_en_path, max_samples=max_samples_per_task)
    dataset_en_zh = TranslationDataset(en_to_zh_path, max_samples=max_samples_per_task)

    loader_zh_en = DataLoader(dataset_zh_en, batch_size=batch_size, shuffle=True, num_workers=2)  # ğŸ‘ˆ å¤šçº¿ç¨‹åŠ é€Ÿ
    loader_en_zh = DataLoader(dataset_en_zh, batch_size=batch_size, shuffle=True, num_workers=2)

    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  - ä¸­â†’è‹±æ ·æœ¬æ•°: {len(dataset_zh_en)}")
    print(f"  - è‹±â†’ä¸­æ ·æœ¬æ•°: {len(dataset_en_zh)}")

    # 5. åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€AMP ç¼©æ”¾å™¨
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
    scaler = GradScaler()  # ğŸ‘ˆ AMP æ ¸å¿ƒï¼šæ¢¯åº¦ç¼©æ”¾å™¨

    # 6. è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(epochs):
        print(f"\nğŸ“… Epoch {epoch + 1}/{epochs}")
        total_loss = 0.0
        total_batches = 0

        # --- ä¸­â†’è‹±è®­ç»ƒ ---
        print("ğŸ§  è®­ç»ƒä¸­â†’è‹±ä»»åŠ¡...")
        pbar_zh_en = tqdm(loader_zh_en, desc="ä¸­â†’è‹±", leave=False)
        for step, batch in enumerate(pbar_zh_en):
            # ç¼–ç è¾“å…¥
            encoded = tokenizer(batch["src"], return_tensors="pt", padding=True, truncation=True, max_length=512)
            input_ids = encoded.input_ids.to(device)
            attention_mask = encoded.attention_mask.to(device)
            task_ids = torch.tensor([0] * input_ids.size(0), device=device)  # task_id=0 for zhâ†’en

            # æ¨¡æ‹Ÿæ•™å¸ˆ logitsï¼ˆå®é™…åº”ä» batch["hyp"] ç”Ÿæˆï¼Œæ­¤å¤„ä¸ºç¤ºæ„ï¼‰
            # TODO: ä» batch["hyp"] ç¼–ç ç”ŸæˆçœŸå® teacher_logits
            teacher_logits = torch.randn(input_ids.size(0), input_ids.size(1), model.vocab_size).to(device)

            # âœ… AMP ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with autocast():  # ğŸ‘ˆ è‡ªåŠ¨æ··åˆç²¾åº¦
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, task_id=task_ids)
                student_logits = outputs["logits"]
                # ... åœ¨è®¡ç®— loss å‰ ...
                print(f"ğŸ” student_logits.shape: {student_logits.shape}")
                print(f"ğŸ” teacher_logits.shape: {teacher_logits.shape}")
                loss = kl_div_loss(student_logits, teacher_logits)
                loss = loss / gradient_accumulation_steps  # ğŸ‘ˆ æ¢¯åº¦ç´¯ç§¯

            # åå‘ä¼ æ’­ï¼ˆAMP ç¼©æ”¾ï¼‰
            scaler.scale(loss).backward()

            # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°åˆ°è¾¾åï¼Œæ›´æ–°æƒé‡
            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(pbar_zh_en):
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            total_loss += loss.item() * gradient_accumulation_steps
            total_batches += 1
            pbar_zh_en.set_postfix({"loss": f"{loss.item() * gradient_accumulation_steps:.4f}"})

        # --- è‹±â†’ä¸­è®­ç»ƒ ---
        print("ğŸ§  è®­ç»ƒè‹±â†’ä¸­ä»»åŠ¡...")
        pbar_en_zh = tqdm(loader_en_zh, desc="è‹±â†’ä¸­", leave=False)
        for step, batch in enumerate(pbar_en_zh):
            # ç¼–ç è¾“å…¥
            encoded = tokenizer(batch["src"], return_tensors="pt", padding=True, truncation=True, max_length=512)
            input_ids = encoded.input_ids.to(device)
            attention_mask = encoded.attention_mask.to(device)
            task_ids = torch.tensor([1] * input_ids.size(0), device=device)  # task_id=1 for enâ†’zh

            # æ¨¡æ‹Ÿæ•™å¸ˆ logits
            teacher_logits = torch.randn(input_ids.size(0), input_ids.size(1), model.vocab_size).to(device)

            # âœ… AMP ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, task_id=task_ids)
                student_logits = outputs["logits"]
                # ... åœ¨è®¡ç®— loss å‰ ...
                print(f"ğŸ” student_logits.shape: {student_logits.shape}")
                print(f"ğŸ” teacher_logits.shape: {teacher_logits.shape}")
                loss = kl_div_loss(student_logits, teacher_logits)
                loss = loss / gradient_accumulation_steps

            # åå‘ä¼ æ’­ï¼ˆAMP ç¼©æ”¾ï¼‰
            scaler.scale(loss).backward()

            # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°åˆ°è¾¾åï¼Œæ›´æ–°æƒé‡
            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(pbar_en_zh):
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            total_loss += loss.item() * gradient_accumulation_steps
            total_batches += 1
            pbar_en_zh.set_postfix({"loss": f"{loss.item() * gradient_accumulation_steps:.4f}"})

        avg_loss = total_loss / total_batches if total_batches > 0 else 0
        print(f"âœ… Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}")

        # 7. ä¿å­˜æ¨¡å‹ï¼ˆæ¯ä¸ª epochï¼‰
        model_path = os.path.join(output_model_dir, f"student_model_amp_epoch_{epoch + 1}.pth")
        torch.save(model.state_dict(), model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    print("\nğŸ‰ è’¸é¦è®­ç»ƒå®Œæˆï¼")


# -----------------------------
# 4. ä¸»ç¨‹åºå…¥å£
# -----------------------------
if __name__ == "__main__":
    train_distill_amp(
        teacher_logits_dir="../scripts/data/teacher_logits",
        output_model_dir="../outputs/models",
        epochs=3,
        batch_size=8,  # ğŸ‘ˆ æ™®é€šPCå»ºè®®8ï¼Œé«˜ç«¯PCå¯è®¾ä¸º64/128
        gradient_accumulation_steps=4,  # ğŸ‘ˆ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate=3e-4,
        max_samples_per_task=100  # ğŸ‘ˆ è°ƒè¯•ç”¨ï¼Œå¯è®¾ä¸º None
    )