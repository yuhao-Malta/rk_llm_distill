import sys
import os

from torch import GradScaler

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast  # æ›´æ–°ä¸º torch.amp.autocast
from tqdm import tqdm
import logging
from models.tiny_transformer import TinyTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/train_distill_amp.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "qwen_tokenizer_offline", "qwen", "Qwen1___5-1___8B")
CACHE_DIR = os.path.join(PROJECT_ROOT, "data", "teacher_logits", "cache")
os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------
# 1. è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼ˆæ”¯æŒ.ptå’Œ.jsonlï¼‰
# -----------------------------
class TranslationDataset(Dataset):
    def __init__(self, file_path, max_samples=None, is_jsonl=True, max_seq_len=64, tokenizer=None):
        self.file_path = file_path
        self.max_samples = max_samples
        self.is_jsonl = is_jsonl
        self.max_seq_len = max_seq_len
        self.tokenizer = tokenizer
        self.data = []

        if is_jsonl:
            logging.info(f"åŠ è½½å¹¶åˆ†è¯ {file_path}")
            with open(file_path, "r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    if max_samples and i >= max_samples:
                        break
                    item = json.loads(line)
                    src_encoded = tokenizer(item["src"], return_tensors="pt", truncation=True, max_length=max_seq_len)
                    tgt_encoded = tokenizer(item["hyp"], return_tensors="pt", truncation=True, max_length=max_seq_len)
                    self.data.append({
                        "src_input_ids": src_encoded.input_ids.squeeze(0).to(dtype=torch.long),
                        "src_attention_mask": src_encoded.attention_mask.squeeze(0).to(dtype=torch.long),
                        "tgt_input_ids": tgt_encoded.input_ids.squeeze(0).to(dtype=torch.long),
                        "tgt_attention_mask": tgt_encoded.attention_mask.squeeze(0).to(dtype=torch.long),
                        "task_id": item["task_id"]
                    })
        else:
            logging.info(f"åŠ è½½ .pt æ–‡ä»¶: {file_path}")
            data = torch.load(file_path)
            self.data = data[:max_samples] if max_samples else data
            for item in self.data:
                assert "logits" in item, f"ç¼ºå°‘ teacher_logits: {item}"
                assert item["logits"].shape == (max_seq_len, 151936), f"æ— æ•ˆ logits å½¢çŠ¶: {item['logits'].shape}"
                assert item.get("src_input_ids", item.get("input_ids")) is not None, f"ç¼ºå°‘ src_input_ids æˆ– input_ids: {item}"
                assert item.get("tgt_input_ids", item.get("hyp_input_ids", item.get("input_ids"))) is not None, f"ç¼ºå°‘ tgt_input_ids, hyp_input_ids æˆ– input_ids: {item}"
                # ç¡®ä¿æ•°æ®ç±»å‹
                item["src_input_ids"] = item.get("src_input_ids", item.get("input_ids")).to(dtype=torch.long)
                item["src_attention_mask"] = item.get("src_attention_mask", item.get("attention_mask")).to(dtype=torch.long)
                item["tgt_input_ids"] = item.get("tgt_input_ids", item.get("hyp_input_ids", item.get("input_ids"))).to(dtype=torch.long)
                item["tgt_attention_mask"] = item.get("tgt_attention_mask", item.get("hyp_attention_mask", item.get("attention_mask"))).to(dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# -----------------------------
# 2. è‡ªå®šä¹‰ collate_fnï¼ˆæ‰¹æ¬¡å¡«å……ï¼‰
# -----------------------------
def custom_collate_fn(batch, max_seq_len=64, pad_token_id=151643):
    src_input_ids = [item.get("src_input_ids", item.get("input_ids")) for item in batch]
    src_attention_mask = [item.get("src_attention_mask", item.get("attention_mask")) for item in batch]
    tgt_input_ids = [item.get("tgt_input_ids", item.get("hyp_input_ids", item.get("input_ids"))) for item in batch]
    tgt_attention_mask = [item.get("tgt_attention_mask", item.get("hyp_attention_mask", item.get("attention_mask"))) for item in batch]
    task_ids = [item["task_id"] for item in batch]
    teacher_logits = [item.get("logits") for item in batch] if not any("logits" not in item for item in batch) else None

    if any(x is None for x in src_input_ids + src_attention_mask + tgt_input_ids + tgt_attention_mask):
        raise KeyError("ç¼ºå°‘å¿…è¦çš„è¾“å…¥å­—æ®µï¼šsrc_input_ids, src_attention_mask, tgt_input_ids, æˆ– tgt_attention_mask")

    src_input_ids = torch.nn.utils.rnn.pad_sequence(
        src_input_ids, batch_first=True, padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)
    src_attention_mask = torch.nn.utils.rnn.pad_sequence(
        src_attention_mask, batch_first=True, padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)
    tgt_input_ids = torch.nn.utils.rnn.pad_sequence(
        tgt_input_ids, batch_first=True, padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)
    tgt_attention_mask = torch.nn.utils.rnn.pad_sequence(
        tgt_attention_mask, batch_first=True, padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)
    task_ids = torch.tensor(task_ids, dtype=torch.long)

    batch_dict = {
        "src_input_ids": src_input_ids,
        "src_attention_mask": src_attention_mask,
        "tgt_input_ids": tgt_input_ids,
        "tgt_attention_mask": tgt_attention_mask,
        "task_id": task_ids
    }
    if teacher_logits:
        batch_dict["teacher_logits"] = torch.stack(teacher_logits)
    return batch_dict

# -----------------------------
# 3. KL æ•£åº¦æŸå¤±å‡½æ•°
# -----------------------------
def kl_div_loss(student_logits, teacher_logits, temperature=2.0, vocab_size=151936):
    seq_len_s = student_logits.size(1)
    seq_len_t = teacher_logits.size(1)
    min_len = min(seq_len_s, seq_len_t)
    if seq_len_s != seq_len_t:
        logging.warning(f"âš ï¸ åºåˆ—é•¿åº¦ä¸åŒ¹é… ({seq_len_s} vs {seq_len_t})ï¼ŒåŒæ­¥åˆ° {min_len}")
        student_logits = student_logits[:, :min_len, :]
        teacher_logits = teacher_logits[:, :min_len, :]

    teacher_vocab_size = teacher_logits.size(-1)
    if teacher_vocab_size != vocab_size:
        logging.warning(f"âš ï¸ è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é… (æ•™å¸ˆ: {teacher_vocab_size} vs å­¦ç”Ÿ: {vocab_size})ï¼Œè£å‰ªåˆ° {vocab_size}")
        teacher_logits = teacher_logits[:, :, :vocab_size]

    s_dist = nn.functional.log_softmax(student_logits / temperature, dim=-1)
    t_dist = nn.functional.softmax(teacher_logits / temperature, dim=-1)
    return nn.functional.kl_div(s_dist, t_dist, reduction='batchmean') * (temperature ** 2)

# -----------------------------
# 4. é¢„è®¡ç®— teacher_logits
# -----------------------------
def precompute_teacher_logits(teacher_model, dataloader, cache_file, device, max_seq_len=64):
    if os.path.exists(cache_file):
        logging.info(f"âœ… æ‰¾åˆ°ç¼“å­˜: {cache_file}")
        return torch.load(cache_file)

    logging.info(f"ğŸ§  é¢„è®¡ç®— teacher_logitsï¼Œä¿å­˜è‡³: {cache_file}")
    teacher_model.eval()
    cached_data = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="é¢„è®¡ç®— teacher_logits"):
            input_ids = batch["tgt_input_ids"].to(device).to(dtype=torch.long)
            attention_mask = batch["tgt_attention_mask"].to(device).to(dtype=torch.long)
            try:
                logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits.cpu()
                for i in range(len(batch["src_input_ids"])):
                    cached_data.append({
                        "src_input_ids": batch["src_input_ids"][i].cpu().to(dtype=torch.long),
                        "src_attention_mask": batch["src_attention_mask"][i].cpu().to(dtype=torch.long),
                        "tgt_input_ids": batch["tgt_input_ids"][i].cpu().to(dtype=torch.long),
                        "tgt_attention_mask": batch["tgt_attention_mask"][i].cpu().to(dtype=torch.long),
                        "task_id": batch["task_id"][i].cpu(),
                        "logits": logits[i]
                    })
            except Exception as e:
                logging.error(f"âŒ é¢„è®¡ç®—å¤±è´¥: {e}")
                raise

    torch.save(cached_data, cache_file)
    logging.info(f"ğŸ’¾ teacher_logits å·²ç¼“å­˜è‡³: {cache_file}")
    return cached_data

# -----------------------------
# 5. è’¸é¦è®­ç»ƒä¸»å‡½æ•°
# -----------------------------
def train_distill_amp(
        teacher_logits_dir="data/teacher_logits",
        output_model_dir="outputs/models",
        epochs=3,
        batch_size=2,
        gradient_accumulation_steps=1,
        learning_rate=3e-4,
        max_samples_per_task=None,
        device=None,
        use_jsonl=False,
        teacher_model_path=MODEL_PATH,
        max_seq_len=64,
        compile=False,
        shard_idx=0,
        patience=2
):
    logging.info("ğŸš€ å¼€å§‹å¤šä»»åŠ¡è’¸é¦è®­ç»ƒ (AMP + å¤§ Batch)...")
    logging.info(f"ğŸ“¦ ä½¿ç”¨æ•°æ®æ ¼å¼: {'jsonl' if use_jsonl else 'pt'}")

    # 1. è®¾ç½®è®¾å¤‡
    if isinstance(device, str):
        device = torch.device(device.lower() if torch.cuda.is_available() and device.lower() == "cuda" else "cpu")
    elif device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"ğŸ“¦ ä½¿ç”¨è®¾å¤‡: {device}")

    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_model_dir, exist_ok=True)

    # 3. åŠ è½½ tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            teacher_model_path, local_files_only=True, trust_remote_code=True
        )
        config_path = os.path.join(teacher_model_path, "config.json")
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        vocab_size = config.get("vocab_size", len(tokenizer))
        logging.info(f"âœ… Qwen Tokenizer è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}ï¼Œæ¨¡å‹ vocab_size: {vocab_size}")
    except Exception as e:
        logging.error(f"âŒ åŠ è½½ Qwen Tokenizer æˆ– config.json å¤±è´¥: {e}")
        raise

    # 4. åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹
    try:
        model = TinyTransformer(
            vocab_size=vocab_size,
            max_seq_len=max_seq_len,
            d_model=128,
            nhead=4,
            num_layers=2,
            share_weights=True
        ).to(device)
        model.tokenizer = tokenizer
        if compile and device.type == "cuda":
            model = torch.compile(model)
        logging.info(f"âœ… å­¦ç”Ÿæ¨¡å‹ TinyTransformer åˆå§‹åŒ–å®Œæˆï¼Œå‚æ•°é‡: {model.num_parameters()/1e6:.1f}M")
    except Exception as e:
        logging.error(f"âŒ åˆå§‹åŒ– TinyTransformer å¤±è´¥: {e}")
        raise

    # 5. åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆä»…åœ¨ use_jsonl=True ä¸”æ— ç¼“å­˜æ—¶éœ€è¦ï¼‰
    teacher_model = None
    if use_jsonl:
        try:
            teacher_model = AutoModelForCausalLM.from_pretrained(
                teacher_model_path, device_map=device, torch_dtype=torch.float16,
                low_cpu_mem_usage=True, trust_remote_code=True
            ).eval()
            if compile and device.type == "cuda":
                teacher_model = torch.compile(teacher_model)
            logging.info(f"âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ: {teacher_model_path}")
        except Exception as e:
            logging.error(f"âŒ åŠ è½½æ•™å¸ˆæ¨¡å‹å¤±è´¥: {e}")
            raise

    # 6. åŠ è½½æ•°æ®é›†
    zh_to_en_path = os.path.join(teacher_logits_dir, f"zh_to_en_shard_{shard_idx}.{'jsonl' if use_jsonl else 'pt'}")
    en_to_zh_path = os.path.join(teacher_logits_dir, f"en_to_zh_shard_{shard_idx}.{'jsonl' if use_jsonl else 'pt'}")
    logging.info(f"åŠ è½½ä¸­â†’è‹±æ•°æ®: {zh_to_en_path}")
    logging.info(f"åŠ è½½è‹±â†’ä¸­æ•°æ®: {en_to_zh_path}")

    try:
        if not os.path.exists(zh_to_en_path):
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ä¸­â†’è‹±æ•°æ®: {zh_to_en_path}")
        if not os.path.exists(en_to_zh_path):
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°è‹±â†’ä¸­æ•°æ®: {en_to_zh_path}")
        dataset_zh_en = TranslationDataset(zh_to_en_path, max_samples=max_samples_per_task, is_jsonl=use_jsonl, max_seq_len=max_seq_len, tokenizer=tokenizer)
        dataset_en_zh = TranslationDataset(en_to_zh_path, max_samples=max_samples_per_task, is_jsonl=use_jsonl, max_seq_len=max_seq_len, tokenizer=tokenizer)
    except Exception as e:
        logging.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        raise

    # 7. é¢„è®¡ç®— teacher_logitsï¼ˆä»…åœ¨ use_jsonl=True æ—¶ï¼‰
    if use_jsonl:
        zh_to_en_cache = os.path.join(CACHE_DIR, f"zh_to_en_shard_{shard_idx}_cache.pt")
        en_to_zh_cache = os.path.join(CACHE_DIR, f"en_to_zh_shard_{shard_idx}_cache.pt")
        loader_zh_en = DataLoader(
            dataset_zh_en, batch_size=batch_size, shuffle=True, num_workers=0,
            pin_memory=(device.type == "cuda"), collate_fn=lambda batch: custom_collate_fn(batch, max_seq_len=max_seq_len, pad_token_id=tokenizer.pad_token_id)
        )
        loader_en_zh = DataLoader(
            dataset_en_zh, batch_size=batch_size, shuffle=True, num_workers=0,
            pin_memory=(device.type == "cuda"), collate_fn=lambda batch: custom_collate_fn(batch, max_seq_len=max_seq_len, pad_token_id=tokenizer.pad_token_id)
        )
        if teacher_model:
            dataset_zh_en.data = precompute_teacher_logits(teacher_model, loader_zh_en, zh_to_en_cache, device, max_seq_len)
            dataset_en_zh.data = precompute_teacher_logits(teacher_model, loader_en_zh, en_to_zh_cache, device, max_seq_len)
            loader_zh_en = DataLoader(
                dataset_zh_en, batch_size=batch_size, shuffle=True, num_workers=0,
                pin_memory=(device.type == "cuda"), collate_fn=lambda batch: custom_collate_fn(batch, max_seq_len=max_seq_len, pad_token_id=tokenizer.pad_token_id)
            )
            loader_en_zh = DataLoader(
                dataset_en_zh, batch_size=batch_size, shuffle=True, num_workers=0,
                pin_memory=(device.type == "cuda"), collate_fn=lambda batch: custom_collate_fn(batch, max_seq_len=max_seq_len, pad_token_id=tokenizer.pad_token_id)
            )
    else:
        loader_zh_en = DataLoader(
            dataset_zh_en, batch_size=batch_size, shuffle=True, num_workers=0,
            pin_memory=(device.type == "cuda"), collate_fn=lambda batch: custom_collate_fn(batch, max_seq_len=max_seq_len, pad_token_id=tokenizer.pad_token_id)
        )
        loader_en_zh = DataLoader(
            dataset_en_zh, batch_size=batch_size, shuffle=True, num_workers=0,
            pin_memory=(device.type == "cuda"), collate_fn=lambda batch: custom_collate_fn(batch, max_seq_len=max_seq_len, pad_token_id=tokenizer.pad_token_id)
        )

    logging.info(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    logging.info(f"  - ä¸­â†’è‹±æ ·æœ¬æ•°: {len(dataset_zh_en)}")
    logging.info(f"  - è‹±â†’ä¸­æ ·æœ¬æ•°: {len(dataset_en_zh)}")

    # 8. åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€AMP ç¼©æ”¾å™¨
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
    scaler = GradScaler(enabled=(device.type == "cuda"))

    # 9. è®­ç»ƒå¾ªç¯ï¼ˆå¸¦ early stoppingï¼‰
    model.train()
    best_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(epochs):
        logging.info(f"\nğŸ“… Epoch {epoch + 1}/{epochs}")
        total_loss = 0.0
        total_batches = 0

        # ä¸­â†’è‹±è®­ç»ƒ
        logging.info("ğŸ§  è®­ç»ƒä¸­â†’è‹±ä»»åŠ¡...")
        pbar_zh_en = tqdm(loader_zh_en, desc="ä¸­â†’è‹±", leave=False)
        for step, batch in enumerate(pbar_zh_en):
            try:
                input_ids = batch["src_input_ids"].to(device).to(dtype=torch.long)
                attention_mask = batch["src_attention_mask"].to(device).to(dtype=torch.long)
                task_ids = batch["task_id"].to(device)
                tgt_input_ids = batch["tgt_input_ids"].to(device).to(dtype=torch.long)
                tgt_attention_mask = batch["tgt_attention_mask"].to(device).to(dtype=torch.long)

                with torch.no_grad():
                    teacher_logits = batch.get("teacher_logits", None)
                    if teacher_logits is None and teacher_model:
                        teacher_logits = teacher_model(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask).logits
                    elif teacher_logits is None:
                        teacher_logits = model(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask, task_id=task_ids)["logits"]
                    else:
                        teacher_logits = teacher_logits.to(device)

                with autocast(device_type="cuda" if device.type == "cuda" else "cpu"):
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, task_id=task_ids)
                    student_logits = outputs["logits"]
                    logging.debug(f"ğŸ” student_logits.shape: {student_logits.shape}")
                    logging.debug(f"ğŸ” teacher_logits.shape: {teacher_logits.shape}")
                    loss = kl_div_loss(student_logits, teacher_logits, vocab_size=vocab_size)
                    loss = loss / gradient_accumulation_steps

                scaler.scale(loss).backward()
                if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(pbar_zh_en):
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()

                total_loss += loss.item() * gradient_accumulation_steps
                total_batches += 1
                pbar_zh_en.set_postfix({"loss": f"{loss.item() * gradient_accumulation_steps:.4f}"})
            except Exception as e:
                logging.error(f"âŒ ä¸­â†’è‹±è®­ç»ƒå¤±è´¥ (step {step}): {e}")
                raise

        # è‹±â†’ä¸­è®­ç»ƒ
        logging.info("ğŸ§  è®­ç»ƒè‹±â†’ä¸­ä»»åŠ¡...")
        pbar_en_zh = tqdm(loader_en_zh, desc="è‹±â†’ä¸­", leave=False)
        for step, batch in enumerate(pbar_en_zh):
            try:
                input_ids = batch["src_input_ids"].to(device).to(dtype=torch.long)
                attention_mask = batch["src_attention_mask"].to(device).to(dtype=torch.long)
                task_ids = batch["task_id"].to(device)
                tgt_input_ids = batch["tgt_input_ids"].to(device).to(dtype=torch.long)
                tgt_attention_mask = batch["tgt_attention_mask"].to(device).to(dtype=torch.long)

                with torch.no_grad():
                    teacher_logits = batch.get("teacher_logits", None)
                    if teacher_logits is None and teacher_model:
                        teacher_logits = teacher_model(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask).logits
                    elif teacher_logits is None:
                        teacher_logits = model(input_ids=tgt_input_ids, attention_mask=tgt_attention_mask, task_id=task_ids)["logits"]
                    else:
                        teacher_logits = teacher_logits.to(device)

                with autocast(device_type="cuda" if device.type == "cuda" else "cpu"):
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, task_id=task_ids)
                    student_logits = outputs["logits"]
                    logging.debug(f"ğŸ” student_logits.shape: {student_logits.shape}")
                    logging.debug(f"ğŸ” teacher_logits.shape: {teacher_logits.shape}")
                    loss = kl_div_loss(student_logits, teacher_logits, vocab_size=vocab_size)
                    loss = loss / gradient_accumulation_steps

                scaler.scale(loss).backward()
                if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(pbar_en_zh):
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()

                total_loss += loss.item() * gradient_accumulation_steps
                total_batches += 1
                pbar_en_zh.set_postfix({"loss": f"{loss.item() * gradient_accumulation_steps:.4f}"})
            except Exception as e:
                logging.error(f"âŒ è‹±â†’ä¸­è®­ç»ƒå¤±è´¥ (step {step}): {e}")
                raise

        avg_loss = total_loss / total_batches if total_batches > 0 else 0
        logging.info(f"âœ… Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}")

        # Early stopping
        if avg_loss < best_loss:
            best_loss = avg_loss
            epochs_no_improve = 0
            best_model_path = os.path.join(output_model_dir, f"student_model_amp_shard_{shard_idx}_best.pth")
            torch.save(model.state_dict(), best_model_path)
            logging.info(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                logging.info(f"ğŸ›‘ Early stopping åœ¨ epoch {epoch + 1} è§¦å‘")
                break

        # ä¿å­˜æ¯è½®æ¨¡å‹
        model_path = os.path.join(output_model_dir, f"student_model_amp_shard_{shard_idx}_epoch_{epoch + 1}.pth")
        torch.save(model.state_dict(), model_path)
        logging.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    logging.info("\nğŸ‰ è’¸é¦è®­ç»ƒå®Œæˆï¼")
    return best_model_path

# -----------------------------
# 6. ä¸»ç¨‹åºå…¥å£
# -----------------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="å¤šä»»åŠ¡è’¸é¦è®­ç»ƒ")
    parser.add_argument("--teacher_logits_dir", type=str, default="data/teacher_logits", help="æ•™å¸ˆlogitsç›®å½•")
    parser.add_argument("--output_model_dir", type=str, default="outputs/models", help="æ¨¡å‹è¾“å‡ºç›®å½•")
    parser.add_argument("--epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--learning_rate", type=float, default=3e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--max_samples_per_task", type=int, default=None, help="æ¯ä»»åŠ¡æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--device", type=str, default=None, help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--use_jsonl", action="store_true", help="ä½¿ç”¨jsonlæ ¼å¼ï¼ˆé»˜è®¤ä½¿ç”¨.ptï¼‰")
    parser.add_argument("--teacher_model_path", type=str, default=MODEL_PATH, help="æ•™å¸ˆæ¨¡å‹è·¯å¾„")
    parser.add_argument("--max_seq_len", type=int, default=64, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--compile", action="store_true", help="ä½¿ç”¨torch.compileåŠ é€Ÿ")
    parser.add_argument("--shard_idx", type=int, default=0, help="åˆ†ç‰‡ç´¢å¼•")
    parser.add_argument("--patience", type=int, default=2, help="Early stopping è€å¿ƒå€¼")
    args = parser.parse_args()

    train_distill_amp(
        teacher_logits_dir=args.teacher_logits_dir,
        output_model_dir=args.output_model_dir,
        epochs=args.epochs,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        max_samples_per_task=args.max_samples_per_task,
        device=args.device,
        use_jsonl=args.use_jsonl,
        teacher_model_path=args.teacher_model_path,
        max_seq_len=args.max_seq_len,
        compile=args.compile,
        shard_idx=args.shard_idx,
        patience=args.patience
    )