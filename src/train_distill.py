# src/train_distill.py
import os
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from models.tiny_transformer import TinyTransformer  # ğŸ‘ˆ ä½ çš„å­¦ç”Ÿæ¨¡å‹


# -----------------------------
# 1. è‡ªå®šä¹‰æ•°æ®é›†ç±»
# -----------------------------
class TranslationDataset(Dataset):
    """åŠ è½½æ•™å¸ˆè½¯æ ‡ç­¾æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼‰"""

    def __init__(self, file_path, max_samples=None):
        self.data = []
        with open(file_path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                item = json.loads(line)
                self.data.append(item)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            "id": item["id"],
            "src": item["src"],
            "ref": item["ref"],
            "hyp": item["hyp"],
            "task_id": item["task_id"],  # 0=ä¸­â†’è‹±, 1=è‹±â†’ä¸­
            "timestamp": item["timestamp"]
        }


# -----------------------------
# 2. KL æ•£åº¦æŸå¤±å‡½æ•°
# -----------------------------
def kl_div_loss(student_logits, teacher_logits, temperature=2.0):
    """
    çŸ¥è¯†è’¸é¦ KL æ•£åº¦æŸå¤±
    :param student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º (batch_size, seq_len, vocab_size)
    :param teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º (batch_size, seq_len, vocab_size)
    :param temperature: æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è½¯æ ‡ç­¾å¹³æ»‘åº¦
    :return: KL æ•£åº¦æŸå¤±
    """
    s_dist = nn.functional.log_softmax(student_logits / temperature, dim=-1)
    t_dist = nn.functional.softmax(teacher_logits / temperature, dim=-1)
    return nn.functional.kl_div(s_dist, t_dist, reduction='batchmean') * (temperature ** 2)


# -----------------------------
# 3. è’¸é¦è®­ç»ƒä¸»å‡½æ•°
# -----------------------------
def train_distill(
        teacher_logits_dir="../scripts/data/teacher_logits",
        output_model_dir="../outputs/models",
        epochs=3,
        batch_size=8,
        learning_rate=3e-4,
        max_samples_per_task=None  # é™åˆ¶æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
):
    """
    å¤šä»»åŠ¡è’¸é¦è®­ç»ƒï¼ˆä¸­â†’è‹± + è‹±â†’ä¸­ï¼‰
    """
    print("ğŸš€ å¼€å§‹å¤šä»»åŠ¡è’¸é¦è®­ç»ƒ...")

    # 1. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_model_dir, exist_ok=True)

    # 2. åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹
    model = TinyTransformer()
    tokenizer = model.tokenizer  # å¤ç”¨ Qwen Tokenizer

    # 3. åŠ è½½æ•°æ®é›†
    zh_to_en_path = os.path.join(teacher_logits_dir, "zh_to_en.jsonl")
    en_to_zh_path = os.path.join(teacher_logits_dir, "en_to_zh.jsonl")

    print(f"ğŸ” å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"ğŸ” å°è¯•åŠ è½½ä¸­â†’è‹±æ•°æ®: {os.path.abspath(zh_to_en_path)}")
    print(f"ğŸ” å°è¯•åŠ è½½è‹±â†’ä¸­æ•°æ®: {os.path.abspath(en_to_zh_path)}")

    if not os.path.exists(zh_to_en_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ä¸­â†’è‹±æ•°æ®: {zh_to_en_path}")
    if not os.path.exists(en_to_zh_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°è‹±â†’ä¸­æ•°æ®: {en_to_zh_path}")

    dataset_zh_en = TranslationDataset(zh_to_en_path, max_samples=max_samples_per_task)
    dataset_en_zh = TranslationDataset(en_to_zh_path, max_samples=max_samples_per_task)

    loader_zh_en = DataLoader(dataset_zh_en, batch_size=batch_size, shuffle=True)
    loader_en_zh = DataLoader(dataset_en_zh, batch_size=batch_size, shuffle=True)

    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  - ä¸­â†’è‹±æ ·æœ¬æ•°: {len(dataset_zh_en)}")
    print(f"  - è‹±â†’ä¸­æ ·æœ¬æ•°: {len(dataset_en_zh)}")

    # 4. åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    model.train()

    # 5. è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        print(f"\nğŸ“… Epoch {epoch + 1}/{epochs}")
        total_loss = 0.0
        total_batches = 0

        # --- ä¸­â†’è‹±è®­ç»ƒ ---
        print("ğŸ§  è®­ç»ƒä¸­â†’è‹±ä»»åŠ¡...")
        pbar_zh_en = tqdm(loader_zh_en, desc="ä¸­â†’è‹±", leave=False)
        for batch in pbar_zh_en:
            optimizer.zero_grad()

            # ä½¿ç”¨æ¨¡å‹çš„ tokenizer ç¼–ç è¾“å…¥
            encoded = model.tokenizer(
                batch["src"],
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=model.max_seq_len  # ğŸ‘ˆ ä¿æŒä¸€è‡´
            )
            input_ids = encoded.input_ids
            attention_mask = encoded.attention_mask
            # âœ… è°ƒè¯•ï¼šæ‰“å°åºåˆ—é•¿åº¦
            seq_len = input_ids.size(1)
            if seq_len > 64:
                print(f"ğŸš¨ è­¦å‘Šï¼šå‘ç°è¶…é•¿åºåˆ—ï¼é•¿åº¦={seq_len}ï¼Œå†…å®¹å‰50å­—ç¬¦: {batch['src'][0][:50]}...")

            # å‰å‘ä¼ æ’­ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰
            outputs = model(input_ids=input_ids, attention_mask=attention_mask,
                            task_id=torch.tensor([0] * input_ids.size(0)))  # task_id=0
            student_logits = outputs["logits"]

            # æ¨¡æ‹Ÿæ•™å¸ˆ logitsï¼ˆå®é™…åº”ä»æ–‡ä»¶åŠ è½½ï¼‰
            # TODO: ä» batch["hyp"] ç”ŸæˆçœŸå® teacher_logitsï¼ˆéœ€ tokenizer ç¼–ç ï¼‰
            # è¿™é‡Œä»…ä¸ºç¤ºæ„ï¼Œå®é™…åº”åŠ è½½çœŸå® teacher_logits
            teacher_logits = torch.randn_like(student_logits)  # ğŸ‘ˆ å ä½ç¬¦

            # è®¡ç®—æŸå¤±
            loss = kl_div_loss(student_logits, teacher_logits)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_batches += 1
            pbar_zh_en.set_postfix({"loss": f"{loss.item():.4f}"})

        # --- è‹±â†’ä¸­è®­ç»ƒ ---
        print("ğŸ§  è®­ç»ƒè‹±â†’ä¸­ä»»åŠ¡...")
        pbar_en_zh = tqdm(loader_en_zh, desc="è‹±â†’ä¸­", leave=False)
        for batch in pbar_en_zh:
            optimizer.zero_grad()

            # ç¼–ç è¾“å…¥
            encoded = tokenizer(batch["src"], return_tensors="pt", padding=True, truncation=True)
            input_ids = encoded.input_ids
            attention_mask = encoded.attention_mask
            # âœ… è°ƒè¯•ï¼šæ‰“å°åºåˆ—é•¿åº¦
            seq_len = input_ids.size(1)
            if seq_len > 64:
                print(f"ğŸš¨ è­¦å‘Šï¼šå‘ç°è¶…é•¿åºåˆ—ï¼é•¿åº¦={seq_len}ï¼Œå†…å®¹å‰50å­—ç¬¦: {batch['src'][0][:50]}...")

            # å‰å‘ä¼ æ’­ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰
            outputs = model(input_ids=input_ids, attention_mask=attention_mask,
                            task_id=torch.tensor([1] * input_ids.size(0)))  # task_id=1
            student_logits = outputs["logits"]

            # æ¨¡æ‹Ÿæ•™å¸ˆ logitsï¼ˆå®é™…åº”ä»æ–‡ä»¶åŠ è½½ï¼‰
            teacher_logits = torch.randn_like(student_logits)  # ğŸ‘ˆ å ä½ç¬¦

            # è®¡ç®—æŸå¤±
            loss = kl_div_loss(student_logits, teacher_logits)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_batches += 1
            pbar_en_zh.set_postfix({"loss": f"{loss.item():.4f}"})

        avg_loss = total_loss / total_batches if total_batches > 0 else 0
        print(f"âœ… Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}")

        # 6. ä¿å­˜æ¨¡å‹ï¼ˆæ¯ä¸ª epochï¼‰
        model_path = os.path.join(output_model_dir, f"student_model_epoch_{epoch + 1}.pth")
        torch.save(model.state_dict(), model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    print("\nğŸ‰ è’¸é¦è®­ç»ƒå®Œæˆï¼")


# -----------------------------
# 4. ä¸»ç¨‹åºå…¥å£
# -----------------------------
if __name__ == "__main__":
    train_distill(
        teacher_logits_dir="../scripts/data/teacher_logits",
        output_model_dir="../outputs/models",
        epochs=3,  # ğŸ‘ˆ å¯è°ƒæ•´
        batch_size=8,  # ğŸ‘ˆ æ ¹æ® GPU å†…å­˜è°ƒæ•´
        learning_rate=3e-4,
        max_samples_per_task=100  # ğŸ‘ˆ è°ƒè¯•æ—¶é™åˆ¶æ ·æœ¬æ•°
    )