# src/coordinate_distill.py (ä¿®æ­£ç‰ˆ)
import os
import argparse
import logging
import subprocess
import torch

# ===========================
# æ—¥å¿—é…ç½®
# ===========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/coordinate_distill.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
GENERATE_SCRIPT = os.path.join(PROJECT_ROOT, "scripts", "generate_logits_grok.py")
TRAIN_SCRIPT = os.path.join(PROJECT_ROOT, "src", "train_distill_amp_grok.py")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "teacher_logits")
OUTPUT_MODEL_DIR = os.path.join(PROJECT_ROOT, "outputs", "models")


def main(args):
    total_shards = (args.max_samples + args.shard_size - 1) // args.shard_size
    logging.info(f"ğŸš€ å¼€å§‹åè°ƒå¼è’¸é¦è®­ç»ƒï¼Œå…± {total_shards} ä¸ªåˆ†ç‰‡")
    logging.info(f"å…¨é‡æ ·æœ¬: {args.max_samples:,}  åˆ†ç‰‡å¤§å°: {args.shard_size:,}")

    for shard_idx in range(total_shards):
        start = shard_idx * args.shard_size
        end = min(start + args.shard_size, args.max_samples)
        logging.info(f"\n{'='*70}\nğŸ§© åˆ†ç‰‡ {shard_idx} [{start:,} - {end:,})\n{'='*70}")

        # --------------------------------------------------------
        # Step 1: ç”Ÿæˆ Teacher Logits
        # --------------------------------------------------------
        cmd_gen = [
            "python", GENERATE_SCRIPT,
            "--dataset_path", args.dataset_path,
            "--batch_size", str(args.batch_size),
            "--max_seq_len", str(args.max_seq_len),
            "--max_samples", str(args.max_samples),
            "--start_from", str(start),
            "--shard_size", str(args.shard_size),
            "--device", args.device,
            "--shard_idx", str(shard_idx),
        ]
        if args.compile:
            cmd_gen.append("--compile")
        if args.int8:
            cmd_gen.append("--int8")
        if args.simulate_quant_noise:
            cmd_gen.append("--simulate_quant_noise")
            cmd_gen += ["--noise_std", str(args.noise_std)]

        logging.info(f"âš™ï¸ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd_gen)}")
        ret = subprocess.call(cmd_gen)
        if ret != 0:
            logging.error(f"âŒ åˆ†ç‰‡ {shard_idx} Teacher Logits ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            continue
        logging.info(f"âœ… åˆ†ç‰‡ {shard_idx} logits ç”Ÿæˆå®Œæˆ")

        # --------------------------------------------------------
        # Step 2: è’¸é¦è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
        # --------------------------------------------------------
        cmd_train = [
            "python", TRAIN_SCRIPT,
            "--teacher_logits_dir", OUTPUT_DIR,
            "--output_model_dir", OUTPUT_MODEL_DIR,
            "--batch_size", str(args.batch_size),
            "--max_samples_per_task", str(args.shard_size),
            "--device", args.device,
            "--shard_idx", str(shard_idx),
        ]
        if args.compile:
            cmd_train.append("--compile")

        logging.info(f"âš™ï¸ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd_train)}")
        ret = subprocess.call(cmd_train)
        if ret != 0:
            logging.error(f"âŒ åˆ†ç‰‡ {shard_idx} å­¦ç”Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            continue
        logging.info(f"âœ… åˆ†ç‰‡ {shard_idx} è’¸é¦è®­ç»ƒå®Œæˆ")

        # --------------------------------------------------------
        # Step 3: æ¸…ç†åˆ†ç‰‡æ–‡ä»¶
        # --------------------------------------------------------
        for direction in ["zh_to_en", "en_to_zh"]:
            pt_file = os.path.join(OUTPUT_DIR, f"{direction}_shard_{shard_idx}.pt")
            if os.path.exists(pt_file):
                os.remove(pt_file)
                logging.info(f"ğŸ—‘ï¸ åˆ é™¤åˆ†ç‰‡æ–‡ä»¶: {pt_file}")

    logging.info("\nğŸ‰ å…¨é‡è’¸é¦è®­ç»ƒå®Œæˆï¼æ‰€æœ‰åˆ†ç‰‡å·²å¤„ç†ã€‚")
    logging.info(f"ğŸ“¦ æœ€ç»ˆå­¦ç”Ÿæ¨¡å‹ç›®å½•: {OUTPUT_MODEL_DIR}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åè°ƒåˆ†ç‰‡ç”Ÿæˆ + è®­ç»ƒ + æ¸…ç† (GPU ä¼˜åŒ–ç‰ˆ)")
    parser.add_argument("--dataset_path", type=str, default="data/raw/wmt19_zh_en", help="WMT19 æ•°æ®è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹å¤„ç†å¤§å°ï¼ˆGPU å¯è®¾å¤§ä¸€ç‚¹ï¼‰")
    parser.add_argument("--max_seq_len", type=int, default=64, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--max_samples", type=int, default=26000000, help="å…¨é‡æ ·æœ¬æ•°")
    parser.add_argument("--shard_size", type=int, default=100000, help="æ¯ä¸ªåˆ†ç‰‡çš„æ ·æœ¬æ•°")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--compile", action="store_true", help="ä½¿ç”¨ torch.compile")
    parser.add_argument("--int8", action="store_true", help="INT8 é‡åŒ–æ•™å¸ˆæ¨¡å‹")
    parser.add_argument("--simulate_quant_noise", action="store_true", help="æ¨¡æ‹Ÿé‡åŒ–å™ªå£°å¢å¼ºå­¦ç”Ÿé²æ£’æ€§")
    parser.add_argument("--noise_std", type=float, default=0.01, help="æ¨¡æ‹Ÿé‡åŒ–å™ªå£°æ ‡å‡†å·®")
    args = parser.parse_args()

    main(args)