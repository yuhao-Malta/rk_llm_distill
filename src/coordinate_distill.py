# src/coordinate_distill.py
import os
import argparse
import logging
import subprocess
import torch

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/coordinate_distill.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
GENERATE_SCRIPT = os.path.join(PROJECT_ROOT, "scripts", "generate_logits_grok.py")
TRAIN_SCRIPT = os.path.join(PROJECT_ROOT, "src", "train_distill_amp_grok.py")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "teacher_logits")
OUTPUT_MODEL_DIR = os.path.join(PROJECT_ROOT, "outputs", "models")


def main(args):
    logging.info("ğŸš€ å¼€å§‹åè°ƒç”Ÿæˆåˆ†ç‰‡ã€è®­ç»ƒã€åˆ é™¤å¾ªç¯...")
    logging.info(
        f"å…¨é‡æ ·æœ¬: {args.max_samples}, åˆ†ç‰‡å¤§å°: {args.shard_size}, æ€»åˆ†ç‰‡æ•°: {(args.max_samples + args.shard_size - 1) // args.shard_size}")

    for shard_idx in range((args.max_samples + args.shard_size - 1) // args.shard_size):
        start = shard_idx * args.shard_size
        end = min(start + args.shard_size, args.max_samples)
        logging.info(f"åˆ†ç‰‡ {shard_idx}: {start} åˆ° {end} æ¡")

        # ç”Ÿæˆåˆ†ç‰‡
        subprocess.call([
            "python", GENERATE_SCRIPT,
            "--dataset_path", args.dataset_path,
            "--batch_size", str(args.batch_size),
            "--max_seq_len", str(args.max_seq_len),
            "--max_samples", str(args.shard_size),
            "--start_from", str(start),
            "--device", args.device,
            "--compile" if args.compile else "",
            "--int8" if args.int8 else "",
            "--use_api" if args.use_api else "",
        ])
        logging.info(f"âœ… åˆ†ç‰‡ {shard_idx} ç”Ÿæˆå®Œæˆ")

        # è®­ç»ƒåˆ†ç‰‡
        subprocess.call([
            "python", TRAIN_SCRIPT,
            "--teacher_logits_dir", OUTPUT_DIR,
            "--batch_size", str(args.batch_size),
            "--use_jsonl", "True" if args.use_api else "False",
            "--device", args.device,
            "--compile" if args.compile else "",
            "--max_samples_per_task", str(args.shard_size)
        ])
        logging.info(f"âœ… åˆ†ç‰‡ {shard_idx} è®­ç»ƒå®Œæˆï¼Œæƒé‡ä¿å­˜åˆ° {OUTPUT_MODEL_DIR}")

        # åˆ é™¤åˆ†ç‰‡æ•°æ®
        for file in [
            os.path.join(OUTPUT_DIR,
                         f"zh_to_en_shard_{shard_idx}.jsonl" if args.use_api else f"zh_to_en_shard_{shard_idx}.pt"),
            os.path.join(OUTPUT_DIR,
                         f"en_to_zh_shard_{shard_idx}.jsonl" if args.use_api else f"en_to_zh_shard_{shard_idx}.pt")
        ]:
            if os.path.exists(file):
                os.remove(file)
                logging.info(f"ğŸ—‘ï¸ åˆ é™¤åˆ†ç‰‡æ–‡ä»¶: {file}")

    logging.info("ğŸ‰ å…¨é‡è’¸é¦å®Œæˆï¼")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åè°ƒåˆ†ç‰‡ç”Ÿæˆã€è®­ç»ƒã€åˆ é™¤")
    parser.add_argument("--dataset_path", type=str, default="data/raw/wmt19_zh_en", help="WMT19è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--max_seq_len", type=int, default=64, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--max_samples", type=int, default=26000000, help="å…¨é‡æ ·æœ¬æ•°")
    parser.add_argument("--shard_size", type=int, default=100000, help="æ¯ç‰‡æ ·æœ¬æ•°")
    parser.add_argument("--compile", action="store_true", help="ä½¿ç”¨torch.compile")
    parser.add_argument("--int8", action="store_true", help="ä½¿ç”¨INT8é‡åŒ–")
    parser.add_argument("--use_api", action="store", help="ä½¿ç”¨DashScope API")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu", help="è®¡ç®—è®¾å¤‡")
    args = parser.parse_args()

    main(args)
