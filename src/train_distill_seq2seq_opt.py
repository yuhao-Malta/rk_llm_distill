import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast, GradScaler
from transformers import AutoTokenizer
from tqdm import tqdm
import logging

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from models.tiny_seq2seq_transformer import TinySeq2SeqTransformer
from config.config import (
    ModelConfig, TrainingConfig, DataFormat, LogConfig,
    OPUS_MT_ZH_EN, OPUS_MT_EN_ZH,
    TEACHER_LOGITS_DIR, OUTPUT_MODEL_DIR
)

# ==================== æ—¥å¿— ====================
logging.basicConfig(
    level=logging.INFO,
    format=LogConfig.LOG_FORMAT,
    handlers=[
        logging.FileHandler(LogConfig.TRAIN_LOG, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# ==================== æ•°æ®é›†ç±» ====================
class TranslationDataset(Dataset):
    def __init__(self, file_path, max_samples=None):
        self.file_path = file_path
        data = torch.load(file_path)
        self.data = data[:max_samples] if max_samples else data

        if len(self.data) > 0:
            for key in DataFormat.REQUIRED_KEYS:
                if key not in self.data[0]:
                    raise KeyError(f"âŒ ç¼ºå°‘å­—æ®µ: {key}")

        logging.info(f"âœ… åŠ è½½æ•°æ®: {file_path} ({len(self.data)} samples)")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# ==================== collate ====================
def custom_collate_fn(batch, max_seq_len=64, pad_token_id=0):
    batch_dict = {
        "id": torch.tensor([item["id"] for item in batch], dtype=torch.long),
        "src_input_ids": torch.stack([item["src_input_ids"][:max_seq_len] for item in batch]),
        "src_attention_mask": torch.stack([item["src_attention_mask"][:max_seq_len] for item in batch]),
        "tgt_input_ids": torch.stack([item["tgt_input_ids"][:max_seq_len] for item in batch]),
        "tgt_attention_mask": torch.stack([item["tgt_attention_mask"][:max_seq_len] for item in batch]),
        "task_id": torch.tensor([item["task_id"] for item in batch], dtype=torch.long)
    }

    if "logits" in batch[0] and batch[0]["logits"] is not None:
        batch_dict["logits"] = torch.stack([
            item["logits"][:max_seq_len, :ModelConfig.VOCAB_SIZE] for item in batch
        ])

    return batch_dict

# ==================== KL Loss ====================
def kl_div_loss(student_logits, teacher_logits, temperature=2.0):
    s_dist = nn.functional.log_softmax(student_logits / temperature, dim=-1)
    t_dist = nn.functional.softmax(teacher_logits / temperature, dim=-1)

    if torch.isnan(s_dist).any() or torch.isnan(t_dist).any():
        logging.warning("âš ï¸ æ£€æµ‹åˆ° NaNï¼Œè·³è¿‡ batch")
        return torch.tensor(0.0, device=student_logits.device, requires_grad=True)

    loss = nn.functional.kl_div(s_dist, t_dist, reduction='batchmean') * (temperature ** 2)
    return loss

# ==================== ä¸»è®­ç»ƒ ====================
def train_distill_seq2seq(
    teacher_logits_dir=TEACHER_LOGITS_DIR,
    output_model_dir=OUTPUT_MODEL_DIR,
    epochs=None,
    batch_size=None,
    gradient_accumulation_steps=None,
    learning_rate=None,
    max_samples_per_task=None,
    device=None,
    max_seq_len=None,
    compile=None,
    shard_idx=0,
    patience=None
):
    epochs = epochs or TrainingConfig.EPOCHS
    batch_size = batch_size or TrainingConfig.BATCH_SIZE
    gradient_accumulation_steps = gradient_accumulation_steps or TrainingConfig.GRADIENT_ACCUMULATION_STEPS
    learning_rate = learning_rate or TrainingConfig.LEARNING_RATE
    patience = patience or TrainingConfig.PATIENCE
    max_seq_len = max_seq_len or ModelConfig.MAX_SEQ_LEN

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device)

    logging.info("=" * 60)
    logging.info("ğŸš€ å¯åŠ¨ Seq2Seq è’¸é¦è®­ç»ƒ (Opus-MT æ•™å¸ˆ)")
    logging.info("=" * 60)

    # ==== åˆ†åˆ«åŠ è½½ä¸¤ä¸ªæ–¹å‘çš„ tokenizer ====
    tokenizer_zh_en = AutoTokenizer.from_pretrained(OPUS_MT_ZH_EN, local_files_only=True)
    tokenizer_en_zh = AutoTokenizer.from_pretrained(OPUS_MT_EN_ZH, local_files_only=True)
    pad_id_zh_en = tokenizer_zh_en.pad_token_id or 0
    pad_id_en_zh = tokenizer_en_zh.pad_token_id or 0

    vocab_size_zh_en = len(tokenizer_zh_en)
    vocab_size_en_zh = len(tokenizer_en_zh)

    if vocab_size_zh_en != vocab_size_en_zh:
        logging.warning(f"âš ï¸ åŒå‘ vocab_size ä¸ä¸€è‡´: zhâ†’en={vocab_size_zh_en}, enâ†’zh={vocab_size_en_zh}ï¼Œå–æœ€å¤§å€¼")
    vocab_size = max(vocab_size_zh_en, vocab_size_en_zh)

    logging.info(f"ğŸ§© ä½¿ç”¨æ•™å¸ˆè¯è¡¨å¤§å°: {vocab_size}")

    logging.info("ğŸ§  æ­£åœ¨åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹ (TinySeq2SeqTransformer)...")

    try:
        model = TinySeq2SeqTransformer(
            teacher_model_path_zh2en=OPUS_MT_ZH_EN,
            teacher_model_path_en2zh=OPUS_MT_EN_ZH,
            d_model=ModelConfig.CURRENT_CONFIG.get("d_model", 128),
            nhead=ModelConfig.CURRENT_CONFIG.get("nhead", 4),
            num_encoder_layers=ModelConfig.CURRENT_CONFIG.get("num_encoder_layers", 2),
            num_decoder_layers=ModelConfig.CURRENT_CONFIG.get("num_decoder_layers", 2),
            dim_feedforward=ModelConfig.CURRENT_CONFIG.get("dim_feedforward", 256),
            dropout=ModelConfig.CURRENT_CONFIG.get("dropout", 0.1),
            max_seq_len=args.max_seq_len,
            share_weights=True,  # âœ… ä¿ç•™æƒé‡å…±äº«
        ).to(device)

        total_params = sum(p.numel() for p in model.parameters())
        # logging.info(f"âœ… å­¦ç”Ÿæ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        logging.info(f"âœ… TinySeq2SeqTransformer åˆå§‹åŒ–æˆåŠŸï¼Œå­¦ç”Ÿæ¨¡å‹å‚æ•°æ€»æ•°:  {total_params / 1e6:.2f}M")

    except Exception as e:
        logging.error(f"âŒ å­¦ç”Ÿæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise
    # ===== Sanity check =====
    if hasattr(model, "zh2en_tokenizer") and model.zh2en_tokenizer:
        logging.info(f"æ•™å¸ˆ zhâ†’en è¯è¡¨: {len(model.zh2en_tokenizer)}")
    if hasattr(model, "en2zh_tokenizer") and model.en2zh_tokenizer:
        logging.info(f"æ•™å¸ˆ enâ†’zh è¯è¡¨: {len(model.en2zh_tokenizer)}")
    logging.info(f"å­¦ç”Ÿæ¨¡å‹è¯è¡¨: {model.vocab_size}")

    if compile and device.type == "cuda":
        model = torch.compile(model)

    # ==== æ•°æ®è·¯å¾„ ====
    zh_to_en_path = os.path.join(teacher_logits_dir, f"zh_to_en_shard_{shard_idx}.pt")
    en_to_zh_path = os.path.join(teacher_logits_dir, f"en_to_zh_shard_{shard_idx}.pt")

    loader_zh_en = DataLoader(
        TranslationDataset(zh_to_en_path, max_samples=max_samples_per_task),
        batch_size=batch_size, shuffle=True, num_workers=0,
        collate_fn=lambda b: custom_collate_fn(b, max_seq_len=max_seq_len, pad_token_id=pad_id_zh_en)
    )
    loader_en_zh = DataLoader(
        TranslationDataset(en_to_zh_path, max_samples=max_samples_per_task),
        batch_size=batch_size, shuffle=True, num_workers=0,
        collate_fn=lambda b: custom_collate_fn(b, max_seq_len=max_seq_len, pad_token_id=pad_id_en_zh)
    )

    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
    scaler = GradScaler(device.type, enabled=(device.type == "cuda" and TrainingConfig.USE_AMP))

    ce_loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id_zh_en)  # âœ… ä½¿ç”¨ Opus pad_token_id

    model.train()
    best_loss = float("inf")
    epochs_no_improve = 0

    for epoch in range(epochs):
        logging.info(f"\n{'=' * 60}\nğŸ“… Epoch {epoch + 1}/{epochs}\n{'=' * 60}")
        total_loss = 0.0
        total_batches = 0

        for task_name, loader in [("ä¸­â†’è‹±", loader_zh_en), ("è‹±â†’ä¸­", loader_en_zh)]:
            logging.info(f"ğŸ§  è®­ç»ƒä»»åŠ¡: {task_name}")
            pbar = tqdm(loader, desc=task_name)

            for step, batch in enumerate(pbar):
                try:
                    src_input_ids = batch["src_input_ids"].to(device)
                    src_attention_mask = batch["src_attention_mask"].to(device)
                    tgt_input_ids = batch["tgt_input_ids"].to(device)
                    tgt_attention_mask = batch["tgt_attention_mask"].to(device)
                    task_ids = batch["task_id"].to(device)

                    teacher_logits = batch.get("logits", None)
                    if teacher_logits is not None:
                        teacher_logits = teacher_logits.to(device)

                    with autocast(device_type=device.type, enabled=TrainingConfig.USE_AMP):
                        outputs = model(
                            src_ids=src_input_ids,
                            tgt_ids=tgt_input_ids,
                            task_id=task_ids,
                            src_padding_mask=(1 - src_attention_mask).bool(),
                            tgt_padding_mask=(1 - tgt_attention_mask).bool()
                        )

                        student_logits = outputs["logits"]

                        # -------------------------
                        # CE éƒ¨åˆ†ï¼ˆteacher forcingï¼‰
                        # -------------------------
                        ce_student_logits = student_logits[:, :-1].contiguous()
                        ce_labels = tgt_input_ids[:, 1:].contiguous()

                        ce_loss = ce_loss_fn(
                            ce_student_logits.reshape(-1, ce_student_logits.size(-1)),
                            ce_labels.reshape(-1)
                        )

                        # -------------------------
                        # KL éƒ¨åˆ†ï¼ˆè’¸é¦ï¼‰
                        # -------------------------
                        if teacher_logits is not None:
                            # å¯¹é½é•¿åº¦
                            min_len = min(student_logits.size(1), teacher_logits.size(1))
                            kl_loss = kl_div_loss(
                                student_logits[:, :min_len, :],
                                teacher_logits[:, :min_len, :],
                                temperature=TrainingConfig.TEMPERATURE
                            )
                        else:
                            kl_loss = torch.tensor(0.0, device=device)

                        # -------------------------
                        # æ–¹æ¡ˆ B: 0.5 CE + 0.5 KL
                        # -------------------------
                        loss = 0.5 * ce_loss + 0.5 * kl_loss
                        loss = loss / gradient_accumulation_steps

                    scaler.scale(loss).backward()

                    # ========== æ¢¯åº¦ç´¯ç§¯ ==========
                    if (step + 1) % gradient_accumulation_steps == 0:
                        scaler.step(optimizer)
                        scaler.update()
                        optimizer.zero_grad()

                    total_loss += loss.item() * gradient_accumulation_steps
                    total_batches += 1
                    pbar.set_postfix({"loss": f"{loss.item() * gradient_accumulation_steps:.4f}"})

                except Exception as e:
                    logging.error(f"âŒ {task_name} step {step} å¤±è´¥: {e}")
                    continue

        avg_loss = total_loss / max(total_batches, 1)
        logging.info(f"âœ… Epoch {epoch + 1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

        # ä¿å­˜ checkpoint
        model_path = os.path.join(output_model_dir, f"student_seq2seq_shard_{shard_idx}_epoch_{epoch + 1}.pth")
        torch.save(model.state_dict(), model_path)

        # åˆ¤æ–­æœ€ä½³
        if avg_loss < best_loss:
            best_loss = avg_loss
            epochs_no_improve = 0
            best_model_path = os.path.join(output_model_dir, f"student_seq2seq_shard_{shard_idx}_best.pth")
            torch.save(model.state_dict(), best_model_path)
            logging.info(f"ğŸ† æ›´æ–°æœ€ä½³æ¨¡å‹: {best_model_path}")
        else:
            epochs_no_improve += 1
            os.remove(model_path)
            logging.info(f"ğŸ—‘ï¸ åˆ é™¤éæœ€ä½³æ¨¡å‹: {model_path}")

            if epochs_no_improve >= patience:
                logging.info(f"ğŸ›‘ Early Stopping (Epoch {epoch + 1})")
                break

    logging.info("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    return best_model_path


# ================================
# CLI
# ================================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="TinySeq2Seq è’¸é¦è®­ç»ƒ")
    parser.add_argument("--teacher_logits_dir", type=str, default=TEACHER_LOGITS_DIR)
    parser.add_argument("--output_model_dir", type=str, default=OUTPUT_MODEL_DIR)
    parser.add_argument("--epochs", type=int, default=None)
    parser.add_argument("--batch_size", type=int, default=None)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=None)
    parser.add_argument("--learning_rate", type=float, default=None)
    parser.add_argument("--max_samples_per_task", type=int, default=None)
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--max_seq_len", type=int, default=None)
    parser.add_argument("--compile", action="store_true")
    parser.add_argument("--shard_idx", type=int, default=0)
    parser.add_argument("--patience", type=int, default=None)
    args = parser.parse_args()

    train_distill_seq2seq(
        teacher_logits_dir=args.teacher_logits_dir,
        output_model_dir=args.output_model_dir,
        epochs=args.epochs,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        max_samples_per_task=args.max_samples_per_task,
        device=args.device,
        max_seq_len=args.max_seq_len,
        compile=args.compile,
        shard_idx=args.shard_idx,
        patience=args.patience
    )