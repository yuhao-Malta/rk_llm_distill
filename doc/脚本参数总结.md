# 脚本参数总结

以下是项目中四个主要脚本的参数总结。每个参数包括含义、类型、默认值、CPU 设置建议和 GPU 设置建议。

## generate_logits_grok.py（生成教师标签）

作用：从 WMT19 数据集生成 teacher logits 或翻译文本，保存为 .pt 或 .jsonl 文件。
参数列表：

--dataset_path (type: str, default: "data/raw/wmt19_zh_en")：WMT19 数据集路径（包含 .parquet 文件）。

CPU 设置：默认值，确认路径存在。
GPU 设置：默认值，路径需 GPU 服务器可用。


--batch_size (type: int, default: 1)：批处理大小。

CPU 设置: 1-4（避免内存溢出）。
GPU 设置: 8-32（利用 GPU 并行）。


--max_seq_len (type: int, default: 64)：最大序列长度。

CPU/GPU 设置：64（适配端侧短序列）。


--max_samples (type: int, default: None)：最大样本数（None=全量）。

CPU 设置: 100-1000（小数据集测试）。
GPU 设置: 100,000-1,000,000（大规模）。


--start_from (type: int, default: 0)：开始样本索引。

CPU/GPU 设置：0（从头开始）。


--shard_size (type: int, default: 100000)：每片样本数（本地模式）。

CPU 设置: 100-500（减少内存压力）。
GPU 设置: 100,000（高效处理）。


--compile (action="store_true", default: False)：使用 torch.compile 加速。

CPU 设置: False（CPU 不支持）。
GPU 设置: True（加速 ~20-30%）。


--int8 (action="store_true", default: False)：使用 INT8 量化（CPU）。

CPU 设置: True（减少内存，加速推理）。
GPU 设置: False（GPU 优先 FP16）。


--debug (action="store_true", default: False)：调试模式（1条样本）。

CPU/GPU 设置: True（测试时）。


--device (type: str, default: "cuda" if torch.cuda.is_available() else "cpu")：计算设备。

CPU 设置: "cpu"。
GPU 设置: "cuda"（自动检测）。


--use_api (action="store_true", default: False)：使用 DashScope API（生成 .jsonl）。

CPU/GPU 设置: False（使用本地 QWen 模型）。




示例命令：

CPU：
bashpython scripts/generate_logits_grok.py --dataset_path data/raw/wmt19_zh_en --batch_size 1 --max_seq_len 64 --max_samples 100 --shard_size 100 --device cpu --int8

GPU：
bashpython scripts/generate_logits_grok.py --dataset_path data/raw/wmt19_zh_en --batch_size 16 --max_seq_len 64 --max_samples 100000 --shard_size 100000 --device cuda --compile

## train_distill_amp_grok.py（蒸馏训练）

作用：使用 teacher logits 训练 TinyTransformer，生成权重文件。
参数列表：

--teacher_logits_dir (type: str, default: "data/teacher_logits")：teacher logits 目录。

CPU/GPU 设置: 默认值。


--output_model_dir (type: str, default: "outputs/models")：模型输出目录。

CPU/GPU 设置: 默认值。


--epochs (type: int, default: 3)：训练轮数。

CPU/GPU 设置: 3（小数据集）。


--batch_size (type: int, default: 2)：批处理大小。

CPU 设置: 2-4（内存限制）。
GPU 设置: 8-32（加速）。


--gradient_accumulation_steps (type: int, default: 1)：梯度累积步数。

CPU/GPU 设置: 1（减少开销）。


--learning_rate (type: float, default: 3e-4)：学习率。

CPU/GPU 设置: 默认值。


--max_samples_per_task (type: int, default: None)：每任务最大样本数。

CPU 设置: 100-1000（小数据集测试）。
GPU 设置: 100,000（大规模）。


--device (type: str, default: None)：计算设备。

CPU 设置: "cpu"。
GPU 设置: "cuda"。


--use_jsonl (action="store_true", default: False)：使用 .jsonl 格式（默认 .pt）。

CPU/GPU 设置: False（使用 .pt）。


--teacher_model_path (type: str, default: MODEL_PATH)：教师模型路径。

CPU/GPU 设置: 默认值。


--max_seq_len (type: int, default: 64)：最大序列长度。

CPU/GPU 设置: 64（端侧优化）。


--compile (action="store_true", default: False)：使用 torch.compile 加速。

CPU 设置: False。
GPU 设置: True。


--shard_idx (type: int, default: 0)：分片索引。

CPU/GPU 设置: 0（单个分片）。


--patience (type: int, default: 2)：early stopping 耐心值。

CPU/GPU 设置: 2（损失不下降则停止）。




示例命令：

CPU：
bashpython src/train_distill_amp_grok.py --teacher_logits_dir data/teacher_logits --batch_size 2 --device cpu --max_samples_per_task 100 --shard_idx 0 --max_seq_len 64 --gradient_accumulation_steps 1 --patience 2

GPU：
bashpython src/train_distill_amp_grok.py --teacher_logits_dir data/teacher_logits --batch_size 16 --device cuda --max_samples_per_task 100000 --shard_idx 0 --max_seq_len 64 --gradient_accumulation_steps 1 --patience 2 --compile

## quantize_model.py（量化脚本）

作用：将 FP32 模型量化为 INT8，生成 student_model_int8.pth。
参数列表：

无命令行参数（硬编码模型路径）。


示例命令：

CPU/GPU：直接运行：
bashpython scripts/quantize_model.py

## coordinate_distill.py（协调脚本）

作用：协调分片生成 logits 和训练，适合大规模数据集（26,000,000 条）。
参数列表：

--dataset_path (type: str, default: "data/raw/wmt19_zh_en")：WMT19 路径。

CPU/GPU 设置: 默认值。


--batch_size (type: int, default: 8)：批处理大小。

CPU 设置: 1-4。
GPU 设置: 16-32。


--max_seq_len (type: int, default: 64)：最大序列长度。

CPU/GPU 设置: 64。


--max_samples (type: int, default: 26000000)：全量样本数。

CPU 设置: 100-1000（测试）。
GPU 设置: 26,000,000（全量）。


--shard_size (type: int, default: 100000)：每片样本数。

CPU 设置: 100-500。
GPU 设置: 100,000。


--compile (action="store_true", default: False)：使用 torch.compile。

CPU 设置: False。
GPU 设置: True。


--int8 (action="store_true", default: False)：使用 INT8 量化。

CPU 设置: True（加速）。
GPU 设置: False（FP16 更快）。


--use_api (action="store_true", default: False)：使用 DashScope API。

CPU/GPU 设置: False（使用本地 QWen）。

--device (type: str, default: "cuda" if torch.cuda.is_available() else "cpu")：计算设备。

CPU 设置: "cpu"。
GPU 设置: "cuda"。




示例命令：

CPU（小规模测试）：
bashpython src/coordinate_distill.py --dataset_path data/raw/wmt19_zh_en --batch_size 1 --max_seq_len 64 --max_samples 100 --shard_size 100 --device cpu --int8

GPU（全量）：
bashpython src/coordinate_distill.py --dataset_path data/raw/wmt19_zh_en --batch_size 16 --max_seq_len 64 --max_samples 26000000 --shard_size 100000 --device cuda --compile