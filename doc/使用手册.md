# 使用手册

以下是完整的端侧模型开发和评估流程，覆盖模型定义到量化，重点更新评估部分以支持中英互译。

## 环境准备

依赖安装（Windows，CPU 环境）：
powershellpip install torch==2.8.0 transformers datasets sacrebleu psutil tqdm

解决依赖冲突：
powershellpip install torchaudio==2.8.0 torchvision==0.20.0
或：
powershellpip uninstall torchaudio torchvision -y



GPU 服务器（未来高端 PC，假设 NVIDIA GPU，CUDA 12.1）：
bashpip install torch==2.8.0+cu121 transformers datasets sacrebleu psutil tqdm

数据准备：

WMT19 数据集：data/raw/wmt19_zh_en（训练和测试数据，.parquet 格式）。
确保测试数据：data/raw/wmt19_zh_en/test/*.{zh_to_en,en_to_zh}.parquet。
QWen 模型：models/qwen_tokenizer_offline/qwen/Qwen1___5-1___8B。


磁盘空间：

确保 ~10-20GB 空间（训练分片 ~3.6GB，模型权重 ~180MB，量化模型 ~90-100MB）。



## 模型定义（models/tiny_transformer.py）

作用：定义学生模型 TinyTransformer（~43M 参数）。
参数（可调）：

d_model=512：隐藏维度（优化：128 或 256，减少内存）。
nhead=8：注意力头数（优化：4 或 2）。
num_layers=6：Transformer 层数（优化：2 或 4）。
max_seq_len=64：最大序列长度（端侧优化）。
share_weights=False：共享嵌入和 LM 头权重（优化：True，减少参数）。
vocab_size=151936：词汇表大小（从 QWen tokenizer 读取）。


调整：

编辑 models/tiny_transformer.py，例如：
pythond_model = 128
nhead = 4
num_layers = 2
share_weights = True

测试：
bashpython -c "from models.tiny_transformer import TinyTransformer; model = TinyTransformer(vocab_size=151936, max_seq_len=64); print(sum(p.numel() for p in model.parameters()))"
预期参数量：~20M（优化后）。



## 生成教师标签（scripts/generate_logits_grok.py）

作用：从 WMT19 数据集生成 teacher logits，保存为 .pt。
参数：

--dataset_path: "data/raw/wmt19_zh_en"
--batch_size: CPU=1-4，GPU=16-32
--max_seq_len: 64
--max_samples: CPU=100-1000，GPU=100,000
--start_from: 0
--shard_size: CPU=100-500，GPU=100,000
--compile: CPU=False，GPU=True
--int8: CPU=True，GPU=False
--device: CPU="cpu"，GPU="cuda"
--use_api: False


命令：

CPU：
bashpython scripts/generate_logits_grok.py --dataset_path data/raw/wmt19_zh_en --batch_size 1 --max_seq_len 64 --max_samples 100 --shard_size 100 --device cpu --int8

GPU：
bashpython scripts/generate_logits_grok.py --dataset_path data/raw/wmt19_zh_en --batch_size 16 --max_seq_len 64 --max_samples 100000 --shard_size 100000 --device cuda --compile



输出：data/teacher_logits/{zh_to_en,en_to_zh}_shard_X.pt（~3.6GB/100,000 样本）。
验证：检查日志 logs/generate_logits.log。

## 蒸馏训练（src/train_distill_amp_grok.py）

作用：使用 teacher logits 训练 TinyTransformer。
参数：

--teacher_logits_dir: "data/teacher_logits"
--output_model_dir: "outputs/models"
--epochs: 3
--batch_size: CPU=2-4，GPU=8-32
--gradient_accumulation_steps: 1
--learning_rate: 3e-4
--max_samples_per_task: CPU=100-1000，GPU=100,000
--device: CPU="cpu"，GPU="cuda"
--use_jsonl: False
--max_seq_len: 64
--compile: CPU=False，GPU=True
--shard_idx: 0
--patience: 2


命令：

CPU：
bashpython src/train_distill_amp_grok.py --teacher_logits_dir data/teacher_logits --batch_size 2 --device cpu --max_samples_per_task 100 --shard_idx 0 --max_seq_len 64 --gradient_accumulation_steps 1 --patience 2

GPU：
bashpython src/train_distill_amp_grok.py --teacher_logits_dir data/teacher_logits --batch_size 16 --device cuda --max_samples_per_task 100000 --shard_idx 0 --max_seq_len 64 --gradient_accumulation_steps 1 --patience 2 --compile



输出：outputs/models/student_model_amp_shard_X_best.pth（~180MB）。
验证：检查日志 logs/train_distill_amp.log，确认损失下降。

## 协调流程（src/coordinate_distill.py）

作用：分片处理全量数据（生成 logits、训练、删除）。
参数：

--dataset_path: "data/raw/wmt19_zh_en"
--batch_size: CPU=1-4，GPU=16-32
--max_seq_len: 64
--max_samples: CPU=100-1000，GPU=26,000,000
--shard_size: CPU=100-500，GPU=100,000
--compile: CPU=False，GPU=True
--int8: CPU=True，GPU=False
--use_api: False
--device: CPU="cpu"，GPU="cuda"


命令：

CPU：
bashpython src/coordinate_distill.py --dataset_path data/raw/wmt19_zh_en --batch_size 1 --max_seq_len 64 --max_samples 100 --shard_size 100 --device cpu --int8

GPU：
bashpython src/coordinate_distill.py --dataset_path data/raw/wmt19_zh_en --batch_size 16 --max_seq_len 64 --max_samples 26000000 --shard_size 100000 --device cuda --compile



输出：outputs/models 中的权重文件。
验证：检查日志 logs/coordinate_distill.log。

## 量化模型（scripts/quantize_model.py）

作用：将 FP32 模型量化为 INT8，生成 student_model_int8.pth。
参数：无（硬编码路径）。
命令：

CPU/GPU：
bashpython scripts/quantize_model.py



输出：outputs/models/student_model_int8.pth（~90-100MB）。
验证：检查日志 logs/quantize_model.log。

## 评估模型（scripts/evaluate_model.py）

作用：评估 FP32 或 INT8 模型的内存占用、BLEU 分数、推理延迟，支持中英互译。
参数：

--model_path (type: str, required): 模型权重路径（例如 outputs/models/student_model_amp_shard_0_best.pth 或 student_model_int8.pth）。
--is_int8 (action="store_true"): 是否 INT8 模型。
--tasks (type: str, nargs='+', default: ["zh_to_en", "en_to_zh"]): 任务方向。
--max_samples (type: int, default: 100): 每任务测试样本数。


命令：

CPU（FP32，测试中英互译）：
bashpython scripts/evaluate_model.py --model_path outputs/models/student_model_amp_shard_0_best.pth --tasks zh_to_en en_to_zh --max_samples 100

CPU（INT8）：
bashpython scripts/evaluate_model.py --model_path outputs/models/student_model_int8.pth --is_int8 --tasks zh_to_en en_to_zh --max_samples 100

GPU（FP32，假设 CUDA 可用）：
bashpython scripts/evaluate_model.py --model_path outputs/models/student_model_amp_shard_0_best.pth --tasks zh_to_en en_to_zh --max_samples 1000

GPU（INT8）：
bashpython scripts/evaluate_model.py --model_path outputs/models/student_model_int8.pth --is_int8 --tasks zh_to_en en_to_zh --max_samples 1000



输出：日志 logs/evaluate_model.log，示例：
text2025-10-09 10:00:00 - INFO - ✅ 成功加载 tokenizer
2025-10-09 10:00:00 - INFO - ✅ 加载 FP32 模型
2025-10-09 10:00:00 - INFO - ✅ 加载 WMT19 测试数据集 (zh_to_en)，样本数: 100
2025-10-09 10:00:00 - INFO - ✅ 加载 WMT19 测试数据集 (en_to_zh)，样本数: 100
2025-10-09 10:00:05 - INFO - 内存占用: 512.34 MB
2025-10-09 10:00:10 - INFO - 评估任务: zh_to_en
2025-10-09 10:00:15 - INFO - BLEU 分数 (zh_to_en): 35.67
2025-10-09 10:00:20 - INFO - 平均推理延迟 (zh_to_en): 25.45 ms (100 条样本)
2025-10-09 10:00:25 - INFO - 评估任务: en_to_zh
2025-10-09 10:00:30 - INFO - BLEU 分数 (en_to_zh): 32.12
2025-10-09 10:00:35 - INFO - 平均推理延迟 (en_to_zh): 26.78 ms (100 条样本)

验证：确保 BLEU >30，延迟 <30ms，内存 <512MB（RV1126B 目标）。

## 部署到 RV1126B

转换模型：
bashrknn_model_convert --input outputs/models/student_model_int8.pth --output outputs/models/student_model.rknn --target rv1126

配置：

输入张量：input_ids: [1, 64], dtype=int32；attention_mask: [1, 64], dtype=int32。


测试：

在 RV1126B 上运行 WMT19 测试集（中英互译）。
验证 BLEU 分数和推理延迟（<30ms）。



## 参数优化流程

调整 TinyTransformer：

修改 models/tiny_transformer.py，尝试：

d_model=128, nhead=4, num_layers=2, share_weights=True（~20M 参数）。
d_model=256, nhead=4, num_layers=4, share_weights=True（~30M 参数）。




训练：

运行 coordinate_distill.py（小规模测试，max_samples=100）。
生成权重 student_model_amp_shard_0_best.pth。


量化：

运行 quantize_model.py，生成 student_model_int8.pth。


评估：

运行 evaluate_model.py，记录内存、BLEU、延迟（中英互译）。
比较不同参数配置（例如 d_model=128 vs 256）。


选择最佳配置：

目标：BLEU >30，延迟 <30ms，内存 <100MB（INT8）。
例如：若 d_model=128 的 BLEU=34，延迟=20ms，优于 d_model=256 的 BLEU=35，延迟=35ms，则选择前者。