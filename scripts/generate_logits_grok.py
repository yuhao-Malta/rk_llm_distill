import os
import torch
import psutil  # æ·»åŠ å†…å­˜ç›‘æ§
import json
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
import argparse
import logging
import time

try:
    import dashscope
except ImportError:
    dashscope = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,  # æé«˜æ—¥å¿—çº§åˆ«
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/generate_logits.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "qwen_tokenizer_offline", "qwen", "Qwen1___5-1___8B")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "teacher_logits")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# é…ç½®DashScope API Keyï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„çœŸå®Keyï¼‰
DASHSCOPE_API_KEY = "sk-b0c78a77e5ea489b8c68e0b5049204c6"  # è¯·æ›¿æ¢ï¼


# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
def check_model_files(model_path):
    required_files = ["config.json", "tokenizer_config.json"]
    for file in required_files:
        if not os.path.exists(os.path.join(model_path, file)):
            raise FileNotFoundError(f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {os.path.join(model_path, file)}")
    # æ£€æŸ¥æ¨¡å‹æƒé‡
    if not (any(f.endswith("pytorch_model.bin") for f in os.listdir(model_path)) or
            any(f.endswith(".safetensors") for f in os.listdir(model_path))):
        raise FileNotFoundError(f"âŒ æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆpytorch_model.binæˆ–safetensorsï¼‰ä¸å­˜åœ¨: {model_path}")
    # æ£€æŸ¥safetensorså¤§å°
    safetensors_files = [f for f in os.listdir(model_path) if f.endswith(".safetensors")]
    if safetensors_files:
        size_mb = os.path.getsize(os.path.join(model_path, safetensors_files[0])) / 1024 ** 2
        logging.info(f"âœ… æ¨¡å‹æƒé‡æ–‡ä»¶: {safetensors_files[0]}, å¤§å°: {size_mb:.2f} MB")
    logging.info(f"âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {model_path}")


# DashScope APIç¿»è¯‘
def call_qwen_translate_api(text, target_lang="en", max_retries=3):
    if not dashscope:
        raise ImportError("âŒ DashScopeæœªå®‰è£…ï¼Œè¯·è¿è¡Œ 'pip install dashscope'")
    dashscope.api_key = DASHSCOPE_API_KEY
    TEXT_TRANSLATION_AVAILABLE = hasattr(dashscope, 'TextTranslation')

    for attempt in range(max_retries):
        logging.info(f"API ç¿»è¯‘å°è¯• {attempt + 1}/{max_retries}")
        try:
            if TEXT_TRANSLATION_AVAILABLE:
                response = dashscope.TextTranslation.call(
                    model='qwen-max',
                    text=text,
                    target_language=target_lang
                )
                if response.status_code == 200:
                    return response.output['translated_text']
                logging.warning(f"âš ï¸ TextTranslationå¤±è´¥: {response.message}")
            else:
                source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
                target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
                prompt = (
                    f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
                    f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
                    f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{text}"
                )
                response = dashscope.Generation.call(
                    model='qwen-max',
                    prompt=prompt,
                    max_tokens=512,
                    temperature=0.1
                )
                if response.status_code == 200:
                    return response.output.get('text', '').strip()
                logging.warning(f"âš ï¸ Generationå¤±è´¥: {response.message}")
        except Exception as e:
            logging.warning(f"âš ï¸ APIè°ƒç”¨å¼‚å¸¸: {e}")
        if attempt < max_retries - 1:
            time.sleep(2 ** attempt)
    logging.error(f"âŒ APIç¿»è¯‘å¤±è´¥ï¼ˆ{max_retries}æ¬¡å°è¯•ï¼‰")
    return None


# æœ¬åœ°æ¨¡å‹ç¿»è¯‘
def call_qwen_translate_local(model, tokenizer, text, target_lang="en", max_seq_len=64):
    source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
    target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
    prompt = (
        f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
        f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
        f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{text}"
    )
    inputs = tokenizer(
        prompt, max_length=max_seq_len, padding="max_length", truncation=True, return_tensors="pt"
    ).to(model.device)
    inputs["input_ids"] = inputs["input_ids"].to(dtype=torch.long)
    inputs["attention_mask"] = inputs["attention_mask"].to(dtype=torch.long)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.logits  # è¿”å›logits

# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class TranslationDataset(Dataset):
    def __init__(self, dataset, tokenizer, max_seq_len=64, src_lang="zh", tgt_lang="en"):
        self.dataset = dataset
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        src_text = item["translation"][self.src_lang]
        tgt_text = item["translation"][self.tgt_lang]

        source_lang_full = "ä¸­æ–‡" if self.src_lang == "zh" else "è‹±æ–‡"
        target_lang_full = "è‹±æ–‡" if self.tgt_lang == "en" else "ä¸­æ–‡"
        prompt = (
            f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
            f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
            f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{src_text}"
        )

        src_encoding = self.tokenizer(
            prompt, max_length=self.max_seq_len, padding="max_length", truncation=True, return_tensors="pt"
        )
        tgt_encoding = self.tokenizer(
            tgt_text, max_length=self.max_seq_len, padding="max_length", truncation=True, return_tensors="pt"
        )

        return {
            "idx": idx,
            "src_input_ids": src_encoding["input_ids"].squeeze().to(dtype=torch.long),
            "src_attention_mask": src_encoding["attention_mask"].squeeze().to(dtype=torch.long),
            "tgt_input_ids": tgt_encoding["input_ids"].squeeze().to(dtype=torch.long),
            "tgt_attention_mask": tgt_encoding["attention_mask"].squeeze().to(dtype=torch.long),
            "src_text": src_text,
            "tgt_text": tgt_text
        }

# è‡ªå®šä¹‰ collate_fn
def custom_collate_fn(batch, max_seq_len=64, pad_token_id=151643):
    src_input_ids = [item["src_input_ids"] for item in batch]
    src_attention_mask = [item["src_attention_mask"] for item in batch]
    tgt_input_ids = [item["tgt_input_ids"] for item in batch]
    tgt_attention_mask = [item["tgt_attention_mask"] for item in batch]
    idx = [item["idx"] for item in batch]
    src_texts = [item["src_text"] for item in batch]
    tgt_texts = [item["tgt_text"] for item in batch]

    src_input_ids = torch.nn.utils.rnn.pad_sequence(
        src_input_ids, batch_first=True, padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)
    src_attention_mask = torch.nn.utils.rnn.pad_sequence(
        src_attention_mask, batch_first=True, padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)
    tgt_input_ids = torch.nn.utils.rnn.pad_sequence(
        tgt_input_ids, batch_first=True, padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)
    tgt_attention_mask = torch.nn.utils.rnn.pad_sequence(
        tgt_attention_mask, batch_first=True, padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)
    idx = torch.tensor(idx, dtype=torch.long)

    return {
        "idx": idx,
        "src_input_ids": src_input_ids,
        "src_attention_mask": src_attention_mask,
        "tgt_input_ids": tgt_input_ids,
        "tgt_attention_mask": tgt_attention_mask,
        "src_text": src_texts,
        "tgt_text": tgt_texts
    }

# éªŒè¯ logits æ–‡ä»¶
def validate_logits_file(file_path, use_api=False, model_path=None):
    """éªŒè¯ç”Ÿæˆçš„.ptæˆ–.jsonlæ–‡ä»¶ï¼ŒAPIæ¨¡å¼æ£€æŸ¥hyp_textï¼ŒéAPIæ¨¡å¼åŠ¨æ€æ£€æŸ¥logitsç»´åº¦"""
    try:
        # åŠ¨æ€è·å–è¯æ±‡è¡¨å¤§å°
        if not use_api and model_path:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path, local_files_only=True, trust_remote_code=True
            )
            config_path = os.path.join(model_path, "config.json")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            vocab_size = config.get("vocab_size", len(tokenizer))
            logging.info(f"âœ… æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        else:
            vocab_size = None

        if file_path.endswith(".jsonl"):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = [json.loads(line) for line in f]
            count = len(data)
            for item in data:
                assert "id" in item and "task_id" in item, f"ç¼ºå°‘idæˆ–task_id: {item}"
                assert "src" in item and "ref" in item and "hyp" in item, f"ç¼ºå°‘src/ref/hyp: {item}"
                assert isinstance(item["src"], str) and isinstance(item["ref"], str) and isinstance(item["hyp"], str), f"src/ref/hypç±»å‹é”™è¯¯: {item}"
            logging.info(f"âœ… éªŒè¯é€šè¿‡ï¼{file_path} å…± {count} æ¡æœ‰æ•ˆè®°å½•ï¼ˆJSONLï¼ŒAPIæ¨¡å¼ï¼‰")
        else:
            data = torch.load(file_path)
            count = len(data)
            for item in data:
                assert "id" in item and "task_id" in item, f"ç¼ºå°‘idæˆ–task_id: {item}"
                if use_api:
                    assert "hyp_text" in item and item["hyp_text"] is not None, f"APIæ¨¡å¼ç¼ºå°‘hyp_text: {item}"
                    logging.warning("âš ï¸ APIæ¨¡å¼ï¼šlogitsä¸ºNoneï¼Œä»…ä¿å­˜ç¿»è¯‘æ–‡æœ¬")
                else:
                    assert "logits" in item and item["logits"] is not None, f"ç¼ºå°‘logits: {item}"
                    if vocab_size:
                        if item["logits"].shape[-1] != vocab_size:
                            logging.warning(f"âš ï¸ logitsç»´åº¦ä¸åŒ¹é… (é¢„æœŸ: {vocab_size}, å®é™…: {item['logits'].shape[-1]})")
                        else:
                            logging.debug(f"âœ… logitsç»´åº¦éªŒè¯é€šè¿‡: {item['logits'].shape}")
                    else:
                        logging.debug(f"âš ï¸ æœªæä¾›model_pathï¼Œè·³è¿‡logitsç»´åº¦æ£€æŸ¥ï¼Œå®é™…ç»´åº¦: {item['logits'].shape}")
            logging.info(f"âœ… éªŒè¯é€šè¿‡ï¼{file_path} å…± {count} æ¡æœ‰æ•ˆè®°å½•ï¼ˆPTï¼Œ{'APIæ¨¡å¼' if use_api else 'æœ¬åœ°æ¨¡å¼'}ï¼‰")
        return count
    except Exception as e:
        logging.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return 0

# ç”Ÿæˆ teacher logits
def generate_teacher_logits(args):
    """
    ç”ŸæˆQWen-1.5-1.8Bè½¯æ ‡ç­¾ï¼ˆä¸­â†’è‹±ï¼Œè‹±â†’ä¸­ï¼‰ï¼Œä¿å­˜ä¸º.ptæ–‡ä»¶
    ä¼˜åŒ–ï¼šæ”¯æŒæœ¬åœ°/APIã€æ‰¹å¤„ç†ã€FP16/INT8ã€max_seq_len=64
    """
    # å†…å­˜ç›‘æ§
    process = psutil.Process()
    logging.debug(
        f"åˆå§‹å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024 ** 2:.2f} MB, å¯ç”¨ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory().available / 1024 ** 2:.2f} MB")

    # åŠ è½½tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, local_files_only=True, trust_remote_code=True
        )
        logging.info("âœ… æˆåŠŸåŠ è½½ Qwen Tokenizer")
        logging.debug(f"åŠ è½½tokenizeråå†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
    except Exception as e:
        logging.error(f"âŒ åŠ è½½tokenizerå¤±è´¥: {e}")
        raise

    # åŠ è½½æ¨¡å‹ï¼ˆæœ¬åœ°æˆ–APIï¼‰
    model = None
    if not args.use_api:
        try:
            check_model_files(MODEL_PATH)
            logging.debug("å¼€å§‹åŠ è½½ Qwen-1.5-1.8B æ¨¡å‹...")
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map=args.device,
                torch_dtype=torch.float32,  # ä½¿ç”¨ FP32 é¿å…å†…å­˜æº¢å‡º
                local_files_only=True,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True
            )
            if args.int8:
                logging.debug("åº”ç”¨INT8é‡åŒ–...")
                model = torch.quantization.quantize_dynamic(
                    model,
                    qconfig_spec={
                        torch.nn.Linear: torch.quantization.default_dynamic_qconfig,
                        torch.nn.Embedding: torch.quantization.float_qparams_weight_only_qconfig
                    },
                    dtype=torch.qint8
                )
            model.eval()
            if args.compile and args.device == "cuda":
                model = torch.compile(model)
            device = next(model.parameters()).device
            logging.info(f"âœ… æˆåŠŸåŠ è½½ Qwen-1.5-1.8B åˆ° {device}")
            logging.debug(f"åŠ è½½æ¨¡å‹åå†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
        except Exception as e:
            logging.error(f"âŒ åŠ è½½æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
            if args.use_api:
                logging.warning("âš ï¸ å›é€€åˆ°DashScope API")
            else:
                raise
    else:
        device = "cpu"

    # åŠ è½½WMT19æ•°æ®é›†
    dataset_path = os.path.join(PROJECT_ROOT, args.dataset_path)
    try:
        dataset = load_dataset("parquet", data_files={"train": f"{dataset_path}/train/*.parquet"})["train"]
        if args.debug:
            dataset = dataset.select(range(1))
        logging.info(f"âœ… åŠ è½½WMT19æ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {len(dataset)}")
        logging.debug(f"åŠ è½½æ•°æ®é›†åå†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
    except Exception as e:
        logging.error(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        raise

    # è®¾ç½®æ ·æœ¬èŒƒå›´å’Œåˆ†ç‰‡
    total_samples = len(dataset) if args.max_samples is None else min(args.max_samples, len(dataset))
    shard_size = min(args.shard_size, total_samples)  # ç¡®ä¿åˆ†ç‰‡å¤§å°åˆç†
    num_shards = (total_samples + shard_size - 1) // shard_size

    # ç”Ÿæˆä¸­â†’è‹±å’Œè‹±â†’ä¸­
    success_count = 0
    for src_lang, tgt_lang, task_id, output_prefix in [
        ("zh", "en", 0, os.path.join(OUTPUT_DIR, "zh_to_en")),
        ("en", "zh", 1, os.path.join(OUTPUT_DIR, "en_to_zh"))
    ]:
        logging.info(f"ğŸ§  ç”Ÿæˆ {src_lang}â†’{tgt_lang} è½¯æ ‡ç­¾ï¼Œåˆ†ç‰‡å¤§å°: {shard_size}")
        for shard_idx in range(num_shards):
            start_idx = shard_idx * shard_size
            end_idx = min(start_idx + shard_size, total_samples)
            shard_dataset = dataset.select(range(args.start_from + start_idx, args.start_from + end_idx))
            shard_dataloader = DataLoader(
                TranslationDataset(shard_dataset, tokenizer, max_seq_len=args.max_seq_len, src_lang=src_lang,
                                   tgt_lang=tgt_lang),
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=0,
                pin_memory=(args.device == "cuda"),
                collate_fn=lambda batch: custom_collate_fn(batch, max_seq_len=args.max_seq_len, pad_token_id=tokenizer.pad_token_id)
            )

            output_file = f"{output_prefix}_shard_{shard_idx}.{'jsonl' if args.use_api else 'pt'}"
            output_data = []

            if args.use_api:
                with open(output_file, 'w', encoding='utf-8') as f:
                    for batch_idx, batch in enumerate(
                            tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {shard_idx}")):
                        start_time = time.time()
                        logging.debug(f"æ‰¹æ¬¡ {batch_idx + 1} å¼€å§‹ï¼Œå†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")

                        idx = batch["idx"]
                        src_texts = batch["src_text"]
                        tgt_texts = batch["tgt_text"]

                        try:
                            for i in range(len(idx)):
                                trans_text = call_qwen_translate_api(src_texts[i], tgt_lang)
                                if trans_text:
                                    item = {
                                        "id": idx[i].item(),
                                        "src": src_texts[i],
                                        "ref": tgt_texts[i],
                                        "hyp": trans_text,
                                        "task_id": task_id,
                                        "timestamp": time.time()
                                    }
                                    json.dump(item, f, ensure_ascii=False)
                                    f.write('\n')
                                    output_data.append(item)
                                    success_count += 1
                                else:
                                    logging.warning(f"âš ï¸ APIç¿»è¯‘å¤±è´¥: ID {idx[i].item()}")
                            logging.info(
                                f"æ‰¹æ¬¡ {batch_idx + 1}/{len(shard_dataloader)} å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s, å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
                        except Exception as e:
                            logging.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¤±è´¥: {e}")
            else:
                with torch.no_grad():
                    for batch_idx, batch in enumerate(
                            tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {shard_idx}")):
                        start_time = time.time()
                        logging.debug(f"æ‰¹æ¬¡ {batch_idx + 1} å¼€å§‹ï¼Œå†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")

                        idx = batch["idx"]
                        src_texts = batch["src_text"]
                        tgt_texts = batch["tgt_text"]
                        src_input_ids = batch["src_input_ids"].to(device).to(dtype=torch.long)
                        src_attention_mask = batch["src_attention_mask"].to(device).to(dtype=torch.long)
                        try:
                            outputs = model(input_ids=src_input_ids, attention_mask=src_attention_mask)
                            logits = outputs.logits  # [batch, seq_len, vocab_size]
                            for i in range(len(idx)):
                                output_data.append({
                                    "id": idx[i].item(),
                                    "src_input_ids": src_input_ids[i].cpu().to(dtype=torch.long),
                                    "src_attention_mask": src_attention_mask[i].cpu().to(dtype=torch.long),
                                    "tgt_input_ids": batch["tgt_input_ids"][i].cpu().to(dtype=torch.long),
                                    "tgt_attention_mask": batch["tgt_attention_mask"][i].cpu().to(dtype=torch.long),
                                    "logits": logits[i].cpu(),
                                    "task_id": task_id,
                                    "src_text": src_texts[i],
                                    "tgt_text": tgt_texts[i],
                                    "hyp_text": None
                                })
                            success_count += len(idx)
                            logging.info(
                                f"æ‰¹æ¬¡ {batch_idx + 1}/{len(shard_dataloader)} å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s, å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
                        except Exception as e:
                            logging.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¤±è´¥: {e}")

                        if args.device == "cuda":
                            torch.cuda.empty_cache()
                        # é‡Šæ”¾ CPU å†…å­˜
                        torch.cuda.empty_cache() if args.device == "cuda" else torch.cpu.empty_cache()

                    torch.save(output_data, output_file)

            logging.info(f"âœ… {src_lang}â†’{tgt_lang} åˆ†ç‰‡ {shard_idx} å®Œæˆï¼æˆåŠŸ: {success_count}, ä¿å­˜åˆ° {output_file}")
            validate_logits_file(output_file, use_api=args.use_api, model_path=MODEL_PATH)
            # é‡Šæ”¾ output_data
            output_data = []
            import gc
            gc.collect()

    # é‡Šæ”¾æ¨¡å‹å†…å­˜
    if model is not None:
        del model
        torch.cuda.empty_cache() if args.device == "cuda" else torch.cpu.empty_cache()
        gc.collect()

    return success_count, 0

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ QWen-1.5-1.8B è½¯æ ‡ç­¾")
    parser.add_argument("--dataset_path", type=str, default="data/raw/wmt19_zh_en", help="WMT19 æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--max_seq_len", type=int, default=64, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--max_samples", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆNone=å…¨é‡ï¼‰")
    parser.add_argument("--start_from", type=int, default=0, help="å¼€å§‹æ ·æœ¬ç´¢å¼•")
    parser.add_argument("--shard_size", type=int, default=100000, help="æ¯ç‰‡æ ·æœ¬æ•°ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰")
    parser.add_argument("--compile", action="store_true", help="ä½¿ç”¨ torch.compile åŠ é€Ÿ")
    parser.add_argument("--int8", action="store_true", help="ä½¿ç”¨ INT8 é‡åŒ–ï¼ˆCPUï¼‰")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼ï¼ˆ1 æ¡ï¼‰")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--use_api", action="store_true", help="ä½¿ç”¨ DashScope APIï¼ˆç”Ÿæˆ jsonlï¼‰")
    args = parser.parse_args()

    try:
        success, fail = generate_teacher_logits(args)
        logging.info(f"ğŸ‰ å¤„ç†å®Œæˆï¼æˆåŠŸ: {success}, å¤±è´¥: {fail}")
    except Exception as e:
        logging.error(f"âŒ ä¸»ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        raise
