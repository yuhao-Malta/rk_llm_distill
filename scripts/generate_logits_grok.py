import os
import sys
import torch
import psutil
import json
import gc

from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
import argparse
import logging
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import (
    ModelConfig, DataFormat, LogConfig,
    MODEL_PATH, TEACHER_LOGITS_DIR, RAW_DATA_PATH
)

try:
    import dashscope
except ImportError:
    dashscope = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format=LogConfig.LOG_FORMAT,
    handlers=[
        logging.FileHandler(LogConfig.GENERATE_LOG, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# DashScope API Key
DASHSCOPE_API_KEY = "sk-b0c78a77e5ea489b8c68e0b5049204c6"  # è¯·æ›¿æ¢


# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
def check_model_files(model_path):
    required_files = ["config.json", "tokenizer_config.json"]
    for file in required_files:
        if not os.path.exists(os.path.join(model_path, file)):
            raise FileNotFoundError(f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {os.path.join(model_path, file)}")
    # æ£€æŸ¥æ¨¡å‹æƒé‡
    if not (any(f.endswith("pytorch_model.bin") for f in os.listdir(model_path)) or
            any(f.endswith(".safetensors") for f in os.listdir(model_path))):
        raise FileNotFoundError(f"âŒ æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆpytorch_model.binæˆ–safetensorsï¼‰ä¸å­˜åœ¨: {model_path}")
    # æ£€æŸ¥safetensorså¤§å°
    safetensors_files = [f for f in os.listdir(model_path) if f.endswith(".safetensors")]
    if safetensors_files:
        size_mb = os.path.getsize(os.path.join(model_path, safetensors_files[0])) / 1024 ** 2
        logging.info(f"âœ… æ¨¡å‹æƒé‡æ–‡ä»¶: {safetensors_files[0]}, å¤§å°: {size_mb:.2f} MB")
    logging.info(f"âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {model_path}")


# DashScope APIç¿»è¯‘
def call_qwen_translate_api(text, target_lang="en", max_retries=3):
    if not dashscope:
        raise ImportError("âŒ DashScopeæœªå®‰è£…ï¼Œè¯·è¿è¡Œ 'pip install dashscope'")
    dashscope.api_key = DASHSCOPE_API_KEY
    TEXT_TRANSLATION_AVAILABLE = hasattr(dashscope, 'TextTranslation')

    for attempt in range(max_retries):
        logging.info(f"API ç¿»è¯‘å°è¯• {attempt + 1}/{max_retries}")
        try:
            if TEXT_TRANSLATION_AVAILABLE:
                response = dashscope.TextTranslation.call(
                    model='qwen-max',
                    text=text,
                    target_language=target_lang
                )
                if response.status_code == 200:
                    return response.output['translated_text']
                logging.warning(f"âš ï¸ TextTranslationå¤±è´¥: {response.message}")
            else:
                source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
                target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
                prompt = (
                    f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
                    f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
                    f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{text}"
                )
                response = dashscope.Generation.call(
                    model='qwen-max',
                    prompt=prompt,
                    max_tokens=512,
                    temperature=0.1
                )
                if response.status_code == 200:
                    return response.output.get('text', '').strip()
                logging.warning(f"âš ï¸ Generationå¤±è´¥: {response.message}")
        except Exception as e:
            logging.warning(f"âš ï¸ APIè°ƒç”¨å¼‚å¸¸: {e}")
        if attempt < max_retries - 1:
            time.sleep(2 ** attempt)
    logging.error(f"âŒ APIç¿»è¯‘å¤±è´¥ï¼ˆ{max_retries}æ¬¡å°è¯•ï¼‰")
    return None


# æœ¬åœ°æ¨¡å‹ç¿»è¯‘
def call_qwen_translate_local(model, tokenizer, text, target_lang="en", max_seq_len=64):
    source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
    target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
    prompt = (
        f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
        f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
        f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{text}"
    )
    inputs = tokenizer(
        prompt, max_length=max_seq_len, padding="max_length", truncation=True, return_tensors="pt"
    ).to(model.device)
    inputs["input_ids"] = inputs["input_ids"].to(dtype=torch.long)
    inputs["attention_mask"] = inputs["attention_mask"].to(dtype=torch.long)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.logits  # è¿”å›logits


# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class TranslationDataset(Dataset):
    """
    ç¿»è¯‘æ•°æ®é›†ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
    è¾“å‡ºå­—æ®µä¸¥æ ¼éµå¾ª DataFormat.REQUIRED_KEYS
    """

    def __init__(self, dataset, tokenizer, max_seq_len=64, src_lang="zh", tgt_lang="en", task_id=0):
        self.dataset = dataset
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.task_id = task_id

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        src_text = item["translation"][self.src_lang]
        tgt_text = item["translation"][self.tgt_lang]

        # æ„é€ ç¿»è¯‘æç¤ºè¯
        source_lang_full = "ä¸­æ–‡" if self.src_lang == "zh" else "è‹±æ–‡"
        target_lang_full = "è‹±æ–‡" if self.tgt_lang == "en" else "ä¸­æ–‡"
        prompt = (
            f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
            f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
            f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{src_text}"
        )

        # åˆ†è¯
        src_encoding = self.tokenizer(
            prompt,
            max_length=self.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        tgt_encoding = self.tokenizer(
            tgt_text,
            max_length=self.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # âœ… ç»Ÿä¸€è¾“å‡ºæ ¼å¼ (éµå¾ª DataFormat)
        return {
            "id": idx,
            "src_text": src_text,
            "tgt_text": tgt_text,
            "src_input_ids": src_encoding["input_ids"].squeeze(0).to(dtype=torch.long),
            "src_attention_mask": src_encoding["attention_mask"].squeeze(0).to(dtype=torch.long),
            "tgt_input_ids": tgt_encoding["input_ids"].squeeze(0).to(dtype=torch.long),
            "tgt_attention_mask": tgt_encoding["attention_mask"].squeeze(0).to(dtype=torch.long),
            "task_id": self.task_id
        }


# ==================== è‡ªå®šä¹‰ collate_fn ====================
def custom_collate_fn(batch, max_seq_len=64, pad_token_id=151643):
    """æ‰¹æ¬¡æ•°æ®æ•´ç† (ç»Ÿä¸€æ ¼å¼)"""
    keys = ["id", "src_input_ids", "src_attention_mask", "tgt_input_ids", "tgt_attention_mask", "task_id"]
    src_texts = [item["src_text"] for item in batch]
    tgt_texts = [item["tgt_text"] for item in batch]

    # Pad åºåˆ—
    src_input_ids = torch.nn.utils.rnn.pad_sequence(
        [item["src_input_ids"] for item in batch],
        batch_first=True,
        padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)

    src_attention_mask = torch.nn.utils.rnn.pad_sequence(
        [item["src_attention_mask"] for item in batch],
        batch_first=True,
        padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)

    tgt_input_ids = torch.nn.utils.rnn.pad_sequence(
        [item["tgt_input_ids"] for item in batch],
        batch_first=True,
        padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)

    tgt_attention_mask = torch.nn.utils.rnn.pad_sequence(
        [item["tgt_attention_mask"] for item in batch],
        batch_first=True,
        padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)

    task_ids = torch.tensor([item["task_id"] for item in batch], dtype=torch.long)
    ids = torch.tensor([item["id"] for item in batch], dtype=torch.long)

    return {
        "id": ids,
        "src_text": src_texts,
        "tgt_text": tgt_texts,
        "src_input_ids": src_input_ids,
        "src_attention_mask": src_attention_mask,
        "tgt_input_ids": tgt_input_ids,
        "tgt_attention_mask": tgt_attention_mask,
        "task_id": task_ids
    }


# éªŒè¯ logits æ–‡ä»¶
def validate_logits_file(file_path, use_api=False, model_path=None):
    """éªŒè¯ç”Ÿæˆçš„.ptæˆ–.jsonlæ–‡ä»¶ï¼ŒAPIæ¨¡å¼æ£€æŸ¥hyp_textï¼ŒéAPIæ¨¡å¼åŠ¨æ€æ£€æŸ¥logitsç»´åº¦"""
    try:
        # åŠ¨æ€è·å–è¯æ±‡è¡¨å¤§å°
        if not use_api and model_path:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path, local_files_only=True, trust_remote_code=True
            )
            config_path = os.path.join(model_path, "config.json")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            vocab_size = config.get("vocab_size", len(tokenizer))
            logging.info(f"âœ… æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        else:
            vocab_size = None

        if file_path.endswith(".jsonl"):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = [json.loads(line) for line in f]
            count = len(data)
            for item in data:
                assert "id" in item and "task_id" in item, f"ç¼ºå°‘idæˆ–task_id: {item}"
                assert "src" in item and "ref" in item and "hyp" in item, f"ç¼ºå°‘src/ref/hyp: {item}"
                assert isinstance(item["src"], str) and isinstance(item["ref"], str) and isinstance(item["hyp"],
                                                                                                    str), f"src/ref/hypç±»å‹é”™è¯¯: {item}"
            logging.info(f"âœ… éªŒè¯é€šè¿‡ï¼{file_path} å…± {count} æ¡æœ‰æ•ˆè®°å½•ï¼ˆJSONLï¼ŒAPIæ¨¡å¼ï¼‰")
        else:
            data = torch.load(file_path)
            count = len(data)
            for item in data:
                assert "id" in item and "task_id" in item, f"ç¼ºå°‘idæˆ–task_id: {item}"
                if use_api:
                    assert "hyp_text" in item and item["hyp_text"] is not None, f"APIæ¨¡å¼ç¼ºå°‘hyp_text: {item}"
                    logging.warning("âš ï¸ APIæ¨¡å¼ï¼šlogitsä¸ºNoneï¼Œä»…ä¿å­˜ç¿»è¯‘æ–‡æœ¬")
                else:
                    assert "logits" in item and item["logits"] is not None, f"ç¼ºå°‘logits: {item}"
                    if vocab_size:
                        if item["logits"].shape[-1] != vocab_size:
                            logging.warning(
                                f"âš ï¸ logitsç»´åº¦ä¸åŒ¹é… (é¢„æœŸ: {vocab_size}, å®é™…: {item['logits'].shape[-1]})")
                        else:
                            logging.debug(f"âœ… logitsç»´åº¦éªŒè¯é€šè¿‡: {item['logits'].shape}")
                    else:
                        logging.debug(f"âš ï¸ æœªæä¾›model_pathï¼Œè·³è¿‡logitsç»´åº¦æ£€æŸ¥ï¼Œå®é™…ç»´åº¦: {item['logits'].shape}")
            logging.info(f"âœ… éªŒè¯é€šè¿‡ï¼{file_path} å…± {count} æ¡æœ‰æ•ˆè®°å½•ï¼ˆPTï¼Œ{'APIæ¨¡å¼' if use_api else 'æœ¬åœ°æ¨¡å¼'}ï¼‰")
        return count
    except Exception as e:
        logging.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return 0


# ==================== ç”Ÿæˆ Teacher Logits (ä¸»å‡½æ•°) ====================
def generate_teacher_logits(args):
    """
    ç”Ÿæˆ QWen-1.5-1.8B è½¯æ ‡ç­¾ (ä¸­â†’è‹±, è‹±â†’ä¸­)

    æ”¹è¿›ç‚¹ï¼š
    1. âœ… ç»Ÿä¸€æ•°æ®æ ¼å¼ (éµå¾ª DataFormat)
    2. âœ… åˆ†æ‰¹ä¿å­˜ï¼ŒåŠæ—¶é‡Šæ”¾å†…å­˜
    3. âœ… æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—
    """
    # å†…å­˜ç›‘æ§
    process = psutil.Process()
    logging.info(f"åˆå§‹å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")

    # 1. åŠ è½½ tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, local_files_only=True, trust_remote_code=True
        )
        logging.info("âœ… æˆåŠŸåŠ è½½ Qwen Tokenizer")
        logging.debug(f"åŠ è½½tokenizeråå†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
    except Exception as e:
        logging.error(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        raise

    # åŠ è½½æ¨¡å‹ï¼ˆæœ¬åœ°æˆ–APIï¼‰
    model = None
    device = args.device

    if not args.use_api:
        try:
            check_model_files(MODEL_PATH)
            logging.info(f"ğŸ“¥ åŠ è½½ QWen-1.5-1.8B æ¨¡å‹åˆ° {device}...")
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map=device,
                torch_dtype=torch.float32,
                local_files_only=True,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True
            )

            if args.int8:
                logging.debug("åº”ç”¨INT8é‡åŒ–...")
                model = torch.quantization.quantize_dynamic(
                    model,
                    {nn.Linear, nn.Embedding},
                    dtype=torch.qint8
                )
                logging.info("âœ… INT8é‡åŒ–å·²åº”ç”¨")

            model.eval()

            if args.compile and device == "cuda":
                model = torch.compile(model)
                logging.info("âœ… torch.compileå·²å¯ç”¨")

            logging.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB)")
        except Exception as e:
            logging.error(f"âŒ åŠ è½½æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
            if args.use_api:
                logging.warning("âš ï¸ å›é€€åˆ°DashScope API")
            else:
                raise
    else:
        device = "cpu"

    # 3. åŠ è½½æ•°æ®é›†
    try:
        dataset_path = os.path.join(RAW_DATA_PATH, "train/*.parquet")
        dataset = load_dataset("parquet", data_files={"train": dataset_path})["train"]

        if args.debug:
            dataset = dataset.select(range(1))
        logging.info(f"âœ… åŠ è½½WMT19æ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {len(dataset)}")
        logging.debug(f"åŠ è½½æ•°æ®é›†åå†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
    except Exception as e:
        logging.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        raise

    # 4. ç”Ÿæˆ logits (ä¸­â†’è‹±, è‹±â†’ä¸­)
    total_samples = len(dataset) if args.max_samples is None else min(args.max_samples, len(dataset))
    shard_size = min(args.shard_size, total_samples)  # ç¡®ä¿åˆ†ç‰‡å¤§å°åˆç†
    num_shards = (total_samples + shard_size - 1) // shard_size

    success_count = 0
    for src_lang, tgt_lang, task_id, output_prefix in [
        ("zh", "en", 0, os.path.join(TEACHER_LOGITS_DIR, "zh_to_en")),
        ("en", "zh", 1, os.path.join(TEACHER_LOGITS_DIR, "en_to_zh"))
    ]:
        logging.info(f"ğŸ§  ç”Ÿæˆ {src_lang}â†’{tgt_lang} logits (ä»»åŠ¡ID: {task_id})")

        for shard_idx in range(num_shards):
            start_idx = args.start_from + shard_idx * shard_size
            end_idx = min(start_idx + shard_size, total_samples)
            shard_dataset = dataset.select(range(start_idx, end_idx))

            # åˆ›å»º DataLoader
            shard_dataloader = DataLoader(
                TranslationDataset(
                    shard_dataset, tokenizer,
                    max_seq_len=args.max_seq_len,
                    src_lang=src_lang, tgt_lang=tgt_lang, task_id=task_id
                ),
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=0,
                pin_memory=(device == "cuda"),
                collate_fn=lambda b: custom_collate_fn(b, max_seq_len=args.max_seq_len,
                                                       pad_token_id=tokenizer.pad_token_id)
            )

            output_file = f"{output_prefix}_shard_{shard_idx}.{'jsonl' if args.use_api else 'pt'}"
            output_data = []

            # æœ¬åœ°æ¨¡å¼: ç”Ÿæˆ logits
            if not args.use_api:
                with torch.no_grad():
                    for batch_idx, batch in enumerate(
                            tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {shard_idx}")):
                        try:
                            src_input_ids = batch["src_input_ids"].to(device)
                            src_attention_mask = batch["src_attention_mask"].to(device)

                            # ç”Ÿæˆ logits
                            outputs = model(input_ids=src_input_ids, attention_mask=src_attention_mask)
                            logits = outputs.logits.cpu()  # ç§»å› CPU

                            # âœ… ä¿å­˜ç»Ÿä¸€æ ¼å¼æ•°æ®
                            for i in range(len(batch["id"])):
                                output_data.append({
                                    "id": batch["id"][i].item(),
                                    "src_text": batch["src_text"][i],
                                    "tgt_text": batch["tgt_text"][i],
                                    "src_input_ids": batch["src_input_ids"][i].cpu(),
                                    "src_attention_mask": batch["src_attention_mask"][i].cpu(),
                                    "tgt_input_ids": batch["tgt_input_ids"][i].cpu(),
                                    "tgt_attention_mask": batch["tgt_attention_mask"][i].cpu(),
                                    "task_id": batch["task_id"][i].item(),
                                    "logits": logits[i]  # [seq_len, vocab_size]
                                })

                            success_count += len(batch["id"])

                            # âœ… åˆ†æ‰¹ä¿å­˜ï¼Œé‡Šæ”¾å†…å­˜ (æ¯ 10 batch)
                            if (batch_idx + 1) % 10 == 0:
                                gc.collect()
                                if device == "cuda":
                                    torch.cuda.empty_cache()

                        except Exception as e:
                            logging.error(f"âŒ Batch {batch_idx} å¤„ç†å¤±è´¥: {e}")
                            continue

            # APIæ¨¡å¼: ä»…ç”Ÿæˆç¿»è¯‘æ–‡æœ¬
            else:
                for batch_idx, batch in enumerate(
                        tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {shard_idx}")):
                    for i in range(len(batch["id"])):
                        hyp_text = call_qwen_translate_api(batch["src_text"][i], tgt_lang)
                        if hyp_text:
                            output_data.append({
                                "id": batch["id"][i].item(),
                                "src_text": batch["src_text"][i],
                                "tgt_text": batch["tgt_text"][i],
                                "src_input_ids": batch["src_input_ids"][i].cpu(),
                                "src_attention_mask": batch["src_attention_mask"][i].cpu(),
                                "tgt_input_ids": batch["tgt_input_ids"][i].cpu(),
                                "tgt_attention_mask": batch["tgt_attention_mask"][i].cpu(),
                                "task_id": batch["task_id"][i].item(),
                                "logits": None,  # APIæ¨¡å¼æ— logits
                                "hyp_text": hyp_text
                            })
                            success_count += 1

            # ä¿å­˜åˆ†ç‰‡
            torch.save(output_data, output_file)
            logging.info(f"ğŸ’¾ ä¿å­˜åˆ†ç‰‡: {output_file} ({len(output_data)} æ¡)")
            validate_logits_file(output_file)

            # é‡Šæ”¾å†…å­˜
            del output_data
            gc.collect()

    # æ¸…ç†
    if model is not None:
        del model
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()

    logging.info(f"ğŸ‰ å®Œæˆï¼æˆåŠŸ: {success_count}")
    return success_count


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ Teacher Logits (ä¼˜åŒ–ç‰ˆ)")
    parser.add_argument("--dataset_path", type=str, default="data/raw/wmt19_zh_en", help="WMT19è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹å¤§å°")
    parser.add_argument("--max_seq_len", type=int, default=ModelConfig.MAX_SEQ_LEN, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--max_samples", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--start_from", type=int, default=0, help="èµ·å§‹ç´¢å¼•")
    parser.add_argument("--shard_size", type=int, default=100000, help="åˆ†ç‰‡å¤§å°")
    parser.add_argument("--compile", action="store_true", help="torch.compile")
    parser.add_argument("--int8", action="store_true", help="INT8é‡åŒ–")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--use_api", action="store_true", help="ä½¿ç”¨API")
    args = parser.parse_args()

    try:
        success, fail = generate_teacher_logits(args)
        logging.info(f"ğŸ‰ å¤„ç†å®Œæˆï¼æˆåŠŸ: {success}, å¤±è´¥: {fail}")
    except Exception as e:
        logging.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        raise
