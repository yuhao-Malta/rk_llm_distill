import os
import sys
import torch
import psutil
import json
import gc

from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
import argparse
import logging
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import (
    ModelConfig, DataFormat, LogConfig,
    OPUS_MT_ZH_EN, OPUS_MT_EN_ZH, TEACHER_LOGITS_DIR, RAW_DATA_PATH
)

try:
    import dashscope
except ImportError:
    dashscope = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format=LogConfig.LOG_FORMAT,
    handlers=[
        logging.FileHandler(LogConfig.GENERATE_LOG, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# DashScope API Key
DASHSCOPE_API_KEY = "sk-b0c78a77e5ea489b8c68e0b5049204c6"  # è¯·æ›¿æ¢


# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
def check_model_files(model_path):
    required_files = ["config.json", "tokenizer_config.json"]
    for file in required_files:
        if not os.path.exists(os.path.join(model_path, file)):
            raise FileNotFoundError(f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {os.path.join(model_path, file)}")
    # æ£€æŸ¥æ¨¡å‹æƒé‡
    if not (any(f.endswith("pytorch_model.bin") for f in os.listdir(model_path)) or
            any(f.endswith(".safetensors") for f in os.listdir(model_path))):
        raise FileNotFoundError(f"âŒ æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆpytorch_model.binæˆ–safetensorsï¼‰ä¸å­˜åœ¨: {model_path}")
    # æ£€æŸ¥safetensorså¤§å°
    safetensors_files = [f for f in os.listdir(model_path) if f.endswith(".safetensors")]
    if safetensors_files:
        size_mb = os.path.getsize(os.path.join(model_path, safetensors_files[0])) / 1024 ** 2
        logging.info(f"âœ… æ¨¡å‹æƒé‡æ–‡ä»¶: {safetensors_files[0]}, å¤§å°: {size_mb:.2f} MB")
    logging.info(f"âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {model_path}")


# DashScope APIç¿»è¯‘
# def call_qwen_translate_api(text, target_lang="en", max_retries=3):
#     if not dashscope:
#         raise ImportError("âŒ DashScopeæœªå®‰è£…ï¼Œè¯·è¿è¡Œ 'pip install dashscope'")
#     dashscope.api_key = DASHSCOPE_API_KEY
#     TEXT_TRANSLATION_AVAILABLE = hasattr(dashscope, 'TextTranslation')
#
#     for attempt in range(max_retries):
#         logging.info(f"API ç¿»è¯‘å°è¯• {attempt + 1}/{max_retries}")
#         try:
#             if TEXT_TRANSLATION_AVAILABLE:
#                 response = dashscope.TextTranslation.call(
#                     model='qwen-max',
#                     text=text,
#                     target_language=target_lang
#                 )
#                 if response.status_code == 200:
#                     return response.output['translated_text']
#                 logging.warning(f"âš ï¸ TextTranslationå¤±è´¥: {response.message}")
#             else:
#                 source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
#                 target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
#                 prompt = (
#                     f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
#                     f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
#                     f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{text}"
#                 )
#                 response = dashscope.Generation.call(
#                     model='qwen-max',
#                     prompt=prompt,
#                     max_tokens=512,
#                     temperature=0.1
#                 )
#                 if response.status_code == 200:
#                     return response.output.get('text', '').strip()
#                 logging.warning(f"âš ï¸ Generationå¤±è´¥: {response.message}")
#         except Exception as e:
#             logging.warning(f"âš ï¸ APIè°ƒç”¨å¼‚å¸¸: {e}")
#         if attempt < max_retries - 1:
#             time.sleep(2 ** attempt)
#     logging.error(f"âŒ APIç¿»è¯‘å¤±è´¥ï¼ˆ{max_retries}æ¬¡å°è¯•ï¼‰")
#     return None


# æœ¬åœ°æ¨¡å‹ç¿»è¯‘
# def call_qwen_translate_local(model, tokenizer, text, target_lang="en", max_seq_len=64):
#     source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
#     target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
#     prompt = (
#         f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
#         f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
#         f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{text}"
#     )
#     inputs = tokenizer(
#         prompt, max_length=max_seq_len, padding="max_length", truncation=True, return_tensors="pt"
#     ).to(model.device)
#     inputs["input_ids"] = inputs["input_ids"].to(dtype=torch.long)
#     inputs["attention_mask"] = inputs["attention_mask"].to(dtype=torch.long)
#     with torch.no_grad():
#         outputs = model(**inputs)
#     return outputs.logits  # è¿”å›logits


# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class TranslationDataset(Dataset):
    """
    ç¿»è¯‘æ•°æ®é›†ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
    è¾“å‡ºå­—æ®µä¸¥æ ¼éµå¾ª DataFormat.REQUIRED_KEYS
    """

    def __init__(self, dataset, tokenizer, max_seq_len=64, src_lang="zh", tgt_lang="en", task_id=0):
        self.dataset = dataset
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.task_id = task_id

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        src_text = item["translation"][self.src_lang]
        tgt_text = item["translation"][self.tgt_lang]

        # æ„é€ ç¿»è¯‘æç¤ºè¯
        source_lang_full = "ä¸­æ–‡" if self.src_lang == "zh" else "è‹±æ–‡"
        target_lang_full = "è‹±æ–‡" if self.tgt_lang == "en" else "ä¸­æ–‡"
        prompt = (
            f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
            f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
            f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n\n{src_text}"
        )

        # åˆ†è¯
        src_encoding = self.tokenizer(
            prompt,
            max_length=self.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        tgt_encoding = self.tokenizer(
            tgt_text,
            max_length=self.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # âœ… ç»Ÿä¸€è¾“å‡ºæ ¼å¼ (éµå¾ª DataFormat)
        return {
            "id": idx,
            "src_text": src_text,
            "tgt_text": tgt_text,
            "src_input_ids": src_encoding["input_ids"].squeeze(0).to(dtype=torch.long),
            "src_attention_mask": src_encoding["attention_mask"].squeeze(0).to(dtype=torch.long),
            "tgt_input_ids": tgt_encoding["input_ids"].squeeze(0).to(dtype=torch.long),
            "tgt_attention_mask": tgt_encoding["attention_mask"].squeeze(0).to(dtype=torch.long),
            "task_id": self.task_id
        }


# ==================== è‡ªå®šä¹‰ collate_fn ====================
def custom_collate_fn(batch, max_seq_len=64, pad_token_id=151643):
    """æ‰¹æ¬¡æ•°æ®æ•´ç† (ç»Ÿä¸€æ ¼å¼)"""
    keys = ["id", "src_input_ids", "src_attention_mask", "tgt_input_ids", "tgt_attention_mask", "task_id"]
    src_texts = [item["src_text"] for item in batch]
    tgt_texts = [item["tgt_text"] for item in batch]

    # Pad åºåˆ—
    src_input_ids = torch.nn.utils.rnn.pad_sequence(
        [item["src_input_ids"] for item in batch],
        batch_first=True,
        padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)

    src_attention_mask = torch.nn.utils.rnn.pad_sequence(
        [item["src_attention_mask"] for item in batch],
        batch_first=True,
        padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)

    tgt_input_ids = torch.nn.utils.rnn.pad_sequence(
        [item["tgt_input_ids"] for item in batch],
        batch_first=True,
        padding_value=pad_token_id
    )[:, :max_seq_len].to(dtype=torch.long)

    tgt_attention_mask = torch.nn.utils.rnn.pad_sequence(
        [item["tgt_attention_mask"] for item in batch],
        batch_first=True,
        padding_value=0
    )[:, :max_seq_len].to(dtype=torch.long)

    task_ids = torch.tensor([item["task_id"] for item in batch], dtype=torch.long)
    ids = torch.tensor([item["id"] for item in batch], dtype=torch.long)

    return {
        "id": ids,
        "src_text": src_texts,
        "tgt_text": tgt_texts,
        "src_input_ids": src_input_ids,
        "src_attention_mask": src_attention_mask,
        "tgt_input_ids": tgt_input_ids,
        "tgt_attention_mask": tgt_attention_mask,
        "task_id": task_ids
    }


# éªŒè¯ logits æ–‡ä»¶
def validate_logits_file(file_path, use_api=False, model_path=None):
    """éªŒè¯ç”Ÿæˆçš„.ptæˆ–.jsonlæ–‡ä»¶ï¼ŒAPIæ¨¡å¼æ£€æŸ¥hyp_textï¼ŒéAPIæ¨¡å¼åŠ¨æ€æ£€æŸ¥logitsç»´åº¦"""
    try:
        # åŠ¨æ€è·å–è¯æ±‡è¡¨å¤§å°
        if not use_api and model_path:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path, local_files_only=True, trust_remote_code=True
            )
            config_path = os.path.join(model_path, "config.json")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            vocab_size = config.get("vocab_size", len(tokenizer))
            logging.info(f"âœ… æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        else:
            vocab_size = None

        if file_path.endswith(".jsonl"):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = [json.loads(line) for line in f]
            count = len(data)
            for item in data:
                assert "id" in item and "task_id" in item, f"ç¼ºå°‘idæˆ–task_id: {item}"
                assert "src" in item and "ref" in item and "hyp" in item, f"ç¼ºå°‘src/ref/hyp: {item}"
                assert isinstance(item["src"], str) and isinstance(item["ref"], str) and isinstance(item["hyp"],
                                                                                                    str), f"src/ref/hypç±»å‹é”™è¯¯: {item}"
            logging.info(f"âœ… éªŒè¯é€šè¿‡ï¼{file_path} å…± {count} æ¡æœ‰æ•ˆè®°å½•ï¼ˆJSONLï¼ŒAPIæ¨¡å¼ï¼‰")
        else:
            data = torch.load(file_path)
            count = len(data)
            for item in data:
                assert "id" in item and "task_id" in item, f"ç¼ºå°‘idæˆ–task_id: {item}"
                if use_api:
                    assert "hyp_text" in item and item["hyp_text"] is not None, f"APIæ¨¡å¼ç¼ºå°‘hyp_text: {item}"
                    logging.warning("âš ï¸ APIæ¨¡å¼ï¼šlogitsä¸ºNoneï¼Œä»…ä¿å­˜ç¿»è¯‘æ–‡æœ¬")
                else:
                    assert "logits" in item and item["logits"] is not None, f"ç¼ºå°‘logits: {item}"
                    if vocab_size:
                        if item["logits"].shape[-1] != vocab_size:
                            logging.warning(
                                f"âš ï¸ logitsç»´åº¦ä¸åŒ¹é… (é¢„æœŸ: {vocab_size}, å®é™…: {item['logits'].shape[-1]})")
                        else:
                            logging.debug(f"âœ… logitsç»´åº¦éªŒè¯é€šè¿‡: {item['logits'].shape}")
                    else:
                        logging.debug(f"âš ï¸ æœªæä¾›model_pathï¼Œè·³è¿‡logitsç»´åº¦æ£€æŸ¥ï¼Œå®é™…ç»´åº¦: {item['logits'].shape}")
            logging.info(f"âœ… éªŒè¯é€šè¿‡ï¼{file_path} å…± {count} æ¡æœ‰æ•ˆè®°å½•ï¼ˆPTï¼Œ{'APIæ¨¡å¼' if use_api else 'æœ¬åœ°æ¨¡å¼'}ï¼‰")
        return count
    except Exception as e:
        logging.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return 0

# # ==================== ç”Ÿæˆ Teacher Logits (ä¸»å‡½æ•°) ====================
# def generate_teacher_logits(args):
#     """
#     å¢å¼ºç‰ˆï¼šç”Ÿæˆ Teacher æ¨¡å‹ logits æ–‡ä»¶
#     âœ… åŠ å…¥æ˜¾å­˜ä¼˜åŒ– / è‡ªåŠ¨ batch_size å›é€€ / æ­»é”é˜²æŠ¤ / åŠç²¾åº¦æ”¯æŒ
#     """
#     process = psutil.Process()
#     logging.info(f"åˆå§‹å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")
#
#     # ===== 1. åŠ è½½ tokenizer =====
#     try:
#         tokenizer = AutoTokenizer.from_pretrained(
#             MODEL_PATH, local_files_only=True, trust_remote_code=True
#         )
#         logging.info("âœ… æˆåŠŸåŠ è½½ Qwen Tokenizer")
#     except Exception as e:
#         logging.error(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
#         raise
#
#     # ===== 2. åŠ è½½æ¨¡å‹ =====
#     model = None
#     device = torch.device(args.device)
#     if not args.use_api:
#         try:
#             check_model_files(MODEL_PATH)
#             logging.info(f"ğŸ“¥ åŠ è½½ Qwen æ¨¡å‹åˆ° {device}...")
#
#             try:
#                 model = AutoModelForCausalLM.from_pretrained(
#                     MODEL_PATH,
#                     device_map=device,
#                     torch_dtype=torch.float32,  # âœ… æ”¹ä¸ºä¼˜å…ˆä½¿ç”¨ FP32 ç²¾åº¦
#                     local_files_only=True,
#                     trust_remote_code=True,
#                     low_cpu_mem_usage=False,  # FP32 æ¨¡å¼ä¸‹ç¦ç”¨ä½å†…å­˜åŠ è½½ï¼Œé¿å…æˆªæ–­
#                     use_safetensors=True
#                 )
#                 logging.info("âœ… æˆåŠŸåŠ è½½æ¨¡å‹ (float32 å…¨ç²¾åº¦)")
#             except Exception as e:
#                 logging.warning(f"âš ï¸ åŠ è½½ float32 æ¨¡å‹å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ float16: {e}")
#                 model = AutoModelForCausalLM.from_pretrained(
#                     MODEL_PATH,
#                     device_map=device,
#                     torch_dtype=torch.float16,
#                     local_files_only=True,
#                     trust_remote_code=True,
#                     low_cpu_mem_usage=True,
#                     use_safetensors=True
#                 )
#                 logging.info("âœ… å›é€€åˆ° float16 åŠç²¾åº¦æ¨¡å‹")
#
#             model.eval()
#
#             # âœ… å¯ç”¨ TF32 + cuDNN benchmark
#             if device.type == "cuda":
#                 torch.backends.cuda.matmul.allow_tf32 = True
#                 torch.backends.cudnn.benchmark = True
#                 logging.info("ğŸ’¡ å¯ç”¨ TF32 ä¸ cuDNN Benchmark ä»¥ä¼˜åŒ– CUDA ç¨³å®šæ€§")
#                 logging.info("ğŸ“Š åˆå§‹CUDAå†…å­˜æ‘˜è¦ï¼š")
#                 logging.info(torch.cuda.memory_summary(device=device, abbreviated=True))
#         except Exception as e:
#             logging.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
#             raise
#
#     # ===== 3. åŠ è½½æ•°æ®é›† =====
#     try:
#         dataset_path = os.path.join(RAW_DATA_PATH, "train/*.parquet")
#         dataset = load_dataset("parquet", data_files={"train": dataset_path})["train"]
#         total_samples = len(dataset)
#         logging.info(f"âœ… åŠ è½½WMT19æ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {total_samples}")
#     except Exception as e:
#         logging.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
#         raise
#
#     total_samples = min(args.max_samples or total_samples, total_samples)
#     shard_size = min(args.shard_size, total_samples)
#
#     success_count, fail_count = 0, 0
#
#     # ===== 4. åŒå‘ç¿»è¯‘ä»»åŠ¡ =====
#     for src_lang, tgt_lang, task_id, output_prefix in [
#         ("zh", "en", 0, os.path.join(TEACHER_LOGITS_DIR, "zh_to_en")),
#         ("en", "zh", 1, os.path.join(TEACHER_LOGITS_DIR, "en_to_zh"))
#     ]:
#         logging.info(f"ğŸ§  ç”Ÿæˆ {src_lang}â†’{tgt_lang} logits (ä»»åŠ¡ID: {task_id})")
#
#         start_idx = args.start_from
#         end_idx = min(start_idx + args.shard_size, total_samples)
#         shard_dataset = dataset.select(range(start_idx, end_idx))
#         if len(shard_dataset) == 0:
#             logging.warning(f"âš ï¸ åˆ†ç‰‡ {args.shard_idx} æ— æ ·æœ¬ï¼Œè·³è¿‡ã€‚")
#             continue
#
#         # æ„å»º DataLoader
#         shard_dataloader = DataLoader(
#             TranslationDataset(
#                 shard_dataset, tokenizer,
#                 max_seq_len=args.max_seq_len,
#                 src_lang=src_lang, tgt_lang=tgt_lang, task_id=task_id
#             ),
#             batch_size=args.batch_size,
#             shuffle=False,
#             num_workers=0,
#             pin_memory=(device == "cuda"),
#             collate_fn=lambda b: custom_collate_fn(
#                 b, max_seq_len=args.max_seq_len, pad_token_id=tokenizer.pad_token_id
#             )
#         )
#
#         output_file = f"{output_prefix}_shard_{args.shard_idx}.{'jsonl' if args.use_api else 'pt'}"
#         output_data = []
#
#         # == == = 5. ä¸»å¾ªç¯ == == =
#         with torch.no_grad():
#             for batch_idx, batch in enumerate(
#                     tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {args.shard_idx}")
#             ):
#                 batch_start = time.time()
#                 try:
#                     src_input_ids = batch["src_input_ids"].to(device)
#                     src_attention_mask = batch["src_attention_mask"].to(device)
#                     tgt_input_ids = batch["tgt_input_ids"].to(device)
#                     tgt_attention_mask = batch["tgt_attention_mask"].to(device)
#
#                     # ============================================================
#                     # âœ… 1ï¸âƒ£ åŒºåˆ†æ¨¡å‹ç±»å‹
#                     # ============================================================
#                     model_type = getattr(model.config, "model_type", "").lower()
#
#                     if "qwen" in model_type or "llama" in model_type or "mistral" in model_type:
#                         # ============================================================
#                         # âœ… CausalLM å‹ (å¦‚ Qwen)ï¼šæ‹¼æ¥ src+tgtï¼Œæ‰‹åŠ¨shift labels
#                         # ============================================================
#                         input_ids = torch.cat([src_input_ids, tgt_input_ids], dim=1)
#                         attention_mask = torch.cat([src_attention_mask, tgt_attention_mask], dim=1)
#
#                         # æ„é€  labelsï¼Œä½¿å¾—æ¨¡å‹åªé¢„æµ‹ target æ®µ
#                         labels = input_ids.clone()
#                         labels[:, :src_input_ids.size(1)] = -100  # å¿½ç•¥æºå¥éƒ¨åˆ†çš„loss
#
#                         with torch.cuda.amp.autocast(dtype=torch.float16 if device.type == "cuda" else torch.float32):
#                             outputs = model(
#                                 input_ids=input_ids,
#                                 attention_mask=attention_mask,
#                                 labels=labels,
#                                 output_hidden_states=False,
#                                 output_attentions=False,
#                             )
#                             full_logits = outputs.logits  # [batch, total_len, vocab_size]
#                             # ä»…å– target æ®µ logits
#                             logits = full_logits[:, -tgt_input_ids.size(1):, :].detach().cpu()
#
#                     elif "marian" in model_type or "opus" in model_type or "t5" in model_type:
#                         # ============================================================
#                         # âœ… Seq2Seq å‹ (å¦‚ Opus-MT / MarianMT / T5)
#                         # ============================================================
#                         with torch.cuda.amp.autocast(dtype=torch.float16 if device.type == "cuda" else torch.float32):
#                             outputs = model(
#                                 input_ids=src_input_ids,
#                                 attention_mask=src_attention_mask,
#                                 labels=tgt_input_ids,
#                                 output_hidden_states=False,
#                                 output_attentions=False,
#                             )
#                             logits = outputs.logits.detach().cpu()  # [batch, tgt_len, vocab_size]
#                     else:
#                         raise ValueError(f"âŒ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
#                     # ============================================================
#
#                     # é‡Šæ”¾æ˜¾å­˜
#                     del outputs
#                     torch.cuda.empty_cache()
#                     gc.collect()
#
#                     for i in range(len(batch["id"])):
#                         output_data.append({
#                             "id": batch["id"][i].item(),
#                             "src_text": batch["src_text"][i],
#                             "tgt_text": batch["tgt_text"][i],
#                             "src_input_ids": batch["src_input_ids"][i].cpu(),
#                             "src_attention_mask": batch["src_attention_mask"][i].cpu(),
#                             "tgt_input_ids": batch["tgt_input_ids"][i].cpu(),
#                             "tgt_attention_mask": batch["tgt_attention_mask"][i].cpu(),
#                             "task_id": batch["task_id"][i].item(),
#                             "logits": logits[i]
#                         })
#                     success_count += len(batch["id"])
#
#                     # æ¯ 10 æ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢è¿‡å¤§
#                     if (batch_idx + 1) % 10 == 0:
#                         torch.save(output_data, output_file)
#                         output_data.clear()
#                         gc.collect()
#                         torch.cuda.empty_cache()
#                         logging.info(f"ğŸ’¾ ä¸´æ—¶ä¿å­˜ {output_file} (åˆ°ç¬¬ {batch_idx + 1} æ‰¹æ¬¡)")
#
#                     # â±ï¸ è¶…æ—¶æ£€æµ‹ watchdog
#                     elapsed = time.time() - batch_start
#                     if elapsed > 120:
#                         logging.warning(f"âš ï¸ Batch {batch_idx} è¶…æ—¶ {elapsed:.1f}sï¼Œå¼ºåˆ¶æ¸…ç†CUDAä¸Šä¸‹æ–‡")
#                         torch.cuda.empty_cache()
#                         gc.collect()
#
#                 except torch.cuda.OutOfMemoryError:
#                     logging.error(f"ğŸ’¥ CUDA OOM at batch {batch_idx}! è‡ªåŠ¨å›é€€ batch_size...")
#                     torch.cuda.empty_cache()
#                     if args.batch_size > 1:
#                         args.batch_size = max(1, args.batch_size // 2)
#                         logging.warning(f"âš™ï¸ æ–° batch_size={args.batch_size}ï¼Œé‡æ–°æ„å»º DataLoader")
#                         shard_dataloader = DataLoader(
#                             TranslationDataset(
#                                 shard_dataset, tokenizer,
#                                 max_seq_len=args.max_seq_len,
#                                 src_lang=src_lang, tgt_lang=tgt_lang, task_id=task_id
#                             ),
#                             batch_size=args.batch_size,
#                             shuffle=False,
#                             num_workers=0,
#                             pin_memory=(device == "cuda"),
#                             collate_fn=lambda b: custom_collate_fn(
#                                 b, max_seq_len=args.max_seq_len, pad_token_id=tokenizer.pad_token_id
#                             )
#                         )
#                         break
#                     else:
#                         logging.error("âŒ batch_size=1 ä»æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥åˆ†ç‰‡ã€‚")
#                         break
#
#                 except RuntimeError as e:
#                     logging.error(f"âš ï¸ RuntimeError (å¯èƒ½æ­»é”æˆ–é©±åŠ¨é”™è¯¯): {e}")
#                     torch.cuda.empty_cache()
#                     gc.collect()
#                     time.sleep(2)
#                     continue
#
#                 except Exception as e:
#                     logging.error(f"âŒ Batch {batch_idx} å¤„ç†å¤±è´¥: {e}")
#                     torch.cuda.empty_cache()
#                     gc.collect()
#                     continue
#
#         # ===== 6. ä¿å­˜ç»“æœå¹¶éªŒè¯ =====
#         if len(output_data) > 0:
#             torch.save(output_data, output_file)
#         logging.info(f"ğŸ’¾ ä¿å­˜åˆ†ç‰‡: {output_file}")
#         validate_logits_file(output_file)
#         del output_data
#         gc.collect()
#         torch.cuda.empty_cache()
#
#     # ===== 7. æ¸…ç† =====
#     if model is not None:
#         del model
#         gc.collect()
#         torch.cuda.empty_cache()
#
#     logging.info(f"ğŸ‰ å®Œæˆï¼æˆåŠŸ: {success_count} å¤±è´¥: {fail_count}")
#     return success_count, fail_count
# ==================== ç”Ÿæˆ Teacher Logits (åŸºäº Opus-MT æ•™å¸ˆæ¨¡å‹) ====================
def generate_teacher_logits(args):
    """
    âœ… Opus-MT æ•™å¸ˆæ¨¡å‹ç”Ÿæˆ soft logits
    æ”¯æŒ zhâ†’en å’Œ enâ†’zh ä¸¤ä¸ªæ–¹å‘
    è‡ªåŠ¨åˆ‡æ¢æ¨¡å‹è·¯å¾„ + æ˜¾å­˜é˜²æŠ¤ + åŠç²¾åº¦æ”¯æŒ
    """
    process = psutil.Process()
    logging.info(f"åˆå§‹å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")

    device = torch.device(args.device)

    # ===== 1. åŠ è½½æ•°æ®é›† =====
    try:
        dataset_path = os.path.join(RAW_DATA_PATH, "train/*.parquet")
        dataset = load_dataset("parquet", data_files={"train": dataset_path})["train"]
        total_samples = len(dataset)
        logging.info(f"âœ… åŠ è½½ WMT19 æ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {total_samples}")
    except Exception as e:
        logging.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        raise

    total_samples = min(args.max_samples or total_samples, total_samples)

    success_count, fail_count = 0, 0

    # ===== 2. éå†ä¸¤ä¸ªç¿»è¯‘æ–¹å‘ =====
    for src_lang, tgt_lang, task_id, output_prefix, model_path in [
        ("zh", "en", 0, os.path.join(TEACHER_LOGITS_DIR, "zh_to_en"), OPUS_MT_ZH_EN),
        ("en", "zh", 1, os.path.join(TEACHER_LOGITS_DIR, "en_to_zh"), OPUS_MT_EN_ZH),
    ]:
        logging.info(f"ğŸ§  ç”Ÿæˆ {src_lang}â†’{tgt_lang} logits (ä»»åŠ¡ID={task_id})")

        model = None
        tokenizer = None

        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
                local_files_only=True
            ).to(device)
            model.eval()
            logging.info(f"âœ… æˆåŠŸåŠ è½½ Opus æ¨¡å‹ ({src_lang}â†’{tgt_lang})")
        except Exception as e:
            logging.error(f"âŒ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥ ({src_lang}->{tgt_lang}): {e}")
            continue

        if model is None or tokenizer is None:
            logging.error(f"âš ï¸ æœªèƒ½åˆå§‹åŒ–æ¨¡å‹æˆ– tokenizer ({src_lang}->{tgt_lang})ï¼Œè·³è¿‡æ­¤æ–¹å‘ã€‚")
            continue

        # ===== 2.2 é€‰å–åˆ†ç‰‡ =====
        start_idx = args.start_from
        end_idx = min(start_idx + args.shard_size, total_samples)
        shard_dataset = dataset.select(range(start_idx, end_idx))
        if len(shard_dataset) == 0:
            logging.warning(f"âš ï¸ åˆ†ç‰‡ {args.shard_idx} æ— æ ·æœ¬ï¼Œè·³è¿‡ã€‚")
            continue

        pad_id = tokenizer.pad_token_id

        shard_dataloader = DataLoader(
            TranslationDataset(
                shard_dataset, tokenizer,
                max_seq_len=args.max_seq_len,
                src_lang=src_lang, tgt_lang=tgt_lang, task_id=task_id
            ),
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=(device == "cuda"),
            collate_fn=lambda b, pad_id=pad_id: custom_collate_fn(
                b, max_seq_len=args.max_seq_len, pad_token_id=pad_id
            )
        )

        output_file = f"{output_prefix}_shard_{args.shard_idx}.pt"
        output_data = []

        # ===== 3. ä¸»ç”Ÿæˆå¾ªç¯ =====
        with torch.no_grad():
            for batch_idx, batch in enumerate(
                tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {args.shard_idx}")
            ):
                batch_start = time.time()
                try:
                    src_input_ids = batch["src_input_ids"].to(device)
                    src_attention_mask = batch["src_attention_mask"].to(device)
                    tgt_input_ids = batch["tgt_input_ids"].to(device)

                    # ğŸ”¹ Opus-MT / MarianMT æ˜¯æ ‡å‡† encoder-decoder æ¨¡å‹
                    with torch.amp.autocast(device_type=device.type, dtype=torch.float16 if device.type == "cuda" else torch.float32):
                        outputs = model(
                            input_ids=src_input_ids,
                            attention_mask=src_attention_mask,
                            labels=tgt_input_ids,
                            output_hidden_states=False,
                            output_attentions=False,
                        )
                        logits = outputs.logits.detach().cpu()

                    for i in range(len(batch["id"])):
                        output_data.append({
                            "id": batch["id"][i].item(),
                            "src_text": batch["src_text"][i],
                            "tgt_text": batch["tgt_text"][i],
                            "src_input_ids": batch["src_input_ids"][i].cpu(),
                            "src_attention_mask": batch["src_attention_mask"][i].cpu(),
                            "tgt_input_ids": batch["tgt_input_ids"][i].cpu(),
                            "tgt_attention_mask": batch["tgt_attention_mask"][i].cpu(),
                            "task_id": batch["task_id"][i].item(),
                            "logits": logits[i]
                        })
                    success_count += len(batch["id"])

                    # æ¯10æ‰¹æ¬¡ä¸­é—´ä¿å­˜ä¸€æ¬¡
                    if (batch_idx + 1) % 10 == 0:
                        torch.save(output_data, output_file)
                        output_data.clear()
                        torch.cuda.empty_cache()
                        gc.collect()
                        logging.info(f"ğŸ’¾ ä¸´æ—¶ä¿å­˜ {output_file} (åˆ°ç¬¬ {batch_idx + 1} æ‰¹æ¬¡)")

                    # è¶…æ—¶ watchdog
                    elapsed = time.time() - batch_start
                    if elapsed > 120:
                        logging.warning(f"âš ï¸ Batch {batch_idx} è¶…æ—¶ {elapsed:.1f}sï¼Œå¼ºåˆ¶æ¸…ç†CUDAä¸Šä¸‹æ–‡")
                        torch.cuda.empty_cache()
                        gc.collect()

                except torch.cuda.OutOfMemoryError:
                    logging.error(f"ğŸ’¥ CUDA OOM at batch {batch_idx}! è‡ªåŠ¨å›é€€ batch_size...")
                    torch.cuda.empty_cache()
                    if args.batch_size > 1:
                        args.batch_size = max(1, args.batch_size // 2)
                        break
                    else:
                        logging.error("âŒ batch_size=1 ä»æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥åˆ†ç‰‡ã€‚")
                        break

                except Exception as e:
                    logging.error(f"âŒ Batch {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    torch.cuda.empty_cache()
                    gc.collect()
                    continue

        # ===== 4. ä¿å­˜åˆ†ç‰‡ç»“æœ =====
        if len(output_data) > 0:
            torch.save(output_data, output_file)
        logging.info(f"ğŸ’¾ ä¿å­˜åˆ†ç‰‡: {output_file}")
        validate_logits_file(output_file)
        del output_data
        gc.collect()
        torch.cuda.empty_cache()

    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()

    logging.info(f"ğŸ‰ ç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count} å¤±è´¥: {fail_count}")
    return success_count, fail_count


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ Teacher Logits (ä¼˜åŒ–ç‰ˆ)")
    parser.add_argument("--dataset_path", type=str, default="data/raw/wmt19_zh_en", help="WMT19è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹å¤§å°")
    parser.add_argument("--max_seq_len", type=int, default=ModelConfig.MAX_SEQ_LEN, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--max_samples", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--start_from", type=int, default=0, help="èµ·å§‹ç´¢å¼•")
    parser.add_argument("--shard_size", type=int, default=100000, help="åˆ†ç‰‡å¤§å°")
    parser.add_argument("--compile", action="store_true", help="torch.compile")
    parser.add_argument("--int8", action="store_true", help="INT8é‡åŒ–")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--use_api", action="store_true", help="ä½¿ç”¨API")
    parser.add_argument("--simulate_quant_noise", action="store_true",
                        help="æ˜¯å¦åœ¨ç”Ÿæˆ logits æ—¶åŠ å…¥æ¨¡æ‹Ÿé‡åŒ–è¯¯å·®ï¼ˆå¢å¼ºå­¦ç”Ÿé²æ£’æ€§ï¼‰")
    parser.add_argument("--noise_std", type=float, default=0.01,
                        help="æ¨¡æ‹Ÿé‡åŒ–å™ªå£°æ ‡å‡†å·® (é»˜è®¤ 0.01)")
    parser.add_argument("--shard_idx", type=int, default=0, help="å½“å‰åˆ†ç‰‡ç´¢å¼•ï¼ˆç”¨äºå‘½åï¼‰")
    args = parser.parse_args()

    try:
        success, fail = generate_teacher_logits(args)
        logging.info(f"ğŸ‰ å¤„ç†å®Œæˆï¼æˆåŠŸ: {success}, å¤±è´¥: {fail}")
    except Exception as e:
        logging.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        raise
