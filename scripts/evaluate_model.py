# scripts/evaluate_model.py
import os
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
import torch.nn as nn
import psutil
import time
import logging
import sacrebleu
from transformers import AutoTokenizer
from datasets import load_dataset

from models.tiny_transformer import TinyTransformer
from config.config import (
    ModelConfig, EvalConfig, LogConfig,
    MODEL_PATH, RAW_DATA_PATH
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format=LogConfig.LOG_FORMAT,
    handlers=[
        logging.FileHandler(LogConfig.EVALUATE_LOG, encoding='utf-8'),
        logging.StreamHandler()
    ]
)


# ==================== è¾…åŠ©å‡½æ•° ====================
def get_memory_usage():
    """è·å–å½“å‰è¿›ç¨‹å†…å­˜å ç”¨ (MB)"""
    process = psutil.Process()
    return process.memory_info().rss / 1024 / 1024


def load_test_dataset(test_data_path, tasks=None, max_samples=100):
    """
    åŠ è½½æµ‹è¯•æ•°æ®é›† (æ”¯æŒè‡ªå®šä¹‰è·¯å¾„)

    :param test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„ (parquetæ–‡ä»¶)
    :param tasks: ä»»åŠ¡åˆ—è¡¨ ["zh_to_en", "en_to_zh"]
    :param max_samples: æ¯ä¸ªä»»åŠ¡çš„æœ€å¤§æ ·æœ¬æ•°
    :return: {task: [{"translation": {"zh": ..., "en": ...}}]}
    """
    tasks = tasks or EvalConfig.TASKS
    datasets = {}

    try:
        # åŠ è½½ parquet æ–‡ä»¶
        dataset = load_dataset("parquet", data_files={"test": test_data_path})["test"]
        dataset = dataset.select(range(min(max_samples, len(dataset))))

        logging.info(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®é›†: {test_data_path} (æ ·æœ¬æ•°: {len(dataset)})")

        # æŒ‰ä»»åŠ¡åˆ†ç»„
        if "translation" in dataset.column_names:
            for task in tasks:
                src_lang, tgt_lang = task.split('_to_')
                task_dataset = [
                    {"translation": {src_lang: item["translation"][src_lang], tgt_lang: item["translation"][tgt_lang]}}
                    for item in dataset
                    if src_lang in item["translation"] and tgt_lang in item["translation"]
                ]
                datasets[task] = task_dataset
                logging.info(f"  - {task}: {len(task_dataset)} æ¡")
        else:
            raise ValueError(f"æ•°æ®é›†æ ¼å¼æœªçŸ¥: {dataset.column_names}")

    except Exception as e:
        logging.error(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        raise

    return datasets


def compute_bleu(model, tokenizer, dataset, task="zh_to_en", device="cpu", max_seq_len=64):
    """
    è®¡ç®— BLEU åˆ†æ•°

    :param model: å­¦ç”Ÿæ¨¡å‹
    :param tokenizer: Tokenizer
    :param dataset: æµ‹è¯•æ•°æ®é›†
    :param task: ä»»åŠ¡åç§°
    :param device: è®¡ç®—è®¾å¤‡
    :param max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
    :return: BLEU åˆ†æ•°
    """
    model.eval()
    refs, hyps = [], []
    task_id = 0 if task == "zh_to_en" else 1

    for item in dataset:
        src_lang, tgt_lang = task.split('_to_')
        src_text = item["translation"][src_lang]
        ref_text = item["translation"][tgt_lang]

        try:
            # åˆ†è¯
            inputs = tokenizer(
                src_text,
                return_tensors="pt",
                truncation=True,
                max_length=max_seq_len,
                padding=True
            )
            input_ids = inputs["input_ids"].to(device)
            attention_mask = inputs.get("attention_mask", torch.ones_like(input_ids)).to(device)
            task_ids = torch.tensor([task_id], dtype=torch.long, device=device)

            # æ¨ç†
            with torch.no_grad():
                outputs = model(input_ids, task_id=task_ids, attention_mask=attention_mask)

            # è§£ç 
            hyp_text = tokenizer.decode(
                outputs["logits"].argmax(dim=-1)[0],
                skip_special_tokens=True
            )

            refs.append([ref_text])
            hyps.append(hyp_text)

        except Exception as e:
            logging.error(f"âŒ ç¿»è¯‘å¤±è´¥ ({task}, æ–‡æœ¬: {src_text[:50]}...): {e}")
            continue

    # è®¡ç®— BLEU
    try:
        bleu = sacrebleu.corpus_bleu(hyps, refs)
        return bleu.score
    except Exception as e:
        logging.error(f"âŒ è®¡ç®— BLEU å¤±è´¥: {e}")
        return 0.0


def measure_inference_latency(model, tokenizer, dataset, task="zh_to_en", device="cpu", max_seq_len=64,
                              num_samples=100):
    """
    æµ‹é‡æ¨ç†å»¶è¿Ÿ

    :param model: å­¦ç”Ÿæ¨¡å‹
    :param tokenizer: Tokenizer
    :param dataset: æµ‹è¯•æ•°æ®é›†
    :param task: ä»»åŠ¡åç§°
    :param device: è®¡ç®—è®¾å¤‡
    :param max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
    :param num_samples: æµ‹è¯•æ ·æœ¬æ•°
    :return: å¹³å‡å»¶è¿Ÿ (ms)
    """
    model.eval()
    latencies = []
    task_id = 0 if task == "zh_to_en" else 1

    for i, item in enumerate(dataset):
        if i >= num_samples:
            break

        src_lang = task.split('_to_')[0]
        src_text = item["translation"][src_lang]

        try:
            inputs = tokenizer(
                src_text,
                return_tensors="pt",
                truncation=True,
                max_length=max_seq_len,
                padding=True
            )
            input_ids = inputs["input_ids"].to(device)
            attention_mask = inputs.get("attention_mask", torch.ones_like(input_ids)).to(device)
            task_ids = torch.tensor([task_id], dtype=torch.long, device=device)

            # æµ‹é‡æ—¶é—´
            start_time = time.time()
            with torch.no_grad():
                _ = model(input_ids, task_id=task_ids, attention_mask=attention_mask)
            latencies.append(time.time() - start_time)

        except Exception as e:
            logging.error(f"âŒ æ¨ç†å¤±è´¥ ({task}, æ–‡æœ¬: {src_text[:50]}...): {e}")
            continue

    if not latencies:
        logging.error(f"âŒ æ— æœ‰æ•ˆæ¨ç†æ•°æ® ({task})")
        return float('inf')

    return sum(latencies) / len(latencies) * 1000  # è½¬æ¢ä¸º ms


# ==================== ä¸»è¯„ä¼°å‡½æ•° ====================
def main(
        model_path,
        is_int8=False,
        tasks=None,
        max_samples=None,
        test_data_path=None,
        device="cpu"
):
    """
    è¯„ä¼°å­¦ç”Ÿæ¨¡å‹ (æ”¯æŒä¸­è‹±äº’è¯‘)

    :param model_path: æ¨¡å‹æƒé‡è·¯å¾„
    :param is_int8: æ˜¯å¦ INT8 æ¨¡å‹
    :param tasks: ä»»åŠ¡åˆ—è¡¨
    :param max_samples: æ¯ä»»åŠ¡æµ‹è¯•æ ·æœ¬æ•°
    :param test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„ (å¯é€‰)
    :param device: è®¡ç®—è®¾å¤‡
    """
    # é»˜è®¤å€¼
    tasks = tasks or EvalConfig.TASKS
    max_samples = max_samples or EvalConfig.MAX_EVAL_SAMPLES
    test_data_path = test_data_path or EvalConfig.TEST_DATA_PATH

    logging.info("=" * 60)
    logging.info("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°")
    logging.info("=" * 60)
    logging.info(f"ğŸ“¦ æ¨¡å‹è·¯å¾„: {model_path}")
    logging.info(f"ğŸ§ª æµ‹è¯•æ•°æ®: {test_data_path}")
    logging.info(f"ğŸ“Š ä»»åŠ¡: {tasks}")
    logging.info(f"ğŸ’» è®¾å¤‡: {device}")

    # 1. åŠ è½½ tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, local_files_only=True, trust_remote_code=True
        )
        logging.info("âœ… Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        logging.error(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
        raise

    # 2. åŠ è½½æ¨¡å‹
    try:
        model = TinyTransformer(
            vocab_size=ModelConfig.VOCAB_SIZE,
            max_seq_len=ModelConfig.MAX_SEQ_LEN,
            **ModelConfig.CURRENT_CONFIG
        ).to(device)

        # INT8 é‡åŒ–
        if is_int8:
            model = torch.quantization.quantize_dynamic(
                model, {nn.Embedding, nn.Linear}, dtype=torch.qint8
            )
            logging.info("âœ… INT8 é‡åŒ–å·²åº”ç”¨")

        # åŠ è½½æƒé‡
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict, strict=not is_int8)
        model.eval()

        logging.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ ({'INT8' if is_int8 else 'FP32'})")
    except Exception as e:
        logging.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

    # 3. åŠ è½½æµ‹è¯•æ•°æ®
    datasets = load_test_dataset(test_data_path, tasks, max_samples)

    # 4. å†…å­˜å ç”¨
    mem_usage = get_memory_usage()
    logging.info(f"ğŸ“Š å†…å­˜å ç”¨: {mem_usage:.2f} MB")

    # 5. è¯„ä¼°å„ä»»åŠ¡
    results = {}
    for task in tasks:
        logging.info(f"\n{'=' * 60}")
        logging.info(f"ğŸ“ è¯„ä¼°ä»»åŠ¡: {task}")
        logging.info(f"{'=' * 60}")

        # BLEU åˆ†æ•°
        bleu_score = compute_bleu(
            model, tokenizer, datasets[task],
            task=task, device=device, max_seq_len=ModelConfig.MAX_SEQ_LEN
        )
        logging.info(f"  BLEU åˆ†æ•°: {bleu_score:.2f}")

        # æ¨ç†å»¶è¿Ÿ
        avg_latency = measure_inference_latency(
            model, tokenizer, datasets[task],
            task=task, device=device, max_seq_len=ModelConfig.MAX_SEQ_LEN,
            num_samples=min(100, len(datasets[task]))
        )
        logging.info(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ms")

        results[task] = {
            "bleu": bleu_score,
            "latency_ms": avg_latency
        }

    # 6. æ€§èƒ½æ€»ç»“
    logging.info(f"\n{'=' * 60}")
    logging.info("ğŸ“Š è¯„ä¼°æ€»ç»“")
    logging.info(f"{'=' * 60}")
    logging.info(f"å†…å­˜å ç”¨: {mem_usage:.2f} MB (ç›®æ ‡: <{EvalConfig.TARGET_MEMORY_MB} MB)")
    for task, metrics in results.items():
        logging.info(f"{task}:")
        logging.info(f"  - BLEU: {metrics['bleu']:.2f} (ç›®æ ‡: >{EvalConfig.TARGET_BLEU})")
        logging.info(f"  - å»¶è¿Ÿ: {metrics['latency_ms']:.2f} ms (ç›®æ ‡: <{EvalConfig.TARGET_LATENCY_MS} ms)")

    # 7. æ€§èƒ½è¾¾æ ‡æ£€æŸ¥
    all_pass = True
    if mem_usage > EvalConfig.TARGET_MEMORY_MB:
        logging.warning(f"âš ï¸ å†…å­˜è¶…æ ‡: {mem_usage:.2f} MB > {EvalConfig.TARGET_MEMORY_MB} MB")
        all_pass = False

    for task, metrics in results.items():
        if metrics['bleu'] < EvalConfig.TARGET_BLEU:
            logging.warning(f"âš ï¸ {task} BLEU æœªè¾¾æ ‡: {metrics['bleu']:.2f} < {EvalConfig.TARGET_BLEU}")
            all_pass = False
        if metrics['latency_ms'] > EvalConfig.TARGET_LATENCY_MS:
            logging.warning(f"âš ï¸ {task} å»¶è¿Ÿè¶…æ ‡: {metrics['latency_ms']:.2f} ms > {EvalConfig.TARGET_LATENCY_MS} ms")
            all_pass = False

    if all_pass:
        logging.info("\nâœ… æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡ï¼")
    else:
        logging.info("\nâš ï¸ éƒ¨åˆ†æ€§èƒ½æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œéœ€è¦ä¼˜åŒ–")

    return results


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="è¯„ä¼°å­¦ç”Ÿæ¨¡å‹ï¼ˆæ”¯æŒä¸­è‹±äº’è¯‘ï¼‰")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument("--is_int8", action="store_true", help="æ˜¯å¦ INT8 æ¨¡å‹")
    parser.add_argument("--tasks", type=str, nargs='+', default=EvalConfig.TASKS, help="ä»»åŠ¡åˆ—è¡¨")
    parser.add_argument("--max_samples", type=int, default=EvalConfig.MAX_EVAL_SAMPLES, help="æ¯ä»»åŠ¡æµ‹è¯•æ ·æœ¬æ•°")
    parser.add_argument("--test_data_path", type=str, default=EvalConfig.TEST_DATA_PATH, help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--device", type=str, default="cpu", help="è®¡ç®—è®¾å¤‡")
    args = parser.parse_args()

    main(
        model_path=args.model_path,
        is_int8=args.is_int8,
        tasks=args.tasks,
        max_samples=args.max_samples,
        test_data_path=args.test_data_path,
        device=args.device
    )
