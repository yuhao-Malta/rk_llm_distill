#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
check_student_alignment.py
ç”¨äºæ£€æŸ¥å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹åœ¨ tokenizerã€embeddingã€è¾“å‡ºä¸Šçš„å¯¹é½æƒ…å†µã€‚
"""
import os
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
from models.tiny_seq2seq_transformer import TinySeq2SeqTransformer
from config.config import ModelConfig, EvalConfig, MODEL_PATH, OPUS_MT_ZH_EN, OPUS_MT_EN_ZH

# ==== 1ï¸âƒ£ è·¯å¾„é…ç½® ====
TEACHER_PATH = OPUS_MT_ZH_EN  # â† æ”¹æˆ teacher æ¨¡å‹ç›®å½•
STUDENT_MODEL_PATH = "outputs/models/student_model_final_merged.pth"
STUDENT_TOKENIZER_PATH = OPUS_MT_ZH_EN  # æˆ– "outputs/tokenizer"

# ==== 2ï¸âƒ£ åŠ è½½ Tokenizer ====
print("\nğŸ” Loading tokenizers ...")
teacher_tok = AutoTokenizer.from_pretrained(TEACHER_PATH, trust_remote_code=True)
student_tok = AutoTokenizer.from_pretrained(STUDENT_TOKENIZER_PATH, trust_remote_code=True)

print(f"Teacher vocab size: {len(teacher_tok)}")
print(f"Student vocab size: {len(student_tok)}")

if len(teacher_tok) != len(student_tok):
    print(f"âš ï¸ è¯è¡¨å¤§å°ä¸åŒ¹é…: teacher={len(teacher_tok)}, student={len(student_tok)}")

print("\nğŸ§© å‰20ä¸ªtokenå¯¹é½æ£€æŸ¥ï¼š")
for i in range(20):
    t_tok = teacher_tok.convert_ids_to_tokens(i)
    s_tok = student_tok.convert_ids_to_tokens(i)
    marker = "âœ…" if t_tok == s_tok else "âŒ"
    print(f"{i:>5}: {t_tok:<15} | {s_tok:<15} {marker}")

# # å¦‚æœ tokenizer æ²¡æœ‰ bos/eosï¼Œå°±æŒ‡å®šåˆç†çš„é»˜è®¤å€¼
# if student_tok.bos_token_id is None:
#     student_tok.bos_token = "<s>"
#     student_tok.bos_token_id = 151642  # ä½ å¯ä»¥æŸ¥çœ‹ tokenizer.vocab_size é™„è¿‘çš„ä¿ç•™ç¬¦å·
# if student_tok.eos_token_id is None:
#     student_tok.eos_token = "</s>"
#     student_tok.eos_token_id = 151643  # ä½ ä¸Šæ¬¡æ—¥å¿—ä¸­ pad/eos=151643ï¼Œå¾ˆå¯èƒ½å°±æ˜¯å®ƒ
# ==== 3ï¸âƒ£ åŠ è½½å­¦ç”Ÿæ¨¡å‹ ====
print("\nğŸ§  Loading student model ...")
device = "cpu"
model = TinySeq2SeqTransformer(
                teacher_model_path_zh2en=OPUS_MT_ZH_EN,
                teacher_model_path_en2zh=OPUS_MT_EN_ZH,
                d_model=ModelConfig.CURRENT_CONFIG.get("d_model", 128),
                nhead=ModelConfig.CURRENT_CONFIG.get("nhead", 4),
                num_encoder_layers=ModelConfig.CURRENT_CONFIG.get("num_encoder_layers", 2),
                num_decoder_layers=ModelConfig.CURRENT_CONFIG.get("num_decoder_layers", 2),
                dim_feedforward=ModelConfig.CURRENT_CONFIG.get("dim_feedforward", 256),
                dropout=ModelConfig.CURRENT_CONFIG.get("dropout", 0.1),
                max_seq_len=ModelConfig.MAX_SEQ_LEN,
                share_weights=True,
            ).to(device)
model.bos_token_id = student_tok.bos_token_id
model.eos_token_id = student_tok.eos_token_id
model.tokenizer = student_tok  # ç»™ generate ç”¨
state = torch.load(STUDENT_MODEL_PATH, map_location="cpu")
missing, unexpected = model.load_state_dict(state, strict=False)
print("Missing keys:", missing)
print("Unexpected keys:", unexpected)

embed_weight = model.embed.weight
print(f"ğŸ“ Student embedding shape: {embed_weight.shape}")

if embed_weight.shape[0] != len(student_tok):
    print(f"âš ï¸ åµŒå…¥å±‚ä¸ tokenizer ä¸åŒ¹é…: embed={embed_weight.shape[0]}, vocab={len(student_tok)}")
else:
    print("âœ… åµŒå…¥å±‚ä¸ tokenizer å¤§å°åŒ¹é…")

# ==== 4ï¸âƒ£ æµ‹è¯•ç¿»è¯‘ ====
print("\nğŸ§ª ç¿»è¯‘å¯¹ç…§æµ‹è¯• ...")

model.tokenizer = student_tok
model.bos_token_id = getattr(student_tok, "bos_token_id", 151642)
model.eos_token_id = getattr(student_tok, "eos_token_id", 151643)
model.eval()

text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
enc = student_tok(text, return_tensors="pt", padding=False)
with torch.no_grad():
    out = model.generate(input_ids=enc["input_ids"], num_beams=4, max_length=64)
decoded = student_tok.decode(out[0], skip_special_tokens=True)
print(f"ğŸ‘©â€ğŸ“ å­¦ç”Ÿæ¨¡å‹è¾“å‡º: {decoded}")

# ==== 5ï¸âƒ£ Teacher è¾“å‡ºå¯¹æ¯” ====
try:
    teacher_model = AutoModelForSeq2SeqLM.from_pretrained(TEACHER_PATH, trust_remote_code=True)
    teacher_enc = teacher_tok(text, return_tensors="pt")
    with torch.no_grad():
        t_out = teacher_model.generate(**teacher_enc, max_length=64)
    t_decoded = teacher_tok.decode(t_out[0], skip_special_tokens=True)
    print(f"ğŸ‘©â€ğŸ« æ•™å¸ˆæ¨¡å‹è¾“å‡º: {t_decoded}")
except Exception as e:
    print("âš ï¸ æ— æ³•åŠ è½½æ•™å¸ˆæ¨¡å‹:", e)

print("\nâœ… æ£€æŸ¥å®Œæˆï¼æ ¹æ®ä¸Šé¢è¾“å‡ºåˆ¤æ–­ï¼š")
print(" - å¦‚æœ tokenizer å‰20ä¸ªå¯¹ä¸ä¸Š â†’ tokené”™ä½ï¼›")
print(" - å¦‚æœ vocab å¤§å°ä¸åŒ â†’ tokenizerä¸åŒ¹é…ï¼›")
print(" - å¦‚æœ teacher è¾“å‡ºæ­£å¸¸ã€student å…¨ä¹±ç  â†’ å­¦ç”Ÿæ²¡å­¦åˆ°ã€‚")