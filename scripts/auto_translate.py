import os
import torch
import fasttext  # ğŸ‘ˆ æ–°å¢å¯¼å…¥
from transformers import MarianMTModel, MarianTokenizer


class AutoTranslator:
    def __init__(self):
        """åˆå§‹åŒ–ä¸­è‹±åŒå‘ç¿»è¯‘å™¨"""
        print("ğŸš€ æ­£åœ¨åŠ è½½ OPUS-MT ä¸­è‹±åŒå‘æ¨¡å‹...")

        # è·å–é¡¹ç›®æ ¹ç›®å½•
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # åŠ è½½ä¸­â†’è‹±æ¨¡å‹
        zh_en_path = os.path.join(PROJECT_ROOT, "models", "opus_mt_zh_en")
        self.model_zh_en = MarianMTModel.from_pretrained(zh_en_path, local_files_only=True)
        self.tokenizer_zh_en = MarianTokenizer.from_pretrained(zh_en_path, local_files_only=True)

        # åŠ è½½è‹±â†’ä¸­æ¨¡å‹
        en_zh_path = os.path.join(PROJECT_ROOT, "models", "opus_mt_en_zh")
        self.model_en_zh = MarianMTModel.from_pretrained(en_zh_path, local_files_only=True)
        self.tokenizer_en_zh = MarianTokenizer.from_pretrained(en_zh_path, local_files_only=True)

        # ğŸ‘‡ åŠ è½½ fasttext è¯­è¨€æ£€æµ‹æ¨¡å‹
        model_path = os.path.join(PROJECT_ROOT, "models", "lid.176.bin")
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ fasttext è¯­è¨€æ£€æµ‹æ¨¡å‹ä¸å­˜åœ¨ï¼è¯·ä¸‹è½½ lid.176.bin åˆ° models/ ç›®å½•")

        self.lang_detector = fasttext.load_model(model_path)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def detect_language(self, text: str) -> str:
        """
        ä½¿ç”¨ fasttext æ£€æµ‹æ–‡æœ¬è¯­è¨€
        :param text: è¾“å…¥æ–‡æœ¬
        :return: 'zh' (ä¸­æ–‡) æˆ– 'en' (è‹±æ–‡)
        """
        try:
            # é¢„æµ‹è¯­è¨€ï¼ˆè¿”å›æ¦‚ç‡æœ€é«˜çš„è¯­è¨€ï¼‰
            predictions = self.lang_detector.predict(text, k=1)  # k=1 è¡¨ç¤ºåªè¿”å›æœ€å¯èƒ½çš„è¯­è¨€
            lang_code = predictions[0][0].replace('__label__', '')  # å»æ‰ __label__ å‰ç¼€

            # æ˜ å°„åˆ° zh/en
            if lang_code in ['zh', 'zh-cn', 'zh-tw']:
                return 'zh'
            elif lang_code == 'en':
                return 'en'
            else:
                print(f"âš ï¸  æœªçŸ¥è¯­è¨€ '{lang_code}'ï¼Œé»˜è®¤æŒ‰ä¸­æ–‡å¤„ç†")
                return 'zh'
        except Exception as e:
            print(f"âŒ è¯­è¨€æ£€æµ‹å¤±è´¥: {str(e)}ï¼Œé»˜è®¤æŒ‰ä¸­æ–‡å¤„ç†")
            return 'zh'

    def translate(self, text: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶ç¿»è¯‘"""
        src_lang = self.detect_language(text)

        if src_lang == 'zh':
            print("ğŸ‡¨ğŸ‡³ æ£€æµ‹åˆ°ä¸­æ–‡ï¼Œç¿»è¯‘æˆè‹±æ–‡...")
            model = self.model_zh_en
            tokenizer = self.tokenizer_zh_en
        else:
            print("ğŸ‡ºğŸ‡¸ æ£€æµ‹åˆ°è‹±æ–‡ï¼Œç¿»è¯‘æˆä¸­æ–‡...")
            model = self.model_en_zh
            tokenizer = self.tokenizer_en_zh

        try:
            encoded = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            with torch.no_grad():
                generated = model.generate(**encoded)
            translated = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]
            return translated
        except Exception as e:
            print(f"âŒ ç¿»è¯‘å‡ºé”™: {str(e)}")
            return "ç¿»è¯‘å¤±è´¥"


if __name__ == "__main__":
    translator = AutoTranslator()

    test_texts = [
        "ä»Šå¤©å¤©æ°”çœŸå¥½",
        "I love artificial intelligence",
        "ç‘èŠ¯å¾®æ­£åœ¨å®šä¹‰ç«¯ä¾§å¤§æ¨¡å‹çš„æœªæ¥",
        "What is the capital of France?"
    ]

    print("ğŸ” è‡ªåŠ¨è¯­è¨€æ£€æµ‹ + ç¿»è¯‘æµ‹è¯•:")
    print("-" * 50)

    for text in test_texts:
        print(f"\nåŸæ–‡: {text}")
        result = translator.translate(text)
        print(f"ç¿»è¯‘: {result}")