#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
merge_student_checkpoints.py
====================================
ç”¨äºå°†å¤šåˆ†ç‰‡è’¸é¦ç”Ÿæˆçš„å­¦ç”Ÿæ¨¡å‹ï¼ˆ*_best.pthï¼‰èšåˆæˆä¸€ä¸ªæœ€ç»ˆæ¨¡å‹ã€‚

âœ… åŠŸèƒ½ç‰¹æ€§
- è‡ªåŠ¨æ‰«æ outputs/models ä¸‹çš„æ‰€æœ‰ *_best.pth æ–‡ä»¶
- æ”¯æŒæ™®é€šå¹³å‡ä¸åŠ æƒå¹³å‡èåˆ
- å¯é€‰æ‹© CPU / GPU åŠ è½½
- è¾“å‡º student_model_final_merged.pth
- å¯é€‰éªŒè¯å‚æ•°å½¢çŠ¶ä¸€è‡´æ€§

ç¤ºä¾‹ï¼š
    python merge_student_checkpoints.py \
        --model_dir outputs/models \
        --output_path outputs/models/student_model_final_merged.pth \
        --device cpu \
        --mode mean

ä½œè€…: Yuhao + GPT-5
æ—¥æœŸ: 2025-11-05
"""

import os
import glob
import torch
import argparse
import logging
from tqdm import tqdm

# =========================
# ğŸš€ å‚æ•°å®šä¹‰
# =========================
def parse_args():
    parser = argparse.ArgumentParser(description="Merge multiple shard student checkpoints.")
    parser.add_argument("--model_dir", type=str, default="outputs/models",
                        help="ç›®å½•è·¯å¾„ï¼ŒåŒ…å«å¤šä¸ª *_best.pth æ–‡ä»¶")
    parser.add_argument("--output_path", type=str, default="outputs/models/student_model_final_merged.pth",
                        help="åˆå¹¶åè¾“å‡ºæ¨¡å‹è·¯å¾„")
    parser.add_argument("--device", type=str, default="cpu",
                        choices=["cpu", "cuda"],
                        help="åŠ è½½æ—¶ä½¿ç”¨çš„è®¾å¤‡")
    parser.add_argument("--mode", type=str, default="mean",
                        choices=["mean", "weighted"],
                        help="èšåˆæ¨¡å¼ï¼šæ™®é€šå¹³å‡(mean) æˆ– åŠ æƒå¹³å‡(weighted)")
    parser.add_argument("--weights", type=float, nargs="*",
                        help="å¯é€‰æƒé‡åˆ—è¡¨ï¼Œå¯¹åº”æ¯ä¸ªæ¨¡å‹ï¼ˆä»…åœ¨ weighted æ¨¡å¼ç”Ÿæ•ˆï¼‰")
    parser.add_argument("--dry_run", action="store_true",
                        help="ä»…æ‰“å°æ¨¡å‹æ–‡ä»¶ï¼Œä¸å®é™…åˆå¹¶")
    return parser.parse_args()

# =========================
# ğŸ§  æ—¥å¿—é…ç½®
# =========================
def setup_logging():
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(message)s",
        level=logging.INFO
    )

# =========================
# ğŸ” æ£€æŸ¥æ¨¡å‹ä¸€è‡´æ€§
# =========================
def check_model_shapes(models):
    ref_keys = set(models[0].keys())
    for i, state_dict in enumerate(models[1:], start=1):
        if set(state_dict.keys()) != ref_keys:
            missing = ref_keys - set(state_dict.keys())
            extra = set(state_dict.keys()) - ref_keys
            raise ValueError(f"âŒ æ¨¡å‹ {i} å‚æ•°é”®ä¸ä¸€è‡´ã€‚\nç¼ºå¤±: {missing}\nå¤šä½™: {extra}")

# =========================
# âš™ï¸ ä¸»èšåˆé€»è¾‘
# =========================
def merge_checkpoints(args):
    model_paths = sorted(glob.glob(os.path.join(args.model_dir, "*_best.pth")))
    if not model_paths:
        logging.error(f"âŒ æœªæ‰¾åˆ°ä»»ä½• *_best.pth æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {args.model_dir}")
        return

    logging.info(f"ğŸ“¦ å…±æ‰¾åˆ° {len(model_paths)} ä¸ªåˆ†ç‰‡æ¨¡å‹ï¼š")
    for i, path in enumerate(model_paths):
        logging.info(f"  [{i+1:02d}] {path}")

    if args.dry_run:
        logging.info("ğŸŸ¡ Dry-run æ¨¡å¼ï¼Œä»…åˆ—å‡ºæ¨¡å‹ï¼Œä¸è¿›è¡Œåˆå¹¶ã€‚")
        return

    # è®¾å¤‡é€‰æ‹©
    device = torch.device(args.device if torch.cuda.is_available() or args.device == "cpu" else "cpu")
    logging.info(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

    # è½½å…¥ç¬¬ä¸€ä¸ªæ¨¡å‹
    logging.info(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_paths[0]}")
    merged_state = torch.load(model_paths[0], map_location=device)
    for key in merged_state.keys():
        merged_state[key] = merged_state[key].float()

    # æ£€æŸ¥æƒé‡é•¿åº¦
    if args.mode == "weighted":
        if not args.weights or len(args.weights) != len(model_paths):
            raise ValueError("âŒ åŠ æƒæ¨¡å¼éœ€è¦æŒ‡å®šä¸æ¨¡å‹æ•°é‡ä¸€è‡´çš„ --weights å‚æ•°ã€‚")
        total_weight = sum(args.weights)
        normalized_weights = [w / total_weight for w in args.weights]
    else:
        normalized_weights = [1.0 / len(model_paths)] * len(model_paths)

    # åŠ è½½å¹¶ç´¯åŠ åç»­æ¨¡å‹
    for i, path in enumerate(tqdm(model_paths[1:], desc="ğŸ”„ èšåˆä¸­")):
        state_dict = torch.load(path, map_location=device)
        check_model_shapes([merged_state, state_dict])

        weight = normalized_weights[i + 1] if args.mode == "weighted" else normalized_weights[i + 1]
        for key in merged_state.keys():
            merged_state[key] += state_dict[key].float() * weight

    # ä¿å­˜èšåˆç»“æœ
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
    torch.save(merged_state, args.output_path)
    logging.info(f"âœ… èšåˆå®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {args.output_path}")
    logging.info(f"ğŸ“ å‚æ•°æ•°é‡: {len(merged_state)}")

# =========================
# ğŸ¯ ä¸»å‡½æ•°å…¥å£
# =========================
if __name__ == "__main__":
    setup_logging()
    args = parse_args()
    merge_checkpoints(args)