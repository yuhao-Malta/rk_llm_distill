# scripts/evaluate_models.py
import os
import json
import torch
from transformers import MarianMTModel, MarianTokenizer
from models.tiny_transformer import TinyTransformer  # ğŸ‘ˆ ä½ çš„å­¦ç”Ÿæ¨¡å‹
from sacrebleu import corpus_bleu
import jieba
from nltk.translate.bleu_score import sentence_bleu
from nltk.tokenize import word_tokenize
import nltk

# è®¾ç½® NLTK æ•°æ®ç›®å½•ä¸ºé¡¹ç›®å†…çš„ nltk_data æ–‡ä»¶å¤¹
NLTK_DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "nltk_data")
nltk.data.path.append(NLTK_DATA_DIR)  # ğŸ‘ˆ æ·»åŠ è‡ªå®šä¹‰è·¯å¾„

# éªŒè¯ punkt æ˜¯å¦èƒ½åŠ è½½
try:
    nltk.data.find('tokenizers/punkt')
    print("âœ… NLTK punkt æ•°æ®åŠ è½½æˆåŠŸï¼")
except LookupError:
    print("âŒ NLTK punkt æ•°æ®æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼š", NLTK_DATA_DIR)
    raise


# -----------------------------
# 2. æ¨¡å‹åŠ è½½å™¨
# -----------------------------
class ModelEvaluator:
    def __init__(self, model_name_or_path, model_type="opus_mt"):
        # âœ… è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        # âœ… æ„å»ºæ¨¡å‹ç»å¯¹è·¯å¾„
        if model_type == "opus_mt":
            model_name_or_path = os.path.join(PROJECT_ROOT, "models", "opus_mt_zh_en")

        self.model_name = model_name_or_path
        self.model_type = model_type
        self.model = None
        self.tokenizer = None
        self.load_model()

        if self.model_type == "tiny_transformer":
            self.model = TinyTransformer()
            self.model.load_state_dict(torch.load(self.model_name, map_location="cpu"))
            self.model.eval()
            self.tokenizer = self.model.tokenizer  # å¤ç”¨æ¨¡å‹çš„ tokenizer

    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ”¯æŒæœ¬åœ°åŠ è½½ï¼‰"""
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {self.model_name} ({self.model_type})")

        if self.model_type == "opus_mt":
            # âœ… æœ¬åœ°åŠ è½½ OPUS-MT æ¨¡å‹
            LOCAL_MODEL_PATH = "models/opus_mt_zh_en"  # ğŸ‘ˆ ä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
            try:
                self.tokenizer = MarianTokenizer.from_pretrained(
                    self.model_name,
                    local_files_only=True  # ğŸ‘ˆ å¼ºåˆ¶ç¦»çº¿åŠ è½½
                )
                self.model = MarianMTModel.from_pretrained(
                    self.model_name,
                    local_files_only=True  # ğŸ‘ˆ å¼ºåˆ¶ç¦»çº¿åŠ è½½
                )
                print("âœ… OPUS-MT æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"âŒ OPUS-MT æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                raise
        elif self.model_type == "tiny_transformer":
            # å‡è®¾ TinyTransformer æœ‰ä¸€ä¸ª translate æ–¹æ³•
            self.model = TinyTransformer()
            self.model.load_state_dict(torch.load(self.model_name, map_location="cpu"))
            self.model.eval()
            self.tokenizer = self.model.tokenizer
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")

    def translate(self, texts, src_lang="zh", tgt_lang="en"):
        """æ‰¹é‡ç¿»è¯‘æ–‡æœ¬"""
        if self.model_type == "opus_mt":
            return self._translate_opus_mt(texts)
        elif self.model_type == "tiny_transformer":
            return self._translate_tiny_transformer(texts)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")

    def _translate_opus_mt(self, texts):
        """ä½¿ç”¨ OPUS-MT ç¿»è¯‘"""
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            generated = self.model.generate(**inputs)
        return self.tokenizer.batch_decode(generated, skip_special_tokens=True)

    def _translate_tiny_transformer(self, texts):
        """ä½¿ç”¨ TinyTransformer ç¿»è¯‘ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # TODO: å®ç° TinyTransformer çš„å®é™…ç¿»è¯‘é€»è¾‘
        # è¿™é‡Œä»…ä¸ºç¤ºæ„ï¼Œå®é™…åº”è°ƒç”¨æ¨¡å‹å‰å‘ä¼ æ’­
        # print("âš ï¸  TinyTransformer ç¿»è¯‘é€»è¾‘å¾…å®ç°ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ...")
        # return [f"[TinyTransformer ç¿»è¯‘]: {text}" for text in texts]
        try:
            # 1. ç¼–ç è¾“å…¥æ–‡æœ¬
            encoded = self.tokenizer(
                texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            input_ids = encoded.input_ids
            attention_mask = encoded.attention_mask

            # 2. åˆ¤æ–­ä»»åŠ¡ç±»å‹ï¼ˆä¸­â†’è‹± or è‹±â†’ä¸­ï¼‰
            # ç®€å•åˆ¤æ–­ï¼šå¦‚æœç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯ä¸­æ–‡ï¼Œåˆ™ä¸ºä¸­â†’è‹±
            if '\u4e00' <= texts[0] <= '\u9fff':
                task_id = torch.tensor([0])  # 0 = ä¸­â†’è‹±
            else:
                task_id = torch.tensor([1])  # 1 = è‹±â†’ä¸­

            # 3. æ¨¡å‹å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    task_id=task_id
                )
                logits = outputs["logits"]

            # 4. è§£ç è¾“å‡ºï¼ˆè´ªå¿ƒè§£ç ï¼‰
            predicted_ids = torch.argmax(logits, dim=-1)
            translated = self.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]

            return translated.strip()

        except Exception as e:
            print(f"âŒ TinyTransformer ç¿»è¯‘å‡ºé”™: {str(e)}")
            return "[ç¿»è¯‘å¤±è´¥]"


# -----------------------------
# 3. è¯„ä¼°å™¨
# -----------------------------
class TranslationEvaluator:
    def __init__(self, test_data_path="data/test/zh_en.jsonl"):
        self.test_data_path = test_data_path
        self.test_data = self._load_test_data()

    def _load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        data = []
        if os.path.exists(self.test_data_path):
            with open(self.test_data_path, "r", encoding="utf-8") as f:
                for line in f:
                    data.append(json.loads(line))
        else:
            # å¦‚æœæ²¡æœ‰æµ‹è¯•æ•°æ®ï¼Œç”Ÿæˆä¸€äº›ç¤ºä¾‹
            print("âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®...")
            data = [
                {"src": "ä»Šå¤©å¤©æ°”çœŸå¥½", "ref": "The weather is nice today."},
                {"src": "æˆ‘å–œæ¬¢å­¦ä¹ äººå·¥æ™ºèƒ½", "ref": "I like to study artificial intelligence."},
                {"src": "ç‘èŠ¯å¾®æ­£åœ¨å®šä¹‰ç«¯ä¾§å¤§æ¨¡å‹çš„æœªæ¥",
                 "ref": "Rockchip is defining the future of on-device large models."},
                {"src": "The weather is nice today.", "ref": "ä»Šå¤©å¤©æ°”çœŸå¥½"},
                {"src": "I love artificial intelligence.", "ref": "æˆ‘å–œæ¬¢äººå·¥æ™ºèƒ½"},
            ]
            os.makedirs(os.path.dirname(self.test_data_path), exist_ok=True)
            with open(self.test_data_path, "w", encoding="utf-8") as f:
                for item in data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
        return data

    def evaluate_model(self, model_evaluator: ModelEvaluator):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ” è¯„ä¼°æ¨¡å‹: {model_evaluator.model_name}")
        translations = []
        references = []

        for item in self.test_data:
            src_text = item["src"]
            ref_text = item["ref"]

            # è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if len(src_text) > 0 and '\u4e00' <= src_text[0] <= '\u9fff':
                src_lang, tgt_lang = "zh", "en"
            else:
                src_lang, tgt_lang = "en", "zh"

            hyp_text = model_evaluator.translate([src_text], src_lang, tgt_lang)[0]
            translations.append(hyp_text)
            references.append(ref_text)
            print(f"åŸæ–‡: {src_text}")
            print(f"å‚è€ƒ: {ref_text}")
            print(f"ç¿»è¯‘: {hyp_text}\n")

        # è®¡ç®— BLEU
        bleu_score = corpus_bleu(translations, [references]).score
        print(f"âœ… BLEU åˆ†æ•°: {bleu_score:.2f}")
        return {
            "model_name": model_evaluator.model_name,
            "bleu": bleu_score,
            "translations": translations,
            "references": references
        }

    def compare_models(self, evaluators: list):
        """å¯¹æ¯”å¤šä¸ªæ¨¡å‹"""
        results = []
        for evaluator in evaluators:
            result = self.evaluate_model(evaluator)
            results.append(result)

        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        print("\nğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ:")
        print("-" * 80)
        print(f"{'æ¨¡å‹åç§°':<30} {'BLEU åˆ†æ•°':<10}")
        print("-" * 80)
        for result in results:
            print(f"{result['model_name']:<30} {result['bleu']:<10.2f}")
        print("-" * 80)


# -----------------------------
# 4. ä¸»ç¨‹åºå…¥å£
# -----------------------------
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = TranslationEvaluator()

    # 2. å®šä¹‰è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨
    model_evaluators = [
        ModelEvaluator("../models/opus_mt_zh_en", model_type="opus_mt"),  # ğŸ‘ˆ æ”¹ä¸ºæœ¬åœ°è·¯å¾„
        ModelEvaluator("../outputs/models/student_model_amp_epoch_3.pth", model_type="tiny_transformer"),  # ğŸ‘ˆ æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    ]

    # 3. å¼€å§‹å¯¹æ¯”è¯„ä¼°
    evaluator.compare_models(model_evaluators)