import os
import json
import time
import logging
from tqdm import tqdm
import dashscope
from datasets import Dataset
import glob
import threading
import queue
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# âœ… é…ç½® DashScope API Keyï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™… Keyï¼‰
dashscope.api_key = "sk-b0c78a77e5ea489b8c68e0b5049204c6"  # ğŸ‘ˆ æ›¿æ¢ä¸ºä½ çš„çœŸå® API Keyï¼

# âœ… å…¨å±€åŠ è½½ Qwen-1.5-1.8Bï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
QWEN_MODEL = None
QWEN_TOKENIZER = None

# âœ… é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/generate_logits.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)


def load_qwen_model():
    """åŠ è½½ Qwen-1.5-1.8B æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰"""
    global QWEN_MODEL, QWEN_TOKENIZER

    if QWEN_MODEL is None or QWEN_TOKENIZER is None:
        # âœ… è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # âœ… æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆç¡®ä¿æ˜¯ç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶ï¼‰
        MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "qwen_tokenizer_offline", "qwen", "Qwen1___5-1___8B")

        # âœ… æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"âŒ æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_PATH}")

        # âœ… æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ["config.json", "tokenizer_config.json", "vocab.json"]
        for file in required_files:
            if not os.path.exists(os.path.join(MODEL_PATH, file)):
                raise FileNotFoundError(f"âŒ æœ¬åœ°æ¨¡å‹ç¼ºå°‘å…³é”®æ–‡ä»¶: {file}")

        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½ Qwen-1.5-1.8B æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰: {MODEL_PATH}")

        try:
            # âœ… å¼ºåˆ¶ç¦»çº¿åŠ è½½ï¼ˆé¿å…è”ç½‘ï¼‰
            QWEN_TOKENIZER = AutoTokenizer.from_pretrained(
                MODEL_PATH,
                local_files_only=True,  # ğŸ‘ˆ å¼ºåˆ¶ç¦»çº¿åŠ è½½
                trust_remote_code=True
            )
            QWEN_MODEL = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map="auto",
                torch_dtype=torch.float16,
                local_files_only=True,  # ğŸ‘ˆ å¼ºåˆ¶ç¦»çº¿åŠ è½½
                trust_remote_code=True
            )
            QWEN_MODEL.eval()
            print("âœ… Qwen-1.5-1.8B æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ Qwen-1.5-1.8B æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise


def call_qwen_translate_local(text, target_lang="en", max_retries=3):
    """è°ƒç”¨æœ¬åœ° Qwen-1.5-1.8B ç¿»è¯‘"""
    global QWEN_MODEL, QWEN_TOKENIZER
    load_qwen_model()  # ç¡®ä¿æ¨¡å‹å·²åŠ è½½

    for attempt in range(max_retries):
        try:
            # æ„é€  Promptï¼ˆä¸ DashScope API ä¸€è‡´ï¼‰
            source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
            target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
            prompt = (
                f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
                f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
                f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ï¼š\n\n"
                f"{text}"
            )

            # ç¼–ç è¾“å…¥
            inputs = QWEN_TOKENIZER(prompt, return_tensors="pt").to(QWEN_MODEL.device)

            # æ¨ç†ï¼ˆåŠç²¾åº¦ï¼‰
            with torch.no_grad():
                outputs = QWEN_MODEL.generate(
                    **inputs,
                    max_new_tokens=512,  # æœ€å¤§ç”Ÿæˆé•¿åº¦
                    temperature=0.1,  # ä½æ¸©åº¦ï¼Œç¡®ä¿ç¿»è¯‘ä¸€è‡´æ€§
                    do_sample=False,  # è´ªå¿ƒè§£ç 
                    top_k=1,
                    top_p=0.9
                )

            # è§£ç è¾“å‡º
            generated = outputs[0][inputs.input_ids.shape[1]:]  # å»æ‰è¾“å…¥éƒ¨åˆ†
            translated_text = QWEN_TOKENIZER.decode(generated, skip_special_tokens=True).strip()

            return translated_text

        except Exception as e:
            logging.warning(f"âš ï¸  æœ¬åœ°ç¿»è¯‘å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)

    return None


def call_qwen_translate_worker(local_task_queue, local_result_queue):
    """
    å¤šçº¿ç¨‹ç¿»è¯‘å·¥ä½œçº¿ç¨‹ï¼Œä½¿ç”¨ä¼ å…¥çš„é˜Ÿåˆ—ã€‚
    é€šè¿‡è°ƒç”¨ call_qwen_translate æ¥å®ç°ç¿»è¯‘å’Œå›é€€é€»è¾‘ã€‚
    """
    while True:
        task = local_task_queue.get()
        if task is None:
            local_task_queue.task_done()
            break

        text, target_lang, idx = task
        try:
            # --- å…³é”®ä¿®æ”¹ï¼šè°ƒç”¨ä½ å°è£…å¥½çš„å‡½æ•° ---
            translated_text = call_qwen_translate(text=text, target_lang=target_lang)

            if translated_text:
                # æˆåŠŸï¼Œå°†ç»“æœæ”¾å…¥ç»“æœé˜Ÿåˆ—
                local_result_queue.put((idx, translated_text, None))
            else:
                # å‡½æ•°è¿”å› Noneï¼Œè¡¨ç¤ºç¿»è¯‘å¤±è´¥
                local_result_queue.put((idx, None, "Translation returned None or failed after retries."))

        except Exception as e:
            # æ•è· call_qwen_translate å†…éƒ¨æœªå¤„ç†çš„å¼‚å¸¸æˆ–ä¼ é€’è¿‡ç¨‹ä¸­çš„ä»»ä½•é—®é¢˜
            local_result_queue.put((idx, None, f"Worker caught exception: {e}"))
        finally:
            local_task_queue.task_done()


def call_qwen_translate_api(text, target_lang="en", max_retries=3):
    """
    è°ƒç”¨ Qwen-Max ç¿»è¯‘ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œä¼˜åŒ–åçš„é€»è¾‘ã€‚
    ä¸»åŠ¨æ£€æŸ¥ TextTranslation æ˜¯å¦å­˜åœ¨ï¼Œä»¥å†³å®šè°ƒç”¨è·¯å¾„ã€‚
    """

    # --- ä¼˜åŒ–ç‚¹ï¼šä¸»åŠ¨æ£€æŸ¥ TextTranslation æ˜¯å¦å­˜åœ¨ ---
    # ä½¿ç”¨ hasattr è¿›è¡Œæ£€æŸ¥ï¼Œé¿å…ä¸å¿…è¦çš„ AttributeError å¼‚å¸¸
    TEXT_TRANSLATION_AVAILABLE = hasattr(dashscope, 'TextTranslation')
    # --- ä¼˜åŒ–ç‚¹ç»“æŸ ---

    for attempt in range(max_retries):
        logging.info(f"Starting translation attempt {attempt + 1}")

        # --- æ ¹æ®æ£€æŸ¥ç»“æœï¼Œé€‰æ‹©è°ƒç”¨è·¯å¾„ ---
        if TEXT_TRANSLATION_AVAILABLE:
            # å¦‚æœ TextTranslation å­˜åœ¨ï¼Œåˆ™å°è¯•ä½¿ç”¨å®ƒ
            logging.debug(f"Attempt {attempt + 1}: TextTranslation is available, using it.")
            try:
                response = dashscope.TextTranslation.call(
                    model='qwen-max',
                    text=text,
                    target_lang=target_lang
                )
                logging.debug("TextTranslation API call made.")

                if response.status_code == 200:
                    logging.debug("TextTranslation API call successful.")
                    return response.output['translated_text']
                else:
                    logging.warning(f"âš ï¸  TextTranslation API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {response.message}")

            except Exception as specific_api_error:  # æ•è· TextTranslation API è°ƒç”¨è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å…¶ä»–å¼‚å¸¸
                logging.warning(f"âš ï¸  TextTranslation API è°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {specific_api_error}")

        else:
            # å¦‚æœ TextTranslation ä¸å­˜åœ¨ï¼Œæˆ–è€…é¦–é€‰ TextTranslation å¤±è´¥ï¼Œåˆ™ä½¿ç”¨ Generation API
            # æ³¨æ„ï¼šè¿™é‡Œçš„é€»è¾‘ç°åœ¨å˜æˆäº†â€œé¦–é€‰ Generationâ€è€Œä¸æ˜¯â€œå›é€€åˆ° Generationâ€
            logging.debug(f"Attempt {attempt + 1}: TextTranslation not available or preferred, using Generation API.")

            try:
                # æ„é€  Promptï¼Œæ˜ç¡®æŒ‡ç¤ºæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€
                source_lang_full = "ä¸­æ–‡" if target_lang == "en" else "è‹±æ–‡"
                target_lang_full = "è‹±æ–‡" if target_lang == "en" else "ä¸­æ–‡"
                prompt = (
                    f"ä½ æ˜¯ä¸€ä½ç²¾é€š{source_lang_full}å’Œ{target_lang_full}çš„ä¸“ä¸šç¿»è¯‘äººå‘˜ã€‚\n"
                    f"è¯·å°†ä»¥ä¸‹{source_lang_full}æ–‡æœ¬å‡†ç¡®ã€è‡ªç„¶åœ°ç¿»è¯‘æˆ{target_lang_full}ï¼Œ"
                    f"åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ï¼š\n\n"
                    f"{text}"
                )

                response = dashscope.Generation.call(
                    model='qwen-max',
                    prompt=prompt
                    # å¯æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚ max_tokens, temperature ç­‰
                )
                logging.debug("Generation API call made.")

                if response.status_code == 200:
                    # Generation API è¿”å›çš„æ–‡æœ¬é€šå¸¸åœ¨ 'text' æˆ– 'output' å­—æ®µ
                    # å…·ä½“å–å†³äºå“åº”æ ¼å¼ï¼Œè¿™é‡Œå‡è®¾æ˜¯ 'text'
                    translated_text = response.output.get('text', '').strip()
                    if translated_text:
                        logging.debug("Generation API call successful.")
                        return translated_text
                    else:
                        logging.warning(f"âš ï¸  Generation API returned empty text (å°è¯• {attempt + 1})")
                else:
                    logging.warning(f"âš ï¸  Generation API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {response.message}")

            except Exception as general_api_error:  # æ•è· Generation API è°ƒç”¨è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å¼‚å¸¸
                logging.warning(f"âš ï¸  Generation API è°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {general_api_error}")

        # --- é‡è¯•é€»è¾‘ ---
        if attempt < max_retries - 1:
            wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
            logging.info(f"â³ ç­‰å¾… {wait_time} ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡å°è¯•...")
            time.sleep(wait_time)

    # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥åï¼Œè¿”å› None
    logging.error(f"âŒ ç»è¿‡ {max_retries} æ¬¡å°è¯•åï¼Œæ‰€æœ‰ç¿»è¯‘æ–¹æ³•å‡å¤±è´¥ã€‚")
    return None

def call_qwen_translate(text, target_lang="en", max_retries=3):
    """è°ƒç”¨ Qwen ç¿»è¯‘ï¼ˆä¼˜å…ˆæœ¬åœ°ï¼Œå¤±è´¥å›é€€ APIï¼‰"""
    # âœ… ä¼˜å…ˆè°ƒç”¨æœ¬åœ°æ¨¡å‹
    translated = call_qwen_translate_local(text, target_lang, max_retries)
    if translated:
        return translated

    # âŒ å›é€€åˆ° DashScope APIï¼ˆå¦‚æœæœ¬åœ°å¤±è´¥ï¼‰
    logging.warning("âš ï¸  æœ¬åœ°ç¿»è¯‘å¤±è´¥ï¼Œå›é€€åˆ° DashScope API")
    return call_qwen_translate_api(text, target_lang, max_retries)  # ä½ åŸæ¥çš„ API è°ƒç”¨å‡½æ•°

# ç›¸åº”åœ°ï¼Œä¿®æ”¹ generate_teacher_logits å‡½æ•°ï¼Œç§»é™¤å…¶ä¸­çš„çº¿ç¨‹å¯åŠ¨å’Œåœæ­¢é€»è¾‘
def generate_teacher_logits(
        local_dataset_path="data/raw/wmt19_zh_en",
        output_dir="data/teacher_logits",
        max_samples=None,  # None = å…¨é‡ç”Ÿæˆ
        start_from=0,
        # num_threads=4  # ä¸å†éœ€è¦ä½œä¸ºå‚æ•°ä¼ é€’ç»™æ­¤å‡½æ•°ï¼Œå¯ä»¥åœ¨ generate_direction_multithread å†…éƒ¨å®šä¹‰æˆ–ä¼ é€’
):
    """ç”Ÿæˆæ•™å¸ˆæ¨¡å‹è½¯æ ‡ç­¾ï¼ˆä¸­è‹±äº’è¯‘ï¼‰"""
    try:
        end_sample_info = "å…¨éƒ¨" if max_samples is None else str(start_from + max_samples)
        logging.info(f"ğŸ§  å¼€å§‹ç”Ÿæˆæ•™å¸ˆç¿»è¯‘ (æ ·æœ¬ {start_from} åˆ° {end_sample_info})...")

        # âœ… è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        ABS_LOCAL_DATASET_PATH = os.path.join(PROJECT_ROOT, local_dataset_path)

        # âœ… æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(ABS_LOCAL_DATASET_PATH):
            raise FileNotFoundError(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {ABS_LOCAL_DATASET_PATH}")

        os.makedirs(output_dir, exist_ok=True)

        # âœ… ä»æœ¬åœ° .parquet æ–‡ä»¶åŠ è½½æ•°æ®ï¼ˆç›´æ¥åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰
        train_files = glob.glob(os.path.join(ABS_LOCAL_DATASET_PATH, "train", "*.parquet"))
        if not train_files:
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒé›†æ–‡ä»¶: {os.path.join(ABS_LOCAL_DATASET_PATH, 'train', '*.parquet')}")

        dataset = Dataset.from_parquet(train_files[0])  # <--- ç¡®ä¿è¿™è¡Œå­˜åœ¨

        # è®¾ç½®æ ·æœ¬æ•°é‡
        total_samples = len(dataset) if max_samples is None else min(max_samples, len(dataset))
        logging.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")

        # ç”Ÿæˆä¸­â†’è‹±æ•°æ® (ä¼šå†…éƒ¨å¯åŠ¨å’Œåœæ­¢çº¿ç¨‹)
        zh_to_en_file = os.path.join(output_dir, "zh_to_en.jsonl")
        zh_success, zh_fail = generate_direction_multithread(
            dataset=dataset,
            src_lang="zh",
            tgt_lang="en",
            output_file=zh_to_en_file,
            max_samples=total_samples,
            start_from=start_from
        )

        # ç”Ÿæˆè‹±â†’ä¸­æ•°æ® (ä¼šå†…éƒ¨å¯åŠ¨å’Œåœæ­¢çº¿ç¨‹)
        en_to_zh_file = os.path.join(output_dir, "en_to_zh.jsonl")
        en_success, en_fail = generate_direction_multithread(
            dataset=dataset,
            src_lang="en",
            tgt_lang="zh",
            output_file=en_to_zh_file,
            max_samples=total_samples,
            start_from=start_from
        )

        logging.info("âœ… æ•™å¸ˆè½¯æ ‡ç­¾ç”Ÿæˆå®Œæˆï¼")
        return zh_success + en_success, zh_fail + en_fail

    except Exception as e:
        logging.error(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸


def generate_direction_multithread(dataset, src_lang, tgt_lang, output_file, max_samples, start_from):
    """
    å¤šçº¿ç¨‹ç”Ÿæˆå•å‘ç¿»è¯‘æ•°æ®ï¼ˆä¸­â†’è‹± æˆ– è‹±â†’ä¸­ï¼‰
    """
    # åˆ›å»ºå±€éƒ¨é˜Ÿåˆ—
    local_task_queue = queue.Queue()
    local_result_queue = queue.Queue()
    num_threads = 4  # å¯ä»¥ä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œæˆ–åœ¨æ­¤å¤„å®šä¹‰

    # å¯åŠ¨å¤šçº¿ç¨‹
    threads = []
    for i in range(num_threads):
        t = threading.Thread(target=call_qwen_translate_worker, args=(local_task_queue, local_result_queue))
        t.start()
        threads.append(t)

    # åˆå§‹åŒ–è¿”å›å€¼
    success_count = 0
    fail_count = 0

    try:  # ä½¿ç”¨ try...finally ç¡®ä¿çº¿ç¨‹èƒ½è¢«æ­£ç¡®åœæ­¢
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–å·²å¤„ç†æ ·æœ¬æ•°
        processed_count = 0
        if os.path.exists(output_file):
            with open(output_file, 'r', encoding='utf-8') as f:
                processed_count = sum(1 for _ in f)
            logging.info(f"ğŸ“Œ å·²å­˜åœ¨ {processed_count} æ¡è®°å½•ï¼Œä»ç¬¬ {processed_count} æ¡å¼€å§‹")
            start_from = processed_count

        # è·å–æ•°æ®åˆ‡ç‰‡
        end_idx = min(start_from + max_samples, len(dataset))

        # ç”Ÿæˆç¿»è¯‘ï¼ˆå¤šçº¿ç¨‹ï¼‰
        with open(output_file, 'a', encoding='utf-8') as f:
            # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
            for i in range(start_from, end_idx):
                translation = dataset[i]['translation']
                if src_lang == "zh":
                    src_text = translation['zh']
                else:
                    src_text = translation['en']
                local_task_queue.put((src_text, tgt_lang, i))

            # å¤„ç†ç»“æœ
            pbar = tqdm(total=end_idx - start_from, desc=f"ç”Ÿæˆ {src_lang}â†’{tgt_lang} ç¿»è¯‘", initial=0)
            for _ in range(end_idx - start_from):
                idx, trans_text, error = local_result_queue.get()  # ä½¿ç”¨å±€éƒ¨é˜Ÿåˆ—
                if trans_text:
                    translation = dataset[idx]['translation']
                    if src_lang == "zh":
                        ref_text = translation['en']
                    else:
                        ref_text = translation['zh']

                    result = {
                        "id": idx,
                        "src": dataset[idx]['translation'][src_lang],
                        "ref": ref_text,
                        "hyp": trans_text,
                        "task_id": 0 if src_lang == "zh" else 1,
                        "timestamp": time.time()
                    }
                    f.write(json.dumps(result, ensure_ascii=False) + "\n")
                    f.flush()
                    success_count += 1
                else:
                    logging.warning(f"âŒ ç¿»è¯‘å¤±è´¥ (ID {idx}): {error}")
                    fail_count += 1

                pbar.update(1)

            pbar.close()

        logging.info(f"âœ… {src_lang}â†’{tgt_lang} ç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}")

    finally:  # æ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸ï¼Œéƒ½ç¡®ä¿çº¿ç¨‹åœæ­¢
        # åœæ­¢å·¥ä½œçº¿ç¨‹
        for i in range(num_threads):
            local_task_queue.put(None)  # å‘å±€éƒ¨é˜Ÿåˆ—å‘é€åœæ­¢ä¿¡å·
        for t in threads:
            t.join()  # ç­‰å¾…çº¿ç¨‹ç»“æŸ

    # åœ¨ finally å—ä¹‹åè¿”å›ç»“æœ
    return success_count, fail_count


def validate_logits_file(file_path):
    """éªŒè¯ç”Ÿæˆçš„ logits æ–‡ä»¶"""
    try:
        count = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    json.loads(line)
                    count += 1
        logging.info(f"âœ… éªŒè¯é€šè¿‡ï¼å…± {count} æ¡æœ‰æ•ˆè®°å½•")
        return count
    except Exception as e:
        logging.error(f"âŒ éªŒè¯å¤±è´¥: {str(e)}")
        return 0


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—ä»¥ä¾¿æŸ¥çœ‹è°ƒè¯•ä¿¡æ¯ (å¦‚æœéœ€è¦æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå¯ä»¥è®¾ç½®ä¸º logging.DEBUG)
    logging.basicConfig(level=logging.INFO)
    os.makedirs("logs", exist_ok=True)

    # ğŸš€ ç”ŸæˆæŒ‡å®šæ•°é‡çš„æ ·æœ¬
    success, fail = generate_teacher_logits(
        max_samples=150,  # None = å…¨é‡ï¼ˆçº¦ 400 ä¸‡æ¡ï¼‰
        # num_threads=8  # å¦‚æœéœ€è¦è°ƒæ•´ï¼Œå¯ä»¥åœ¨ generate_direction_multithread å†…è°ƒæ•´
    )

    # éªŒè¯ç”Ÿæˆæ–‡ä»¶
    validate_logits_file("data/teacher_logits/zh_to_en.jsonl")
    validate_logits_file("data/teacher_logits/en_to_zh.jsonl")

    # # é…ç½®æ—¥å¿—ä»¥ä¾¿æŸ¥çœ‹è°ƒè¯•ä¿¡æ¯ (å¦‚æœéœ€è¦æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå¯ä»¥è®¾ç½®ä¸º logging.DEBUG)
    # logging.basicConfig(level=logging.INFO)
    #
    # # æµ‹è¯• 1: ä¸­æ–‡ -> è‹±æ–‡
    # zh_text = "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­æ•£æ­¥å§ã€‚"
    # print("-" * 20)
    # print(f"åŸæ–‡ (ä¸­æ–‡): {zh_text}")
    # trans_en = call_qwen_translate(zh_text, target_lang="en")
    # print(f"è¯‘æ–‡ (è‹±æ–‡): {trans_en}")
    # print("-" * 20)
    #
    # # æµ‹è¯• 2: è‹±æ–‡ -> ä¸­æ–‡
    # en_text = "The weather is nice today, let's go for a walk in the park."
    # print(f"åŸæ–‡ (è‹±æ–‡): {en_text}")
    # trans_zh = call_qwen_translate(en_text, target_lang="zh")
    # print(f"è¯‘æ–‡ (ä¸­æ–‡): {trans_zh}")
    # print("-" * 20)
