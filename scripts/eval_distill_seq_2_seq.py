import os
import sys
import gc
import psutil
import time
import torch
import logging
import sacrebleu
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.tiny_seq2seq_transformer import TinySeq2SeqTransformer
from config.config import ModelConfig, EvalConfig, MODEL_PATH, OPUS_MT_ZH_EN, OPUS_MT_EN_ZH

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


def get_memory_usage():
    process = psutil.Process()
    return process.memory_info().rss / 1024 / 1024


# ===================================================================
#   åŠ è½½ä»»æ„æ¨¡å‹ï¼šHF model / Tiny Student model
# ===================================================================
def load_any_model(model_path, device="cpu", is_student=False):
    try:
        if is_student:
            logging.info("ğŸ§  åŠ è½½è’¸é¦å­¦ç”Ÿæ¨¡å‹ (TinySeq2SeqTransformer, åŒå‘) ...")

            # âœ… å­¦ç”Ÿæ¨¡å‹ä»æ•™å¸ˆ tokenizer åŠ¨æ€åŠ è½½
            model = TinySeq2SeqTransformer(
                teacher_model_path_zh2en=OPUS_MT_ZH_EN,
                teacher_model_path_en2zh=OPUS_MT_EN_ZH,
                d_model=ModelConfig.CURRENT_CONFIG.get("d_model", 128),
                nhead=ModelConfig.CURRENT_CONFIG.get("nhead", 4),
                num_encoder_layers=ModelConfig.CURRENT_CONFIG.get("num_encoder_layers", 2),
                num_decoder_layers=ModelConfig.CURRENT_CONFIG.get("num_decoder_layers", 2),
                dim_feedforward=ModelConfig.CURRENT_CONFIG.get("dim_feedforward", 256),
                dropout=ModelConfig.CURRENT_CONFIG.get("dropout", 0.1),
                max_seq_len=ModelConfig.MAX_SEQ_LEN,
                share_weights=True,
            ).to(device)

            # âœ… åŠ è½½å­¦ç”Ÿæ¨¡å‹æƒé‡
            state_dict = torch.load(model_path, map_location=device)
            missing, unexpected = model.load_state_dict(state_dict, strict=False)
            if missing:
                logging.warning(f"âš ï¸ Missing keys: {missing}")
            if unexpected:
                logging.warning(f"âš ï¸ Unexpected keys: {unexpected}")

            model.eval()
            logging.info("âœ… å­¦ç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model

        # è¯•å›¾åŠ è½½ HF seq2seq
        try:
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_path, device_map=device, trust_remote_code=True
            )
            logging.info("âœ… HF Seq2Seq æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception:
            model = AutoModelForCausalLM.from_pretrained(
                model_path, device_map=device, trust_remote_code=True
            )
            logging.info("âœ… HF CausalLM æ¨¡å‹åŠ è½½æˆåŠŸ")

        model.eval()
        return model

    except Exception as e:
        logging.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


# ===================================================================
#   è½½å…¥è¯„ä¼°é›†
# ===================================================================
def load_test_dataset(test_data_path, tasks=None, max_samples=100):
    tasks = tasks or EvalConfig.TASKS
    datasets = {}

    dataset = load_dataset("parquet", data_files={"test": test_data_path})["test"]
    dataset = dataset.select(range(min(max_samples, len(dataset))))

    if "translation" not in dataset.column_names:
        raise ValueError(f"âŒ æ•°æ®é›†æ ¼å¼é”™è¯¯: {dataset.column_names}")

    for task in tasks:
        src_lang, tgt_lang = task.split("_to_")
        task_dataset = []

        for item in dataset:
            if src_lang in item["translation"] and tgt_lang in item["translation"]:
                task_dataset.append({
                    "translation": {
                        src_lang: item["translation"][src_lang],
                        tgt_lang: item["translation"][tgt_lang],
                    }
                })

        datasets[task] = task_dataset
        logging.info(f"ğŸ“š {task}: {len(task_dataset)} æ¡æ ·æœ¬")

    return datasets


# ===================================================================
# ç¿»è¯‘å‡½æ•° (å…¼å®¹ Tiny åŒå‘å­¦ç”Ÿæ¨¡å‹)
# ===================================================================
def translate_with_student(model, text, task, device, max_len):
    task_id = 0 if task == "zh_to_en" else 1

    tokenizer = model.tokenizer_zh2en if task_id == 0 else model.tokenizer_en2zh

    encoded = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_len).to(device)

    bos = tokenizer.bos_token_id or tokenizer.cls_token_id or tokenizer.eos_token_id
    eos = tokenizer.eos_token_id
    pad = tokenizer.pad_token_id or eos

    pred_ids = model.generate(
        input_ids=encoded["input_ids"],
        max_length=max_len,
        task_id=task_id,
        num_beams=4,
        bos_token_id=bos,
        eos_token_id=eos,
        pad_token_id=pad,
    )

    return tokenizer.decode(pred_ids[0], skip_special_tokens=True)


def translate_texts(model, texts, task, device, max_len):
    return [translate_with_student(model, text, task, device, max_len) for text in texts]


# ===================================================================
# BLEU è®¡ç®—
# ===================================================================
def compute_bleu(model, dataset, task, device, max_len):
    src_lang, tgt_lang = task.split("_to_")

    src_texts = [item["translation"][src_lang] for item in dataset]
    ref_texts = [item["translation"][tgt_lang] for item in dataset]

    hyps = translate_texts(model, src_texts, task, device, max_len)

    bleu = sacrebleu.corpus_bleu(hyps, [ref_texts])
    return bleu.score


# ===================================================================
# æ¨ç†å»¶è¿Ÿ
# ===================================================================
def measure_inference_latency(model, dataset, task, device, max_len, num_samples=50):
    src_lang = task.split("_to_")[0]
    src_texts = [item["translation"][src_lang] for item in dataset[:num_samples]]

    times = []

    for text in src_texts:
        start = time.time()
        _ = translate_with_student(model, text, task, device, max_len)
        times.append(time.time() - start)

    return sum(times) / len(times) * 1000


# ===================================================================
# ä¸»è¯„ä¼°å…¥å£
# ===================================================================
def main(model_path, is_student=False, tasks=None, max_samples=None, test_data_path=None, device="cpu"):
    tasks = tasks or EvalConfig.TASKS
    max_samples = max_samples or EvalConfig.MAX_EVAL_SAMPLES
    test_data_path = test_data_path or EvalConfig.TEST_DATA_PATH

    logging.info("=" * 60)
    logging.info("ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°")
    logging.info("=" * 60)

    model = load_any_model(model_path, device=device, is_student=is_student)
    datasets = load_test_dataset(test_data_path, tasks, max_samples)
    mem_usage = get_memory_usage()

    results = {}

    for task in tasks:
        logging.info(f"\nğŸ§ª è¯„ä¼°ä»»åŠ¡: {task}")
        bleu_score = compute_bleu(model, datasets[task], task, device, ModelConfig.MAX_SEQ_LEN)
        latency = measure_inference_latency(model, datasets[task], task, device, ModelConfig.MAX_SEQ_LEN)

        results[task] = {"bleu": bleu_score, "latency_ms": latency}
        logging.info(f"âœ… {task}: BLEU={bleu_score:.2f}, å»¶è¿Ÿ={latency:.2f}ms")

    logging.info(f"\nğŸ“Š å†…å­˜å ç”¨: {mem_usage:.2f} MB")

    for task, m in results.items():
        logging.info(f"{task}: BLEU={m['bleu']:.2f}, å»¶è¿Ÿ={m['latency_ms']:.2f}ms")

    return results


# ===================================================================
# CLI å…¥å£
# ===================================================================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="è¯„ä¼°è’¸é¦æ¨¡å‹ï¼ˆä¸­è‹±äº’è¯‘ï¼‰")
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--is_student", action="store_true")
    parser.add_argument("--device", type=str, default="cpu")
    parser.add_argument("--max_samples", type=int, default=EvalConfig.MAX_EVAL_SAMPLES)
    parser.add_argument("--test_data_path", type=str, default=EvalConfig.TEST_DATA_PATH)
    args = parser.parse_args()

    main(
        model_path=args.model_path,
        is_student=args.is_student,
        max_samples=args.max_samples,
        test_data_path=args.test_data_path,
        device=args.device,
    )

