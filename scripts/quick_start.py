#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬ - ä¸€é”®æµ‹è¯•å®Œæ•´æµç¨‹
ç”¨æ³•: python scripts/quick_start.py --mode [test|small|full]
"""
import os
import sys

import torch

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
import time
import subprocess
import shutil
import logging
from config.config import get_config_summary

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº¦"""
    logging.info(f"\n{'=' * 60}")
    logging.info(f"ğŸš€ {description}")
    logging.info(f"{'=' * 60}")
    logging.info(f"å‘½ä»¤: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True, capture_output=False)
        logging.info(f"âœ… {description} å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ {description} å¤±è´¥: {e}")
        return False


def test_mode():
    """æµ‹è¯•æ¨¡å¼: éªŒè¯åŸºç¡€åŠŸèƒ½"""
    logging.info("\n" + "=" * 60)
    logging.info("ğŸ§ª æµ‹è¯•æ¨¡å¼: éªŒè¯åŸºç¡€åŠŸèƒ½")
    logging.info("=" * 60)

    # 1. æ˜¾ç¤ºé…ç½®
    get_config_summary()

    # 2. è¿è¡Œå•å…ƒæµ‹è¯•
    if not run_command(
            ["python", "tests/test_model.py"],
            "å•å…ƒæµ‹è¯•"
    ):
        return False

    # 3. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
    if not run_command(
            ["python", "models/tiny_transformer.py"],
            "æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•"
    ):
        return False

    logging.info("\nâœ… æµ‹è¯•æ¨¡å¼å®Œæˆï¼æ‰€æœ‰åŸºç¡€åŠŸèƒ½æ­£å¸¸")
    return True


def small_mode():
    """å°è§„æ¨¡æ¨¡å¼: 100 æ ·æœ¬ç«¯åˆ°ç«¯æµ‹è¯•"""
    logging.info("\n" + "=" * 60)
    logging.info("ğŸ“¦ å°è§„æ¨¡æ¨¡å¼: 100 æ ·æœ¬ç«¯åˆ°ç«¯æµ‹è¯•")
    logging.info("=" * 60)

    # 1. ç”Ÿæˆ teacher logits (100 æ ·æœ¬)
    if not run_command(
            [
                "python", "scripts/generate_logits_grok.py",
                "--max_samples", "100",
                "--batch_size", "2",
                "--device", "cpu",
                "--int8"
            ],
            "ç”Ÿæˆ Teacher Logits (100 æ ·æœ¬)"
    ):
        return False

    # 2. è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ (100 æ ·æœ¬)
    if not run_command(
            [
                "python", "src/train_distill_amp_grok.py",
                "--max_samples_per_task", "100",
                "--batch_size", "2",
                "--epochs", "2",
                "--device", "cpu"
            ],
            "è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ (100 æ ·æœ¬)"
    ):
        return False

    # 3. è¯„ä¼°æ¨¡å‹
    model_path = "outputs/models/student_model_amp_shard_0_best.pth"
    if os.path.exists(model_path):
        if not run_command(
                [
                    "python", "scripts/evaluate_model.py",
                    "--model_path", model_path,
                    "--max_samples", "50",
                    "--device", "cpu"
                ],
                "è¯„ä¼°å­¦ç”Ÿæ¨¡å‹"
        ):
            return False
    else:
        logging.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    logging.info("\nâœ… å°è§„æ¨¡æ¨¡å¼å®Œæˆï¼")
    logging.info("ğŸ“Š ä¸‹ä¸€æ­¥:")
    logging.info("  1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: logs/")
    logging.info("  2. æŸ¥çœ‹æ¨¡å‹æƒé‡: outputs/models/")
    logging.info("  3. å¦‚éœ€å…¨é‡è®­ç»ƒï¼Œè¿è¡Œ: python scripts/quick_start.py --mode full")
    return True


def full_mode():
    """å¢å¼ºæ—¥å¿—ç‰ˆï¼š26M å…¨æ ·æœ¬è’¸é¦ + è‡ªåŠ¨åˆå¹¶ + é‡åŒ– + è¯„ä¼° + GPUç›‘æ§"""
    logging.info("\n" + "=" * 80)
    logging.info("ğŸš€ å…¨é‡æ¨¡å¼: 26M æ ·æœ¬è’¸é¦è®­ç»ƒï¼ˆå¢å¼ºæ—¥å¿—ç‰ˆï¼‰")
    logging.info("=" * 80)
    logging.warning("âš ï¸ è¯·ç¡®ä¿ç£ç›˜å¯ç”¨ç©ºé—´ â‰¥ 1TBï¼Œè®­ç»ƒè¿‡ç¨‹æŒç»­æ•°å°æ—¶")

    # ======== ç”¨æˆ·ç¡®è®¤ ========
    response = input("\næ˜¯å¦ç»§ç»­æ‰§è¡Œå…¨é‡è’¸é¦è®­ç»ƒï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        logging.info("âŒ ç”¨æˆ·å–æ¶ˆå…¨é‡è®­ç»ƒ")
        return False

    # ======== ç¯å¢ƒè®¾å®š ========
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        logging.warning("âš ï¸ æœªæ£€æµ‹åˆ° CUDAï¼Œå°†åœ¨ CPU ä¸Šè¿è¡Œï¼ˆææ…¢ï¼ï¼‰")

    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "full_mode_run.log")

    # ======== å‚æ•°é…ç½® ========
    max_samples = 26_000_000
    shard_size = 100_000
    batch_size = 8
    noise_std = 0.01

    logging.info(f"ğŸ’¡ å‚æ•°è®¾å®š:")
    logging.info(f"   max_samples = {max_samples:,}")
    logging.info(f"   shard_size  = {shard_size:,}")
    logging.info(f"   batch_size  = {batch_size}")
    logging.info(f"   device      = {device}")
    logging.info(f"   noise_std   = {noise_std}")
    logging.info(f"   æ—¥å¿—æ–‡ä»¶    = {log_file}")

    # ======== è¾…åŠ©å‡½æ•° ========
    def gpu_status():
        """è¯»å–å½“å‰ GPU çŠ¶æ€"""
        try:
            out = subprocess.check_output(
                ["nvidia-smi", "--query-gpu=temperature.gpu,utilization.gpu,memory.used,memory.total,power.draw", "--format=csv,noheader,nounits"],
                text=True
            ).strip().split("\n")[0]
            temp, util, mem_used, mem_total, power = map(float, out.split(", "))
            return f"GPU {util:.0f}% | Mem {mem_used:.0f}/{mem_total:.0f} MB | Temp {temp:.0f}Â°C | Power {power:.0f}W"
        except Exception:
            return "GPU çŠ¶æ€ä¸å¯ç”¨"

    def log_and_time(cmd, desc):
        """æ‰§è¡Œå‘½ä»¤å¹¶è®¡æ—¶ + GPUç›‘æ§"""
        logging.info(f"\n{'=' * 80}")
        logging.info(f"ğŸš€ å¼€å§‹: {desc}")
        logging.info(f"{'=' * 80}")
        logging.info(f"å‘½ä»¤: {' '.join(cmd)}")

        start_time = time.time()
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"\n\n===== {desc} =====\nå‘½ä»¤: {' '.join(cmd)}\n")

        try:
            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
            peak_mem = 0
            for line in proc.stdout:
                line_stripped = line.strip()
                if "CUDA out of memory" in line_stripped:
                    logging.error("ğŸ’¥ æ£€æµ‹åˆ° OOM é”™è¯¯ï¼")
                if "MiB" in line_stripped and "allocated" in line_stripped:
                    try:
                        mem_val = int(line_stripped.split("MiB")[0].split()[-1])
                        peak_mem = max(peak_mem, mem_val)
                    except:
                        pass
                if time.time() % 60 < 1:  # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡GPUçŠ¶æ€
                    logging.info("ğŸ“Š GPUç›‘æ§: " + gpu_status())
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write(line)
            proc.wait()
            end_time = time.time()
            elapsed_min = (end_time - start_time) / 60
            logging.info(f"âœ… {desc} å®Œæˆï¼Œç”¨æ—¶ {elapsed_min:.1f} åˆ†é’Ÿ")
            logging.info(f"ğŸ“ˆ å³°å€¼æ˜¾å­˜çº¦: {peak_mem} MiB")
            return True
        except Exception as e:
            logging.error(f"âŒ {desc} å¤±è´¥: {e}")
            return False

    # ======== Step 1: åè°ƒå¼è’¸é¦è®­ç»ƒ ========
    cmd_train = [
        "python", "src/coordinate_distill.py",
        "--dataset_path", "data/raw/wmt19_zh_en",
        "--max_samples", str(max_samples),
        "--shard_size", str(shard_size),
        "--batch_size", str(batch_size),
        "--device", device,
        "--compile",
        "--simulate_quant_noise",
        "--noise_std", str(noise_std)
    ]
    if not log_and_time(cmd_train, "åè°ƒå¼åˆ†ç‰‡è’¸é¦è®­ç»ƒ"):
        return False

    # ======== Step 2: æ¨¡å‹é‡åŒ– (INT8) ========
    cmd_quant = [
        "python", "scripts/quantize_model.py",
        "--input_model", "outputs/models/student_model_final_merged.pth",
        "--output_model", "outputs/models/student_model_int8.pth",
        "--report_path", "logs/quantization_report.txt"
    ]
    if not log_and_time(cmd_quant, "æ¨¡å‹é‡åŒ– (INT8)"):
        return False

    # ======== Step 3: æ¨¡å‹è¯„ä¼° ========
    models_to_eval = {
        "FP32": "outputs/models/student_model_final_merged.pth",
        "INT8": "outputs/models/student_model_int8.pth"
    }

    for model_name, model_path in models_to_eval.items():
        if os.path.exists(model_path):
            cmd_eval = [
                "python", "scripts/evaluate_model.py",
                "--model_path", model_path,
                "--max_samples", "1000"
            ]
            if "INT8" in model_name:
                cmd_eval.append("--is_int8")
            log_and_time(cmd_eval, f"è¯„ä¼° {model_name} æ¨¡å‹")
        else:
            logging.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹: {model_path}")

    # ======== Step 4: æ±‡æ€»æŠ¥å‘Š ========
    report_path = os.path.join(log_dir, "distill_run_report.txt")
    with open(report_path, "w", encoding="utf-8") as rpt:
        rpt.write("=== RK-LLM Distillation å…¨æµç¨‹æŠ¥å‘Š ===\n")
        rpt.write(f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        rpt.write(f"æ€»æ ·æœ¬æ•°: {max_samples:,}\n")
        rpt.write(f"åˆ†ç‰‡å¤§å°: {shard_size:,}\n")
        rpt.write(f"æ‰¹é‡å¤§å°: {batch_size}\n")
        rpt.write(f"å™ªå£°å¼ºåº¦: {noise_std}\n")
        rpt.write(f"è®¾å¤‡: {device}\n")
        rpt.write(f"GPU çŠ¶æ€: {gpu_status()}\n")
        rpt.write(f"\nè¾“å‡ºæ¨¡å‹:\n")
        for model_name, model_path in models_to_eval.items():
            rpt.write(f"  - {model_name}: {model_path}\n")
        rpt.write("\næŸ¥çœ‹å®Œæ•´æ—¥å¿—: logs/full_mode_run.log\n")

    logging.info("\nâœ… å…¨æµç¨‹å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜:")
    logging.info(f"ğŸ“„ {report_path}")
    logging.info("ğŸ“Š æ—¥å¿—æ–‡ä»¶:")
    logging.info(f"   {log_file}")
    logging.info("ğŸ“¦ æ¨¡å‹è¾“å‡ºç›®å½•: outputs/models/")

    return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¿«é€Ÿå¯åŠ¨è„šæœ¬")
    parser.add_argument(
        "--mode",
        type=str,
        choices=["test", "small", "full"],
        default="test",
        help="è¿è¡Œæ¨¡å¼: test=åŸºç¡€æµ‹è¯•, small=å°è§„æ¨¡è®­ç»ƒ(100æ ·æœ¬), full=å…¨é‡è®­ç»ƒ"
    )
    args = parser.parse_args()

    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨ RK_LLM_Distill é¡¹ç›®å¿«é€Ÿå¯åŠ¨è„šæœ¬")
    print("=" * 60)
    print(f"å½“å‰æ¨¡å¼: {args.mode.upper()}")
    print("=" * 60)

    # è¿è¡Œå¯¹åº”æ¨¡å¼
    if args.mode == "test":
        success = test_mode()
    elif args.mode == "small":
        success = small_mode()
    elif args.mode == "full":
        success = full_mode()
    else:
        logging.error(f"æœªçŸ¥æ¨¡å¼: {args.mode}")
        success = False

    # é€€å‡º
    if success:
        print("\nâœ… ä»»åŠ¡å®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâŒ ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        sys.exit(1)


if __name__ == "__main__":
    main()
