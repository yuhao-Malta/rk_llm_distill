#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬ - ä¸€é”®æµ‹è¯•å®Œæ•´æµç¨‹
ç”¨æ³•: python scripts/quick_start.py --mode [test|small|full]
"""
import os
import sys

import torch

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
import subprocess
import logging
from config.config import get_config_summary

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº¦"""
    logging.info(f"\n{'=' * 60}")
    logging.info(f"ğŸš€ {description}")
    logging.info(f"{'=' * 60}")
    logging.info(f"å‘½ä»¤: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True, capture_output=False)
        logging.info(f"âœ… {description} å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ {description} å¤±è´¥: {e}")
        return False


def test_mode():
    """æµ‹è¯•æ¨¡å¼: éªŒè¯åŸºç¡€åŠŸèƒ½"""
    logging.info("\n" + "=" * 60)
    logging.info("ğŸ§ª æµ‹è¯•æ¨¡å¼: éªŒè¯åŸºç¡€åŠŸèƒ½")
    logging.info("=" * 60)

    # 1. æ˜¾ç¤ºé…ç½®
    get_config_summary()

    # 2. è¿è¡Œå•å…ƒæµ‹è¯•
    if not run_command(
            ["python", "tests/test_model.py"],
            "å•å…ƒæµ‹è¯•"
    ):
        return False

    # 3. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
    if not run_command(
            ["python", "models/tiny_transformer.py"],
            "æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•"
    ):
        return False

    logging.info("\nâœ… æµ‹è¯•æ¨¡å¼å®Œæˆï¼æ‰€æœ‰åŸºç¡€åŠŸèƒ½æ­£å¸¸")
    return True


def small_mode():
    """å°è§„æ¨¡æ¨¡å¼: 100 æ ·æœ¬ç«¯åˆ°ç«¯æµ‹è¯•"""
    logging.info("\n" + "=" * 60)
    logging.info("ğŸ“¦ å°è§„æ¨¡æ¨¡å¼: 100 æ ·æœ¬ç«¯åˆ°ç«¯æµ‹è¯•")
    logging.info("=" * 60)

    # 1. ç”Ÿæˆ teacher logits (100 æ ·æœ¬)
    if not run_command(
            [
                "python", "scripts/generate_logits_grok.py",
                "--max_samples", "100",
                "--batch_size", "2",
                "--device", "cpu",
                "--int8"
            ],
            "ç”Ÿæˆ Teacher Logits (100 æ ·æœ¬)"
    ):
        return False

    # 2. è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ (100 æ ·æœ¬)
    if not run_command(
            [
                "python", "src/train_distill_amp_grok.py",
                "--max_samples_per_task", "100",
                "--batch_size", "2",
                "--epochs", "2",
                "--device", "cpu"
            ],
            "è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ (100 æ ·æœ¬)"
    ):
        return False

    # 3. è¯„ä¼°æ¨¡å‹
    model_path = "outputs/models/student_model_amp_shard_0_best.pth"
    if os.path.exists(model_path):
        if not run_command(
                [
                    "python", "scripts/evaluate_model.py",
                    "--model_path", model_path,
                    "--max_samples", "50",
                    "--device", "cpu"
                ],
                "è¯„ä¼°å­¦ç”Ÿæ¨¡å‹"
        ):
            return False
    else:
        logging.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    logging.info("\nâœ… å°è§„æ¨¡æ¨¡å¼å®Œæˆï¼")
    logging.info("ğŸ“Š ä¸‹ä¸€æ­¥:")
    logging.info("  1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: logs/")
    logging.info("  2. æŸ¥çœ‹æ¨¡å‹æƒé‡: outputs/models/")
    logging.info("  3. å¦‚éœ€å…¨é‡è®­ç»ƒï¼Œè¿è¡Œ: python scripts/quick_start.py --mode full")
    return True


def full_mode():
    """å…¨é‡æ¨¡å¼: å®Œæ•´è®­ç»ƒæµç¨‹"""
    logging.info("\n" + "=" * 60)
    logging.info("ğŸš€ å…¨é‡æ¨¡å¼: å®Œæ•´è®­ç»ƒæµç¨‹")
    logging.info("=" * 60)
    logging.warning("âš ï¸ å…¨é‡è®­ç»ƒéœ€è¦å¤§é‡æ—¶é—´å’Œèµ„æºï¼Œå»ºè®®åœ¨ GPU æœåŠ¡å™¨ä¸Šè¿è¡Œ")

    # ç¡®è®¤
    response = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        logging.info("å·²å–æ¶ˆ")
        return False

    # ======== GPU è®¾å®š ========
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        logging.warning("âš ï¸ æœªæ£€æµ‹åˆ° CUDAï¼Œå°†åœ¨ CPU ä¸Šè¿è¡Œï¼ˆææ…¢ï¼ï¼‰")

    # RTX 5090 32GB æ¨èè®¾ç½®ï¼š
    # - shard_sizeï¼š10~20 ä¸‡æ¡ä¸€ç‰‡
    # - batch_sizeï¼š32ï¼ˆAMPæ¨¡å¼ä¸‹ï¼‰
    # - compileï¼šå¼€å¯ï¼ˆtorch.compile å¯ä¼˜åŒ–æ¨ç†ï¼‰
    # - simulate_quant_noiseï¼šTrue æå‡å­¦ç”Ÿé²æ£’æ€§

    max_samples = 10000
    shard_size = 1000  # çº¦130ä¸ªåˆ†ç‰‡ï¼Œè®­ç»ƒæ•ˆç‡å’Œæ–‡ä»¶ç®¡ç†æ›´å¹³è¡¡
    batch_size = 2 if device == "cuda" else 4
    noise_std = 0.01  # æ¨¡æ‹Ÿé‡åŒ–å™ªå£°å¼ºåº¦

    logging.info(
        f"ğŸ’¡ å‚æ•°è®¾å®š: max_samples={max_samples:,}, shard_size={shard_size:,}, batch_size={batch_size}, device={device}")
    batch_size = 16

    # ======== å¯åŠ¨åè°ƒå¼è’¸é¦è®­ç»ƒ ========
    cmd = [
        "python", "src/coordinate_distill.py",
        "--dataset_path", "data/raw/wmt19_zh_en",
        "--max_samples", str(max_samples),
        "--shard_size", str(shard_size),
        "--batch_size", str(batch_size),
        "--device", device,
        "--compile",
        "--simulate_quant_noise",
        "--noise_std", str(noise_std)
    ]

    if not run_command(cmd, "åè°ƒå¼åˆ†ç‰‡è’¸é¦è®­ç»ƒ"):
        return False

    # ======== æ¨¡å‹é‡åŒ– (INT8) ========
    if not run_command(
            ["python", "scripts/quantize_model.py"],
            "æ¨¡å‹é‡åŒ– (INT8)"
    ):
        return False

    # ======== è¯„ä¼°é˜¶æ®µ ========
    final_models = {
        "best": "outputs/models/student_model_merged_best.pth",
        "int8": "outputs/models/student_model_int8.pth"
    }

    for model_type, path in final_models.items():
        if os.path.exists(path):
            run_command(
                [
                    "python", "scripts/evaluate_model.py",
                    "--model_path", path,
                    "--is_int8" if model_type == "int8" else "",
                    "--max_samples", "1000"
                ],
                f"è¯„ä¼° {model_type.upper()} æ¨¡å‹"
            )

    logging.info("\nâœ… å…¨é‡è’¸é¦æµç¨‹å®Œæˆï¼")
    logging.info("ğŸ“¦ æ¨¡å‹è¾“å‡ºç›®å½•: outputs/models/")
    logging.info("  - FP32 æ¨¡å‹: student_model_merged_best.pth")
    logging.info("  - INT8 æ¨¡å‹: student_model_int8.pth")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¿«é€Ÿå¯åŠ¨è„šæœ¬")
    parser.add_argument(
        "--mode",
        type=str,
        choices=["test", "small", "full"],
        default="test",
        help="è¿è¡Œæ¨¡å¼: test=åŸºç¡€æµ‹è¯•, small=å°è§„æ¨¡è®­ç»ƒ(100æ ·æœ¬), full=å…¨é‡è®­ç»ƒ"
    )
    args = parser.parse_args()

    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨ RK_LLM_Distill é¡¹ç›®å¿«é€Ÿå¯åŠ¨è„šæœ¬")
    print("=" * 60)
    print(f"å½“å‰æ¨¡å¼: {args.mode.upper()}")
    print("=" * 60)

    # è¿è¡Œå¯¹åº”æ¨¡å¼
    if args.mode == "test":
        success = test_mode()
    elif args.mode == "small":
        success = small_mode()
    elif args.mode == "full":
        success = full_mode()
    else:
        logging.error(f"æœªçŸ¥æ¨¡å¼: {args.mode}")
        success = False

    # é€€å‡º
    if success:
        print("\nâœ… ä»»åŠ¡å®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâŒ ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        sys.exit(1)


if __name__ == "__main__":
    main()
