# scripts/analyze_model_size.py
import os
import sys
import torch
import logging
import matplotlib.pyplot as plt
from prettytable import PrettyTable
from colorama import Fore, Style

# å…¼å®¹å¯¼å…¥è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from models.tiny_transformer import TinyTransformer
from config.config import ModelConfig

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)


# ==================== è¾…åŠ©å‡½æ•° ====================
def get_file_size(path):
    return os.path.getsize(path) / 1024 / 1024 if os.path.exists(path) else 0.0


def estimate_memory_usage(param_count, dtype="fp32"):
    bytes_per_param = {"fp32": 4, "fp16": 2, "int8": 1}.get(dtype, 4)
    mem = param_count * bytes_per_param / 1024 / 1024
    return round(mem * 1.8, 2)


def count_parameters_by_module(model):
    stats = {}
    for name, p in model.named_parameters():
        top = name.split('.')[0]
        stats[top] = stats.get(top, 0) + p.numel()
    return stats


def count_parameters_by_layer(model):
    stats = {}
    for name, p in model.named_parameters():
        if "encoder.layers" in name:
            layer_id = name.split('.')[2]
            stats[layer_id] = stats.get(layer_id, 0) + p.numel()
    return stats


def estimate_flops(cfg):
    """ç²—ç•¥ä¼°ç®— FLOPs"""
    d_model = cfg["d_model"]
    num_layers = cfg["num_layers"]
    seq_len = 64  # ä»¥ ModelConfig.MAX_SEQ_LEN ä¸ºåŸºå‡†
    attn_flops = 4 * seq_len * d_model * d_model
    ffn_flops = 2 * seq_len * d_model * d_model * 2
    return (attn_flops + ffn_flops) * num_layers / 1e6  # MFLOPs


# ==================== ä¸»åˆ†æé€»è¾‘ ====================
def analyze_model(model_path=None, breakdown_layer=False):
    cfg = ModelConfig.CURRENT_CONFIG
    vocab_size = ModelConfig.VOCAB_SIZE

    logging.info(f"ğŸ“˜ ä½¿ç”¨æ¨¡å‹é…ç½®: {cfg}")
    logging.info(f"ğŸ“— è¯æ±‡è¡¨å¤§å°: {vocab_size}")

    # æ„å»ºæ¨¡å‹
    model = TinyTransformer(
        vocab_size=vocab_size,
        d_model=cfg["d_model"],
        nhead=cfg["nhead"],
        num_layers=cfg["num_layers"],
        share_weights=cfg.get("share_weights", True),
    )

    # åŠ è½½æƒé‡
    if model_path and os.path.exists(model_path):
        try:
            state_dict = torch.load(model_path, map_location="cpu")
            model.load_state_dict(state_dict, strict=False)
            logging.info(f"âœ… æˆåŠŸåŠ è½½æƒé‡æ–‡ä»¶: {model_path}")
        except Exception as e:
            logging.warning(f"âš ï¸ æ— æ³•åŠ è½½æƒé‡æ–‡ä»¶ ({e})ï¼Œä»…è¿›è¡Œç»“æ„åˆ†æã€‚")

    total_params = sum(p.numel() for p in model.parameters())
    total_params_m = total_params / 1e6
    file_size = get_file_size(model_path)

    # æ¨¡å—åˆ†å¸ƒ
    sub_stats = count_parameters_by_module(model)
    sub_total = sum(sub_stats.values())
    sub_stats = {k: v / sub_total * 100 for k, v in sub_stats.items()}

    # å†…å­˜å ç”¨ä¼°ç®—
    fp32_size = total_params * 4 / 1024 / 1024
    fp16_size = total_params * 2 / 1024 / 1024
    int8_size = total_params * 1 / 1024 / 1024
    mem_int8 = estimate_memory_usage(total_params, "int8")

    # FLOPs
    total_flops = estimate_flops(cfg)

    # ==================== ä¸»è¡¨æ ¼ ====================
    table = PrettyTable()
    table.field_names = ["ç²¾åº¦ç±»å‹", "å‚æ•°é‡ (M)", "æ–‡ä»¶å¤§å° (MB)", "æ¨ç†å†…å­˜ (MB)"]
    table.add_row(["FP32", f"{total_params_m:.2f}", f"{fp32_size:.2f}", f"{estimate_memory_usage(total_params, 'fp32'):.2f}"])
    table.add_row(["FP16", f"{total_params_m:.2f}", f"{fp16_size:.2f}", f"{estimate_memory_usage(total_params, 'fp16'):.2f}"])
    table.add_row(["INT8", f"{total_params_m:.2f}", f"{int8_size:.2f}", f"{mem_int8:.2f}"])

    print("\n" + "=" * 65)
    print(Fore.CYAN + "ğŸ“Š TinyTransformer æ¨¡å‹åˆ†ææŠ¥å‘Š (å…¼å®¹ç‰ˆ)" + Style.RESET_ALL)
    print("=" * 65)
    print(table)

    # æ¨¡å—å æ¯”
    print("\n" + Fore.YELLOW + "ğŸ” æ¨¡å—å‚æ•°åˆ†å¸ƒï¼š" + Style.RESET_ALL)
    detail = PrettyTable()
    detail.field_names = ["æ¨¡å—", "å‚æ•°å æ¯” (%)"]
    for name, pct in sorted(sub_stats.items(), key=lambda x: x[1], reverse=True):
        color = Fore.GREEN if "encoder" in name else Fore.CYAN
        detail.add_row([color + name + Style.RESET_ALL, f"{pct:.2f}"])
    print(detail)

    # åˆ†å±‚åˆ†æ
    if breakdown_layer:
        layer_stats = count_parameters_by_layer(model)
        if layer_stats:
            print("\n" + Fore.MAGENTA + "ğŸ“ Encoder å±‚å‚æ•°åˆ†æï¼š" + Style.RESET_ALL)
            layer_table = PrettyTable()
            layer_table.field_names = ["å±‚ç¼–å·", "å‚æ•°é‡ (K)", "å æ¯” (%)"]
            total = sum(layer_stats.values())
            for lid, count in layer_stats.items():
                layer_table.add_row([lid, f"{count / 1e3:.1f}", f"{count / total * 100:.2f}"])
            print(layer_table)

            plt.figure(figsize=(8, 4))
            plt.bar(range(len(layer_stats)), [v / 1e6 for v in layer_stats.values()])
            plt.xlabel("Encoder Layer ID")
            plt.ylabel("Params (Million)")
            plt.title("TinyTransformer æ¯å±‚å‚æ•°åˆ†å¸ƒ")
            plt.tight_layout()
            plt.show()

    # ==================== æ±‡æ€» ====================
    print("\n" + "-" * 65)
    print(f"ğŸ“ æƒé‡æ–‡ä»¶: {model_path or 'æœªæä¾›'} ({file_size:.2f} MB)")
    print(f"âš™ï¸ æ€»å‚æ•°é‡: {total_params_m:.2f}M")
    print(f"ğŸ§® ä¼°ç®— FLOPs: {total_flops:.1f} MFLOPs")
    print("-" * 65)

    print(Fore.GREEN + "ğŸ’¡ éƒ¨ç½²å»ºè®®ï¼š" + Style.RESET_ALL)
    if mem_int8 < 50:
        print("âœ… é€‚åˆ RV1126B NPU å®æ—¶éƒ¨ç½²ï¼ˆINT8ï¼‰")
    elif mem_int8 < 150:
        print("âš ï¸ å¯è¿è¡Œäº Jetson Nano / RK3588")
    else:
        print("âŒ æ¨¡å‹è¿‡å¤§ï¼Œå»ºè®®è¿›ä¸€æ­¥è’¸é¦æˆ–å‰ªæ")

    print(f"ğŸ—ï¸ éƒ¨ç½²æ ¼å¼å»ºè®®: ONNX INT8 / RKNN int8")
    print(f"ğŸ“¦ æ¨ç†å†…å­˜é¢„ä¼°: {mem_int8:.2f} MB")
    print("=" * 65 + "\n")


# ==================== ä¸»å…¥å£ ====================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Analyze TinyTransformer model size and structure (compatible version)")
    parser.add_argument("--model_path", type=str, default="outputs/models/student_model_amp_shard_0_best.pth")
    parser.add_argument("--breakdown-layer", action="store_true", help="Show per-layer breakdown")
    args = parser.parse_args()

    analyze_model(args.model_path, breakdown_layer=args.breakdown_layer)