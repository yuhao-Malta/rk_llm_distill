#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆ RKNN é‡åŒ–æ ¡å‡†æ•°æ®é›†
ä» teacher_logits ç›®å½•æˆ–è®­ç»ƒæ ·æœ¬ä¸­éšæœºæŠ½å– 200 æ¡è¾“å…¥æ ·æœ¬
è¾“å‡º:
  - calibration_inputs.npy  (tokenized è¾“å…¥)
  - calibration_texts.txt   (åŸå§‹æ–‡æœ¬)
"""

import os
import torch
import numpy as np
import random
import logging
from config.config import TEACHER_LOGITS_DIR, ModelConfig
from models.tiny_seq2seq_transformer import TinySeq2SeqTransformer as TinyTransformer


# ========================
# æ—¥å¿—é…ç½®
# ========================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)


# ========================
# ä¸»å‡½æ•°
# ========================
def generate_calibration_dataset(
    teacher_logits_dir=TEACHER_LOGITS_DIR,
    output_dir="calibration_dataset",
    max_samples=200,
    max_seq_len=64
):
    os.makedirs(output_dir, exist_ok=True)

    logging.info(f"ğŸ“¦ ä» {teacher_logits_dir} åŠ è½½è’¸é¦æ ·æœ¬ ...")
    all_files = [
        f for f in os.listdir(teacher_logits_dir)
        if f.endswith(".pt")
    ]
    if not all_files:
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ° teacher_logits æ–‡ä»¶äº {teacher_logits_dir}")

    model = TinyTransformer(
        vocab_size=ModelConfig.VOCAB_SIZE,
        max_seq_len=max_seq_len,
        **ModelConfig.CURRENT_CONFIG
    )

    # ç”¨äºå­˜å‚¨æ ·æœ¬
    all_inputs = []
    all_texts = []

    for f in all_files:
        data_path = os.path.join(teacher_logits_dir, f)
        data = torch.load(data_path)
        logging.info(f"âœ… åŠ è½½ {f}, æ ·æœ¬æ•°: {len(data)}")

        for sample in data:
            if "src_input_ids" in sample:
                all_inputs.append(sample["src_input_ids"][:max_seq_len].unsqueeze(0))
            if "src_text" in sample:
                all_texts.append(sample["src_text"])

            if len(all_inputs) >= max_samples:
                break
        if len(all_inputs) >= max_samples:
            break

    if not all_inputs:
        raise ValueError("âŒ æœªæå–åˆ°ä»»ä½•è¾“å…¥æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚")

    # æ‹¼æ¥ä¸ºå•ä¸ª tensor
    calib_tensor = torch.cat(all_inputs, dim=0)
    np.save(os.path.join(output_dir, "calibration_inputs.npy"), calib_tensor.numpy())
    logging.info(f"ğŸ’¾ ä¿å­˜ calibration_inputs.npy, å½¢çŠ¶: {calib_tensor.shape}")

    # ä¿å­˜åŸå§‹æ–‡æœ¬
    if all_texts:
        with open(os.path.join(output_dir, "calibration_texts.txt"), "w", encoding="utf-8") as ftxt:
            for t in all_texts[:max_samples]:
                ftxt.write(t.strip() + "\n")
        logging.info(f"ğŸ’¾ ä¿å­˜ calibration_texts.txt ({len(all_texts[:max_samples])} æ¡)")
    else:
        logging.warning("âš ï¸ æ•°æ®ä¸­æœªåŒ…å« src_text å­—æ®µï¼Œä»…ä¿å­˜ tokenized è¾“å…¥ã€‚")

    logging.info(f"ğŸ‰ æ ¡å‡†æ•°æ®ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºç›®å½•: {output_dir}")


if __name__ == "__main__":
    generate_calibration_dataset()