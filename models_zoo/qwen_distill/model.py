import os
import time
import torch
from transformers import AutoTokenizer
from models_zoo.base_model import BaseModel
from models.tiny_transformer import TinyTransformer  # ğŸ‘ˆ å¯¼å…¥ä½ åŸæœ‰çš„æ¨¡å‹å®šä¹‰


class QwenDistillModel(BaseModel):
    def __init__(self, model_path="models/qwen_distill_80m.pth"):
        """
        åˆå§‹åŒ– Qwen è’¸é¦æ¨¡å‹
        :param model_path: è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.load_model()

    def load_model(self):
        """åŠ è½½ Qwen è’¸é¦æ¨¡å‹æƒé‡"""
        print(f"ğŸ“¥ åŠ è½½ Qwen è’¸é¦æ¨¡å‹æƒé‡: {self.model_path}")

        # 1. åˆå§‹åŒ–æ¨¡å‹æ¶æ„ï¼ˆå¤ç”¨ tiny_transformer.pyï¼‰
        self.model = TinyTransformer()  # ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œæˆ–æ ¹æ®ä½ çš„è®­ç»ƒé…ç½®è°ƒæ•´

        # 2. åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        if os.path.exists(self.model_path):
            # åŠ è½½æƒé‡ï¼ˆå‡è®¾æ˜¯ .pth æ–‡ä»¶ï¼‰
            checkpoint = torch.load(self.model_path, map_location='cpu')  # å…ˆåŠ è½½åˆ° CPUï¼Œé¿å…æ˜¾å­˜é—®é¢˜
            self.model.load_state_dict(checkpoint)
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼")
        else:
            print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹æƒé‡æ–‡ä»¶ {self.model_path} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼")
            # ä½ å¯ä»¥é€‰æ‹©åœ¨è¿™é‡ŒæŠ›å‡ºå¼‚å¸¸ï¼Œæˆ–ç»§ç»­ä½¿ç”¨éšæœºæƒé‡è¿›è¡Œæµ‹è¯•
            # raise FileNotFoundError(f"æ¨¡å‹æƒé‡æ–‡ä»¶ {self.model_path} ä¸å­˜åœ¨ï¼")

        # 3. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # 4. åŠ è½½ Qwen Tokenizerï¼ˆç”¨äºç¼–ç /è§£ç ï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-1.8B", trust_remote_code=True)
        print("âœ… Qwen Tokenizer åŠ è½½æˆåŠŸï¼")

    def translate(self, text: str, src_lang: str = "zh", tgt_lang: str = "en") -> str:
        """ç¿»è¯‘æ–‡æœ¬ï¼ˆä¸­è‹±äº’è¯‘ï¼‰"""
        start_time = time.time()

        try:
            # 1. ç¼–ç è¾“å…¥æ–‡æœ¬
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=64)

            # 2. æ¨¡å‹æ¨ç†ï¼ˆå‰å‘è®¡ç®—ï¼‰
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿæ¨ç†
                outputs = self.model(inputs.input_ids)
                logits = outputs["logits"]

            # 3. è§£ç è¾“å‡ºï¼ˆè´ªå¿ƒè§£ç ï¼‰
            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token
            predicted_ids = torch.argmax(logits, dim=-1)
            # å°† token ID è½¬æ¢ä¸ºæ–‡æœ¬
            translated = self.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]

            # 4. è®°å½•å»¶è¿Ÿ
            self.latency = (time.time() - start_time) * 1000  # ms

            return translated

        except Exception as e:
            print(f"âŒ ç¿»è¯‘å‡ºé”™: {str(e)}")
            return "ç¿»è¯‘å¤±è´¥"

    def get_model_size(self) -> str:
        """è¿”å›æ¨¡å‹å¤§å°ï¼ˆå‚æ•°é‡ï¼‰"""
        param_size = sum(p.numel() for p in self.model.parameters()) * 4 / 1024 / 1024  # MB
        return f"{param_size:.1f}MB"

    def get_latency(self) -> float:
        """è¿”å›æ¨ç†å»¶è¿Ÿ"""
        return getattr(self, 'latency', 0.0)