def generate_teacher_logits(args):
    """
    å¢å¼ºç‰ˆï¼šç”Ÿæˆ Teacher æ¨¡å‹ logits æ–‡ä»¶
    âœ… åŠ å…¥æ˜¾å­˜ä¼˜åŒ– / è‡ªåŠ¨ batch_size å›é€€ / æ­»é”é˜²æŠ¤ / åŠç²¾åº¦æ”¯æŒ
    """
    process = psutil.Process()
    logging.info(f"åˆå§‹å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")

    # ===== 1. åŠ è½½ tokenizer =====
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, local_files_only=True, trust_remote_code=True
        )
        logging.info("âœ… æˆåŠŸåŠ è½½ Qwen Tokenizer")
    except Exception as e:
        logging.error(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        raise

    # ===== 2. åŠ è½½æ¨¡å‹ =====
    model = None
    device = torch.device(args.device)
    if not args.use_api:
        try:
            check_model_files(MODEL_PATH)
            logging.info(f"ğŸ“¥ åŠ è½½ Qwen æ¨¡å‹åˆ° {device}...")
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    MODEL_PATH,
                    device_map=device,
                    torch_dtype=torch.float16,
                    local_files_only=True,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_safetensors=True
                )
                logging.info("âœ… æˆåŠŸåŠ è½½æ¨¡å‹ (float16 åŠç²¾åº¦)")
            except Exception as e:
                logging.warning(f"âš ï¸ åŠ è½½ float16 æ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ° float32: {e}")
                model = AutoModelForCausalLM.from_pretrained(
                    MODEL_PATH,
                    device_map=device,
                    torch_dtype=torch.float32,
                    local_files_only=True,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_safetensors=True
                )
                logging.info("âœ… å›é€€åˆ° float32 ç²¾åº¦æ¨¡å‹")

            model.eval()

            # âœ… å¯ç”¨ TF32 + cuDNN benchmark
            if device.type == "cuda":
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.benchmark = True
                logging.info("ğŸ’¡ å¯ç”¨ TF32 ä¸ cuDNN Benchmark ä»¥ä¼˜åŒ– CUDA ç¨³å®šæ€§")
                logging.info("ğŸ“Š åˆå§‹CUDAå†…å­˜æ‘˜è¦ï¼š")
                logging.info(torch.cuda.memory_summary(device=device, abbreviated=True))
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    # ===== 3. åŠ è½½æ•°æ®é›† =====
    try:
        dataset_path = os.path.join(RAW_DATA_PATH, "train/*.parquet")
        dataset = load_dataset("parquet", data_files={"train": dataset_path})["train"]
        total_samples = len(dataset)
        logging.info(f"âœ… åŠ è½½WMT19æ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {total_samples}")
    except Exception as e:
        logging.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        raise

    total_samples = min(args.max_samples or total_samples, total_samples)
    shard_size = min(args.shard_size, total_samples)

    success_count, fail_count = 0, 0

    # ===== 4. åŒå‘ç¿»è¯‘ä»»åŠ¡ =====
    for src_lang, tgt_lang, task_id, output_prefix in [
        ("zh", "en", 0, os.path.join(TEACHER_LOGITS_DIR, "zh_to_en")),
        ("en", "zh", 1, os.path.join(TEACHER_LOGITS_DIR, "en_to_zh"))
    ]:
        logging.info(f"ğŸ§  ç”Ÿæˆ {src_lang}â†’{tgt_lang} logits (ä»»åŠ¡ID: {task_id})")

        start_idx = args.start_from
        end_idx = min(start_idx + args.shard_size, total_samples)
        shard_dataset = dataset.select(range(start_idx, end_idx))
        if len(shard_dataset) == 0:
            logging.warning(f"âš ï¸ åˆ†ç‰‡ {args.shard_idx} æ— æ ·æœ¬ï¼Œè·³è¿‡ã€‚")
            continue

        # æ„å»º DataLoader
        shard_dataloader = DataLoader(
            TranslationDataset(
                shard_dataset, tokenizer,
                max_seq_len=args.max_seq_len,
                src_lang=src_lang, tgt_lang=tgt_lang, task_id=task_id
            ),
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=(device == "cuda"),
            collate_fn=lambda b: custom_collate_fn(
                b, max_seq_len=args.max_seq_len, pad_token_id=tokenizer.pad_token_id
            )
        )

        output_file = f"{output_prefix}_shard_{args.shard_idx}.{'jsonl' if args.use_api else 'pt'}"
        output_data = []

        # ===== 5. ä¸»å¾ªç¯ =====
        with torch.no_grad():
            for batch_idx, batch in enumerate(
                tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {args.shard_idx}")
            ):
                batch_start = time.time()
                try:
                    src_input_ids = batch["src_input_ids"].to(device)
                    src_attention_mask = batch["src_attention_mask"].to(device)

                    with torch.cuda.amp.autocast(dtype=torch.float16 if device.type == "cuda" else torch.float32):
                        outputs = model(input_ids=src_input_ids, attention_mask=src_attention_mask)
                        logits = outputs.logits.detach().cpu()

                    # é‡Šæ”¾æ˜¾å­˜
                    del outputs
                    torch.cuda.empty_cache()
                    gc.collect()

                    for i in range(len(batch["id"])):
              
