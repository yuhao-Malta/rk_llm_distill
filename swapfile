def generate_teacher_logits(args):
    """
    å¢å¼ºç‰ˆï¼šç”Ÿæˆ Teacher æ¨¡å‹ logits æ–‡ä»¶
    âœ… åŠ å…¥æ˜¾å­˜ä¼˜åŒ– / è‡ªåŠ¨ batch_size å›é€€ / æ­»é”é˜²æŠ¤ / åŠç²¾åº¦æ”¯æŒ
    """
    process = psutil.Process()
    logging.info(f"åˆå§‹å†…å­˜: {process.memory_info().rss / 1024 ** 2:.2f} MB")

    # ===== 1. åŠ è½½ tokenizer =====
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, local_files_only=True, trust_remote_code=True
        )
        logging.info("âœ… æˆåŠŸåŠ è½½ Qwen Tokenizer")
    except Exception as e:
        logging.error(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        raise

    # ===== 2. åŠ è½½æ¨¡å‹ =====
    model = None
    device = torch.device(args.device)
    if not args.use_api:
        try:
            check_model_files(MODEL_PATH)
            logging.info(f"ğŸ“¥ åŠ è½½ Qwen æ¨¡å‹åˆ° {device}...")
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    MODEL_PATH,
                    device_map=device,
                    torch_dtype=torch.float16,
                    local_files_only=True,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_safetensors=True
                )
                logging.info("âœ… æˆåŠŸåŠ è½½æ¨¡å‹ (float16 åŠç²¾åº¦)")
            except Exception as e:
                logging.warning(f"âš ï¸ åŠ è½½ float16 æ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ° float32: {e}")
                model = AutoModelForCausalLM.from_pretrained(
                    MODEL_PATH,
                    device_map=device,
                    torch_dtype=torch.float32,
                    local_files_only=True,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_safetensors=True
                )
                logging.info("âœ… å›é€€åˆ° float32 ç²¾åº¦æ¨¡å‹")

            model.eval()

            # âœ… å¯ç”¨ TF32 + cuDNN benchmark
            if device.type == "cuda":
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.benchmark = True
                logging.info("ğŸ’¡ å¯ç”¨ TF32 ä¸ cuDNN Benchmark ä»¥ä¼˜åŒ– CUDA ç¨³å®šæ€§")
                logging.info("ğŸ“Š åˆå§‹CUDAå†…å­˜æ‘˜è¦ï¼š")
                logging.info(torch.cuda.memory_summary(device=device, abbreviated=True))
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    # ===== 3. åŠ è½½æ•°æ®é›† =====
    try:
        dataset_path = os.path.join(RAW_DATA_PATH, "train/*.parquet")
        dataset = load_dataset("parquet", data_files={"train": dataset_path})["train"]
        total_samples = len(dataset)
        logging.info(f"âœ… åŠ è½½WMT19æ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {total_samples}")
    except Exception as e:
        logging.error(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        raise

    total_samples = min(args.max_samples or total_samples, total_samples)
    shard_size = min(args.shard_size, total_samples)

    success_count, fail_count = 0, 0

    # ===== 4. åŒå‘ç¿»è¯‘ä»»åŠ¡ =====
    for src_lang, tgt_lang, task_id, output_prefix in [
        ("zh", "en", 0, os.path.join(TEACHER_LOGITS_DIR, "zh_to_en")),
        ("en", "zh", 1, os.path.join(TEACHER_LOGITS_DIR, "en_to_zh"))
    ]:
        logging.info(f"ğŸ§  ç”Ÿæˆ {src_lang}â†’{tgt_lang} logits (ä»»åŠ¡ID: {task_id})")

        start_idx = args.start_from
        end_idx = min(start_idx + args.shard_size, total_samples)
        shard_dataset = dataset.select(range(start_idx, end_idx))
        if len(shard_dataset) == 0:
            logging.warning(f"âš ï¸ åˆ†ç‰‡ {args.shard_idx} æ— æ ·æœ¬ï¼Œè·³è¿‡ã€‚")
            continue

        # æ„å»º DataLoader
        shard_dataloader = DataLoader(
            TranslationDataset(
                shard_dataset, tokenizer,
                max_seq_len=args.max_seq_len,
                src_lang=src_lang, tgt_lang=tgt_lang, task_id=task_id
            ),
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=(device == "cuda"),
            collate_fn=lambda b: custom_collate_fn(
                b, max_seq_len=args.max_seq_len, pad_token_id=tokenizer.pad_token_id
            )
        )

        output_file = f"{output_prefix}_shard_{args.shard_idx}.{'jsonl' if args.use_api else 'pt'}"
        output_data = []

    # == == = 5. ä¸»å¾ªç¯ == == =
    with torch.no_grad():
        for batch_idx, batch in enumerate(
                tqdm(shard_dataloader, desc=f"{src_lang}â†’{tgt_lang} åˆ†ç‰‡ {args.shard_idx}")
        ):
            batch_start = time.time()
            try:
                src_input_ids = batch["src_input_ids"].to(device)
                src_attention_mask = batch["src_attention_mask"].to(device)

                with torch.cuda.amp.autocast(dtype=torch.float16 if device.type == "cuda" else torch.float32):
                    outputs = model(input_ids=src_input_ids, attention_mask=src_attention_mask)
                    logits = outputs.logits.detach().cpu()

                # é‡Šæ”¾æ˜¾å­˜
                del outputs
                torch.cuda.empty_cache()
                gc.collect()

                for i in range(len(batch["id"])):
                    output_data.append({
                        "id": batch["id"][i].item(),
                        "src_text": batch["src_text"][i],
                        "tgt_text": batch["tgt_text"][i],
                        "src_input_ids": batch["src_input_ids"][i].cpu(),
                        "src_attention_mask": batch["src_attention_mask"][i].cpu(),
                        "tgt_input_ids": batch["tgt_input_ids"][i].cpu(),
                        "tgt_attention_mask": batch["tgt_attention_mask"][i].cpu(),
                        "task_id": batch["task_id"][i].item(),
                        "logits": logits[i]
                    })
                success_count += len(batch["id"])

                # æ¯ 10 æ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢è¿‡å¤§
                if (batch_idx + 1) % 10 == 0:
                    torch.save(output_data, output_file)
                    output_data.clear()
                    gc.collect()
                    torch.cuda.empty_cache()
                    logging.info(f"ğŸ’¾ ä¸´æ—¶ä¿å­˜ {output_file} (åˆ°ç¬¬ {batch_idx + 1} æ‰¹æ¬¡)")

                # â±ï¸ è¶…æ—¶æ£€æµ‹ watchdog
                elapsed = time.time() - batch_start
                if elapsed > 120:
                    logging.warning(f"âš ï¸ Batch {batch_idx} è¶…æ—¶ {elapsed:.1f}sï¼Œå¼ºåˆ¶æ¸…ç†CUDAä¸Šä¸‹æ–‡")
                    torch.cuda.empty_cache()
                    gc.collect()

            except torch.cuda.OutOfMemoryError:
                logging.error(f"ğŸ’¥ CUDA OOM at batch {batch_idx}! è‡ªåŠ¨å›é€€ batch_size...")
                torch.cuda.empty_cache()
                if args.batch_size > 1:
                    args.batch_size = max(1, args.batch_size // 2)
                    logging.warning(f"âš™ï¸ æ–° batch_size={args.batch_size}ï¼Œé‡æ–°æ„å»º DataLoader")
                    shard_dataloader = DataLoader(
                        TranslationDataset(
                            shard_dataset, tokenizer,
                            max_seq_len=args.max_seq_len,
                            src_lang=src_lang, tgt_lang=tgt_lang, task_id=task_id
                        ),
                        batch_size=args.batch_size,
                        shuffle=False,
                        num_workers=0,
                        pin_memory=(device == "cuda"),
                        collate_fn=lambda b: custom_collate_fn(
                            b, max_seq_len=args.max_seq_len, pad_token_id=tokenizer.pad_token_id
                        )
                    )
                    break
                else:
                    logging.error("âŒ batch_size=1 ä»æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥åˆ†ç‰‡ã€‚")
                    break

            except RuntimeError as e:
                logging.error(f"âš ï¸ RuntimeError (å¯èƒ½æ­»é”æˆ–é©±åŠ¨é”™è¯¯): {e}")
                torch.cuda.empty_cache()
                gc.collect()
                time.sleep(2)
                continue

            except Exception as e:
                logging.error(f"âŒ Batch {batch_idx} å¤„ç†å¤±è´¥: {e}")
                torch.cuda.empty_cache()
                gc.collect()
                continue

    # ===== 6. ä¿å­˜ç»“æœå¹¶éªŒè¯ =====
    if len(output_data) > 0:
        torch.save(output_data, output_file)
    logging.info(f"ğŸ’¾ ä¿å­˜åˆ†ç‰‡: {output_file}")
    validate_logits_file(output_file)
    del output_data
    gc.collect()
    torch.cuda.empty_cache()

    # ===== 7. æ¸…ç† =====
    if model is not None:
        del model
        gc.collect()
        torch.cuda.empty_cache()

    logging.info(f"ğŸ‰ å®Œæˆï¼æˆåŠŸ: {success_count} å¤±è´¥: {fail_count}")
    return success_count, fail_count