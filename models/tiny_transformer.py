# models/tiny_transformer.py
import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM


class TinyTransformer(nn.Module):
    """
    è½»é‡çº§ Transformer å­¦ç”Ÿæ¨¡å‹ï¼ˆç›®æ ‡ï¼š50M~80M å‚æ•°ï¼‰
    æ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ï¼ˆä¸­â†’è‹±ã€è‹±â†’ä¸­ï¼‰ + attention_maskæ¥å¤„ç†åºåˆ—å¡«å……
    æ¨¡å‹ä½¿ç”¨QWen tokenizerï¼ˆä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼Œä»¥æ”¯æŒç¦»çº¿ä½¿ç”¨ï¼‰
    åŸºäºPyTorchçš„nn.TransformerEncoderæ„å»ºç¼–ç å™¨éƒ¨åˆ†
    """

    def __init__(self, d_model=512, nhead=8, num_layers=6, max_seq_len=64):
        """
        :param d_model: éšè—ç»´åº¦
        :param nhead: æ³¨æ„åŠ›å¤´æ•°
        :param num_layers: Transformerå±‚æ•°
        :param max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        super().__init__()
        # self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len

        # âœ… æŒ‡å®šæœ¬åœ° tokenizer è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼Œè®¡ç®—é¡¹ç›®æ ¹ç›®å½•å¹¶æ‹¼æ¥æœ¬åœ°è·¯å¾„
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        TOKENIZER_PATH = os.path.join(PROJECT_ROOT, "models", "qwen_tokenizer_offline", "qwen", "Qwen1___5-1___8B")

        # âœ… æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(TOKENIZER_PATH):
            raise FileNotFoundError(f"âŒ æœ¬åœ° tokenizer è·¯å¾„ä¸å­˜åœ¨: {TOKENIZER_PATH}")

        # âœ… ä½¿ç”¨AutoTokenizer.from_pretrainedä»æœ¬åœ°è·¯å¾„åŠ è½½ tokenizerã€‚ä¸QWenæ•™å¸ˆæ¨¡å‹å…±äº«tokenizerï¼Œä¾¿äºè’¸é¦æ—¶tokenå¯¹é½
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                TOKENIZER_PATH,  # ğŸ‘ˆ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æœ¬åœ°è·¯å¾„
                local_files_only=True,  # ğŸ‘ˆ å¼ºåˆ¶ç¦»çº¿åŠ è½½
                trust_remote_code=True
            )
            print("âœ… æˆåŠŸç¦»çº¿åŠ è½½æœ¬åœ° Qwen Tokenizerï¼")
        except Exception as e:
            print(f"âŒ ç¦»çº¿åŠ è½½æœ¬åœ° tokenizer å¤±è´¥: {e}")
            raise RuntimeError("æ— æ³•åŠ è½½æœ¬åœ° Qwen Tokenizerï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼")

        # âœ… è·å–çœŸå®è¯æ±‡è¡¨å¤§å°ï¼Œç”¨äºåç»§Embeddingåˆå§‹åŒ–
        self.vocab_size = len(self.tokenizer)
        print(f"âœ… Qwen Tokenizer è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")  # åº”è¾“å‡º 151643 æˆ–ç±»ä¼¼

        # âœ… ä½¿ç”¨çœŸå®è¯æ±‡è¡¨å¤§å°åˆå§‹åŒ– Embeddingï¼Œè¾“å…¥tokenåˆ°d_modelç»´åµŒå…¥
        self.embed = nn.Embedding(self.vocab_size, d_model)  # å‚æ•°é‡ï¼š151643*512=77.6M å‚æ•°
        # å¯å­¦ä¹ ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, d_model))  # 0 å‚æ•°ï¼ˆbufferï¼‰

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=2048,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer,
                                             num_layers)  # æ¯å±‚çº¦3.15Mï¼Œ6 å±‚ Transformer çº¦ 18.9M å‚æ•°ï¼ˆåŒ…æ‹¬è‡ªæ³¨æ„åŠ›ã€FFNç­‰ï¼‰

        # ä»»åŠ¡æ ‡è¯†åµŒå…¥ï¼ˆ0=ä¸­â†’è‹±, 1=è‹±â†’ä¸­ï¼‰
        self.task_embed = nn.Embedding(2, d_model)  # 2*512=1K å‚æ•°

        # è¾“å‡ºå¤´ã€‚ä»éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨
        self.lm_head = nn.Linear(d_model, self.vocab_size, bias=False)  # 77.6M å‚æ•°ï¼ˆå¯ä»¥ä¸embedå…±äº«æƒé‡ï¼‰

        self.init_weights()
        # æƒé‡å…±äº«ï¼šLM Head ç›´æ¥å¤ç”¨ Embed çš„æƒé‡ï¼ˆå½¢çŠ¶ç›¸åŒï¼Œæ— éœ€è½¬ç½®ï¼‰
        # self.lm_head.weight = self.embed.weight  # å…±äº«åŒä¸€å¼ é‡ï¼ˆé Parameter åŒ…è£…ï¼Œé¿å…å¤šä½™ overheadï¼‰ã€‚å¯ä»¥ç¼©å°å‚é‡ï¼Œä½†æœ€å¥½æ•™å¸ˆæ¨¡å‹ä¹Ÿå…±äº«æƒé‡

    def init_weights(self):
        """
        åˆå§‹åŒ–æƒé‡ã€‚ä½¿ç”¨æ­£æ³°åˆ†å¸ƒï¼ˆstd=0.02ï¼‰åˆå§‹åŒ–embedã€lm_headå’Œpos_embed
        Xavier/Glorotåˆå§‹åŒ–æœªç”¨ï¼Œé€‚åˆTransformer
        """
        nn.init.normal_(self.embed.weight, std=0.02)
        nn.init.normal_(self.lm_head.weight, std=0.02)
        if self.pos_embed is not None:
            nn.init.normal_(self.pos_embed, std=0.02)

    def forward(self, input_ids, task_id=None, attention_mask=None):
        """
        å‰å‘ä¼ æ’­ã€‚è¿™æ˜¯æ¨¡å‹çš„æ ¸å¿ƒé€»è¾‘ï¼Œæ”¯æŒå› æœè¯­è¨€å»ºæ¨¡ï¼ˆCausal LMï¼‰ï¼Œä½†ä½¿ç”¨Encoderè€ŒéDecoderï¼ˆæ— è‡ªå›å½’maskï¼Œé€‚åˆç¼–ç ä»»åŠ¡å¦‚ç¿»è¯‘ï¼‰
        :param input_ids: (batch_size, seq_len) token IDå¼ é‡
        :param task_id: (batch_size,) ä»»åŠ¡æ ‡è¯†ï¼Œç”¨äºåŒºåˆ†ç¿»è¯‘æ–¹å‘
        :param attention_mask: (batch_size, seq_len) å¯é€‰ï¼Œç”¨äºå±è”½å¡«å……éƒ¨åˆ†ï¼ˆ1=å…³æ³¨ï¼Œ0=æ©ç å¡«å……ï¼‰
        :return: logits ç”¨äºä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, vocab_size)ï¼Œ
        """
        # âœ… è°ƒè¯•ï¼šæ£€æŸ¥ input_ids èŒƒå›´[0,vocab_size-1]
        max_id = input_ids.max().item()
        min_id = input_ids.min().item()
        if max_id >= self.vocab_size or min_id < 0:
            raise ValueError(
                f"âŒ input_ids è¶…å‡ºèŒƒå›´! å…è®¸èŒƒå›´: [0, {self.vocab_size - 1}], å®é™…èŒƒå›´: [{min_id}, {max_id}]")

        # 1. è¾“å…¥åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.embed(input_ids)  # (batch, seq, d_model)  #tokenåµŒå…¥
        seq_len = x.size(1)
        # âœ… åŠ¨æ€æˆªæ–­ï¼šå¦‚æœ seq_len è¶…è¿‡ max_seq_lenï¼Œåˆ™æˆªæ–­
        if seq_len > self.max_seq_len:
            print(f"âš ï¸  è¾“å…¥åºåˆ—è¿‡é•¿ ({seq_len} > {self.max_seq_len})ï¼Œè‡ªåŠ¨æˆªæ–­ï¼")
            x = x[:, :self.max_seq_len, :]  # æˆªæ–­è¾“å…¥
            if attention_mask is not None:
                attention_mask = attention_mask[:, :self.max_seq_len]  # åŒæ­¥æˆªæ–­ attention_mask
            seq_len = self.max_seq_len
        # 2.ä½ç½®ç¼–ç å¹¿æ’­ã€‚æˆªå–ä½ç½®ç¼–ç å¹¶å¹¿æ’­åŠ åˆ°x
        pos_enc = self.pos_embed[:, :seq_len, :]  # (1, seq_len, d_model)
        x = x + pos_enc  # å¹¿æ’­åˆ° (batch, seq_len, d_model)

        # 3. æ³¨æ„åŠ›éªŒç å¤„ç†ã€‚å¦‚æœæä¾›äº† attention_maskï¼Œç”Ÿæˆ Transformer éœ€è¦çš„ key_padding_mask
        key_padding_mask = None
        if attention_mask is not None:
            # TransformerEncoder éœ€è¦çš„æ˜¯ key_padding_mask: (batch, seq_len)
            # True è¡¨ç¤ºè¦ mask æ‰çš„ä½ç½®ï¼Œç¬¦åˆTransformerEncoderè¦æ±‚
            key_padding_mask = (attention_mask == 0)

        # 4. å¦‚æœæŒ‡å®šäº†ä»»åŠ¡ï¼Œæ·»åŠ ä»»åŠ¡åµŒå…¥
        if task_id is not None:
            # ç¡®ä¿ task_id æ˜¯ LongTensor å¹¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if not isinstance(task_id, torch.Tensor):
                task_id = torch.tensor(task_id, dtype=torch.long, device=input_ids.device)
            if task_id.dim() == 1:
                task_emb = self.task_embed(task_id).unsqueeze(1)  # (batch, 1, d_model)
                x = x + task_emb  # å¹¿æ’­åˆ°æ‰€æœ‰ tokenï¼ˆç›¸å½“äºåœ¨åºåˆ—å¼€å¤´æ³¨å…¥ä»»åŠ¡ä¿¡æ¯ï¼‰

        # 5. Transformer ç¼–ç .å¤šå¤´è‡ªæ³¨æ„åŠ›+FFNï¼Œå¤„ç†åºåˆ—ä¾èµ–
        x = self.encoder(
            x,
            src_key_padding_mask=key_padding_mask  # ğŸ‘ˆ ä¼ é€’ key_padding_mask
        )

        # 5. é¢„æµ‹ logitsï¼šçº¿æ€§æŠ•å½±åˆ°è¯æ±‡è¡¨
        logits = self.lm_head(x)
        # è¿”å›å­—å…¸ï¼Œä¾¿äºåç»§æŸå¤±è®¡ç®—ï¼Œå¦‚CrossEntropyLoss
        return {"logits": logits}

    def num_parameters(self, verbose=False):
        """
        è¿”å›å¯è®­ç»ƒå‚æ•°é‡ï¼Œè‡ªåŠ¨å¤„ç†æƒé‡å…±äº«ï¼ˆå¦‚ embed å’Œ lm_head tied æ—¶ä¸é‡å¤è®¡ç®—ï¼‰
        :param verbose: å¦‚æœ Trueï¼Œæ‰“å°å„ç»„ä»¶å‚æ•° breakdown
        """
        total = 0
        seen_data_ptr = set()  # è·Ÿè¸ªå·²è®¡å…¥çš„å­˜å‚¨æŒ‡é’ˆï¼Œé¿å…é‡å¤
        breakdown = {}  # ç”¨äº verbose çš„ç»„ä»¶ç»Ÿè®¡

        for name, p in self.named_parameters():
            if p.requires_grad:
                data_ptr = p.data_ptr()
                if data_ptr not in seen_data_ptr:
                    seen_data_ptr.add(data_ptr)
                    total += p.numel()

                    # åˆ†ç±» breakdownï¼ˆå¯é€‰ï¼ŒåŸºäºåç§°ï¼‰
                    if 'embed' in name:
                        breakdown['Embed'] = p.numel()
                    elif 'lm_head' in name:
                        breakdown['LM Head'] = p.numel()  # å³ä½¿å…±äº«ï¼Œä¹Ÿè®°å½•åŸå§‹å¤§å°
                    elif 'encoder' in name:
                        breakdown['Encoder'] = p.numel() if 'Encoder' not in breakdown else breakdown[
                                                                                                'Encoder'] + p.numel()
                    elif 'task_embed' in name:
                        breakdown['Task Embed'] = p.numel()
                    elif 'pos_embed' in name:
                        breakdown['Pos Embed'] = p.numel()
                    else:
                        # å…¶ä»–å‚æ•°
                        key = name.split('.')[0]
                        breakdown[key] = breakdown.get(key, 0) + p.numel()

        if verbose:
            print("å‚æ•° breakdown:")
            for comp, params in breakdown.items():
                print(f"  {comp}: {params / 1e6:.1f}M")
            print(f"æ€»å‚æ•°é‡: {total / 1e6:.1f}M (å…±äº«å·²å»é‡)")

        return total


# -----------------------------
# æµ‹è¯•ä»£ç ï¼ˆéªŒè¯æ¨¡å‹æ˜¯å¦å¯è¿è¡Œï¼‰
# -----------------------------
if __name__ == "__main__":
    model = TinyTransformer()
    print(f"âœ… æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e6:.1f}M")

    # ä»¿çœŸè¾“å…¥
    input_ids = torch.randint(0, 32000, (2, 32))
    attention_mask = torch.ones_like(input_ids)  # å…¨1ï¼Œæ— mask
    task_id = torch.tensor([0, 1])  # ä»»åŠ¡0å’Œ1æ··åˆbatch

    outputs = model(input_ids, task_id=task_id, attention_mask=attention_mask)
    print(f"âœ… å‰å‘è®¡ç®—é€šè¿‡ï¼Logitså½¢çŠ¶: {outputs['logits'].shape}")
