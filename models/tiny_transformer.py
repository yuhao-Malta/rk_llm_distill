# models/tiny_transformer.py
import os
import json
import torch
import torch.nn as nn
from transformers import AutoTokenizer
from torch.quantization import QuantStub, DeQuantStub

class TinyTransformer(nn.Module):
    """
    è½»é‡çº§ Transformer å­¦ç”Ÿæ¨¡å‹ï¼ˆ174M å‚æ•°ï¼Œä¼˜åŒ–åç›®æ ‡ï¼š~40Må‚æ•°ï¼Œé€‚é…1126B NPUï¼‰
    æ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ï¼ˆä¸­â†’è‹±ã€è‹±â†’ä¸­ï¼‰ + attention_maskæ¥å¤„ç†åºåˆ—å¡«å……
    æ¨¡å‹ä½¿ç”¨QWen tokenizerï¼ˆä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼Œä»¥æ”¯æŒç¦»çº¿ä½¿ç”¨ï¼‰
    åŸºäºPyTorchçš„nn.TransformerEncoderæ„å»ºç¼–ç å™¨éƒ¨åˆ†
    ç«¯ä¾§ä¼˜åŒ–ï¼šPre-normã€FP16/INT8ã€åºåˆ—é•¿åº¦ 64ã€ä½ batch æ¨ç†
    """
    def __init__(self, d_model=128, nhead=4, num_layers=2, max_seq_len=64, share_weights=True, vocab_size=None):
        """
        :param d_model: éšè—ç»´åº¦ï¼ˆ512â†’128ï¼Œå‡å°å‚æ•°é‡ï¼‰
        :param nhead: æ³¨æ„åŠ›å¤´æ•°ï¼ˆ8â†’4ï¼Œé€‚é… d_modelï¼‰
        :param num_layers: Transformer å±‚æ•°ï¼ˆ6â†’2ï¼Œå‡å°‘å‚æ•°ï¼‰
        :param max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆ64ï¼Œé€‚åˆçŸ­åºåˆ—ï¼‰
        :param share_weights: æ˜¯å¦å…±äº« embed å’Œ lm_head æƒé‡ï¼ˆå‡å‚æ•°ï¼‰
        :param vocab_size: è¯æ±‡è¡¨å¤§å°ï¼Œä» config.json è¯»å–
        """
        super().__init__()
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        self.share_weights = share_weights

        # âœ… æŒ‡å®šæœ¬åœ° tokenizer è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼Œè®¡ç®—é¡¹ç›®æ ¹ç›®å½•å¹¶æ‹¼æ¥æœ¬åœ°è·¯å¾„
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        TOKENIZER_PATH = os.path.join(PROJECT_ROOT, "models", "qwen_tokenizer_offline", "qwen", "Qwen1___5-1___8B")

        # âœ… æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(TOKENIZER_PATH):
            raise FileNotFoundError(f"âŒ æœ¬åœ° tokenizer è·¯å¾„ä¸å­˜åœ¨: {TOKENIZER_PATH}")

        # âœ… ä½¿ç”¨AutoTokenizer.from_pretrainedä»æœ¬åœ°è·¯å¾„åŠ è½½ tokenizerã€‚ä¸QWenæ•™å¸ˆæ¨¡å‹å…±äº«tokenizerï¼Œä¾¿äºè’¸é¦æ—¶tokenå¯¹é½
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                TOKENIZER_PATH, local_files_only=True, trust_remote_code=True
            )
            print("âœ… æˆåŠŸç¦»çº¿åŠ è½½æœ¬åœ° Qwen Tokenizerï¼")
        except Exception as e:
            print(f"âŒ ç¦»çº¿åŠ è½½æœ¬åœ° tokenizer å¤±è´¥: {e}")
            raise RuntimeError("æ— æ³•åŠ è½½æœ¬åœ° Qwen Tokenizerï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼")

        # ä» config.json è¯»å– vocab_size
        config_path = os.path.join(TOKENIZER_PATH, "config.json")
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            self.vocab_size = config.get("vocab_size", len(self.tokenizer))
            print(f"âœ… ä½¿ç”¨ config.json çš„è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        except Exception as e:
            print(f"âš ï¸ è¯»å– config.json å¤±è´¥: {e}ï¼Œä½¿ç”¨ tokenizer è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
            self.vocab_size = len(self.tokenizer)
        print(f"âœ… Qwen Tokenizer è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")  # åº”è¾“å‡º 151963 æˆ–ç±»ä¼¼

        # âœ… ä½¿ç”¨çœŸå®è¯æ±‡è¡¨å¤§å°åˆå§‹åŒ– Embeddingï¼Œè¾“å…¥tokenåˆ°d_modelç»´åµŒå…¥
        self.embed = nn.Embedding(self.vocab_size, d_model)  # å‚æ•°é‡ï¼š151643*512=77.6M å‚æ•°
        # å¯å­¦ä¹ ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, d_model))
        # ä»»åŠ¡æ ‡è¯†åµŒå…¥ï¼ˆ0=ä¸­â†’è‹±, 1=è‹±â†’ä¸­ï¼‰
        self.task_embed = nn.Embedding(2, d_model)  # 2*512=1K å‚æ•°

        # è¾“å‡ºå¤´
        if share_weights:
            self.lm_head = self.embed
        else:
            self.lm_head = nn.Linear(d_model, self.vocab_size, bias=False)

        # Transformer ç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=512,  # 2048â†’512ï¼Œå‡å°‘ FFN å‚æ•°
            dropout=0.1,
            batch_first=True,
            norm_first=True  # ç«¯ä¾§ä¼˜åŒ–ï¼šPre-normæé«˜è®­ç»ƒç¨³å®šæ€§å’ŒåŠ é€ŸNPUæ¨ç†ï¼ˆ~10%å»¶è¿Ÿå‡å°‘ï¼‰
        )
        self.encoder = nn.TransformerEncoder(
            encoder_layer, num_layers, enable_nested_tensor=False
        )

        # é‡åŒ–æ”¯æŒ
        self.quant = QuantStub()
        self.dequant = DeQuantStub()

        self.init_weights()

    def init_weights(self):
        """
        åˆå§‹åŒ–ï¼šnormal(std=0.02) for embed, lm_headï¼ˆè‹¥ä¸å…±äº«ï¼‰, pos_embed
        ç«¯ä¾§å…¼å®¹ï¼šç¡®ä¿é‡åŒ–ï¼ˆINT8ï¼‰åæƒé‡åˆ†å¸ƒç¨³å®š
        """
        nn.init.normal_(self.embed.weight, std=0.02)
        if not self.share_weights:
            nn.init.normal_(self.lm_head.weight, std=0.02)  # ç‹¬ç«‹åˆå§‹åŒ–lm_head
        if self.pos_embed is not None:
            nn.init.normal_(self.pos_embed, std=0.02)
        nn.init.normal_(self.task_embed.weight, std=0.02)

    def forward(self, input_ids, task_id=None, attention_mask=None):
        # ç¡®ä¿ input_ids æ˜¯ LongTensor
        input_ids = input_ids.to(dtype=torch.long)
        # é‡åŒ–è¾“å…¥ï¼ˆä»…åœ¨æ¨ç†æ—¶ç”Ÿæ•ˆï¼Œè®­ç»ƒæ—¶ QuantStub æ˜¯ç©ºæ“ä½œï¼‰
        x = self.quant(input_ids)
        x = self.embed(x) * (self.d_model ** 0.5)
        seq_len = x.size(1)
        # âœ… åŠ¨æ€æˆªæ–­ï¼šå¦‚æœ seq_len è¶…è¿‡ max_seq_lenï¼Œåˆ™æˆªæ–­
        if seq_len > self.max_seq_len:
            print(f"âš ï¸  è¾“å…¥åºåˆ—è¿‡é•¿ ({seq_len} > {self.max_seq_len})ï¼Œè‡ªåŠ¨æˆªæ–­ï¼")
            x = x[:, :self.max_seq_len, :]  # æˆªæ–­è¾“å…¥
            if attention_mask is not None:
                attention_mask = attention_mask[:, :self.max_seq_len]  # åŒæ­¥æˆªæ–­ attention_mask
            seq_len = self.max_seq_len
        # 2.ä½ç½®ç¼–ç å¹¿æ’­ã€‚æˆªå–ä½ç½®ç¼–ç å¹¶å¹¿æ’­åŠ åˆ°x
        x = x + self.pos_embed[:, :seq_len, :]  # å¹¿æ’­åˆ° (batch, seq_len, d_model)

        # 3. å¦‚æœæŒ‡å®šäº†ä»»åŠ¡ï¼Œæ·»åŠ ä»»åŠ¡åµŒå…¥
        if task_id is not None:
            # ç¡®ä¿ task_id æ˜¯ LongTensor å¹¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if not isinstance(task_id, torch.Tensor):
                task_id = torch.tensor(task_id, dtype=torch.long, device=input_ids.device)
            if task_id.dim() == 1:
                task_emb = self.task_embed(task_id).unsqueeze(1)  # (batch, 1, d_model)
                x = x + task_emb  # å¹¿æ’­åˆ°æ‰€æœ‰ tokenï¼ˆç›¸å½“äºåœ¨åºåˆ—å¼€å¤´æ³¨å…¥ä»»åŠ¡ä¿¡æ¯ï¼‰

        # 4. æ³¨æ„åŠ›éªŒç å¤„ç†ã€‚å¦‚æœæä¾›äº† attention_maskï¼Œç”Ÿæˆ Transformer éœ€è¦çš„ key_padding_mask
        key_padding_mask = None
        if attention_mask is not None:
            # TransformerEncoder éœ€è¦çš„æ˜¯ key_padding_mask: (batch, seq_len)
            # True è¡¨ç¤ºè¦ mask æ‰çš„ä½ç½®ï¼Œç¬¦åˆTransformerEncoderè¦æ±‚
            key_padding_mask = (attention_mask == 0)

        # 5. Transformer ç¼–ç .å¤šå¤´è‡ªæ³¨æ„åŠ›+FFNï¼Œå¤„ç†åºåˆ—ä¾èµ–(ç¡®ä¿LayerNormè¾“å…¥å’Œè¾“å‡ºå‚æ•°ä¸ºfloat32)
        x = x.to(torch.float32)  # TransformerEncoderè¦æ±‚.å¼ºåˆ¶float32ï¼Œå…¼å®¹CPU LayerNorm
        x = self.encoder(
            x,
            src_key_padding_mask=key_padding_mask  # ğŸ‘ˆ ä¼ é€’ key_padding_mask
        )
        # è§£é‡åŒ–å¹¶ç”Ÿæˆ logits
        x = self.dequant(x)
        logits = self.lm_head(x)
        # è¿”å›å­—å…¸ï¼Œä¾¿äºåç»§æŸå¤±è®¡ç®—ï¼Œå¦‚CrossEntropyLoss
        return {"logits": logits}

    def num_parameters(self, verbose=False):
        """
        å‚æ•°é‡è®¡ç®—ï¼Œæ”¯æŒå…±äº«/ä¸å…±äº«embedå’Œlm_headæƒé‡
        ä¿®å¤Embedæ˜¾ç¤º0.0Mé—®é¢˜
        """
        total = 0
        breakdown = {
            'Embed' + (' (shared with LM Head)' if self.share_weights else ''): 0,
            'LM Head' + (' (shared)' if self.share_weights else ''): 0,
            'Encoder': 0,
            'Task Embed': 0,
            'Pos Embed': 0
        }

        for name, p in self.named_parameters():
            if p.requires_grad:
                param_count = p.numel()
                total += param_count
                if name.startswith('embed.'):
                    breakdown['Embed' + (' (shared with LM Head)' if self.share_weights else '')] = param_count
                elif name.startswith('lm_head.') and not self.share_weights:
                    breakdown['LM Head'] = param_count
                elif 'encoder' in name:
                    breakdown['Encoder'] += param_count
                elif name.startswith('task_embed.'):
                    breakdown['Task Embed'] = param_count
                elif name.startswith('pos_embed'):
                    breakdown['Pos Embed'] = param_count

        if verbose:
            print("| Component | Parameters (M) |")
            print("|-----------|----------------|")
            for comp, params in breakdown.items():
                print(f"| {comp} | {params / 1e6:.1f} |")
            print(f"| **Total** | **{total / 1e6:.1f}** |")

        return total


# -----------------------------
# æµ‹è¯•ä»£ç ï¼ˆéªŒè¯æ¨¡å‹æ˜¯å¦å¯è¿è¡Œï¼‰
# -----------------------------
if __name__ == "__main__":
    # æµ‹è¯•ä¸å…±äº«æƒé‡ï¼ˆ174.2Mï¼‰
    model = TinyTransformer(share_weights=False)
    print(f"âœ… æ¨¡å‹å‚æ•°é‡ï¼ˆä¸å…±äº«æƒé‡ï¼‰: {model.num_parameters(verbose=True) / 1e6:.1f}M")

    input_ids = torch.randint(0, 32000, (2, 32))  # LongTensor
    attention_mask = torch.ones_like(input_ids, dtype=torch.float32)  # float32 for mask
    task_id = torch.tensor([0, 1], dtype=torch.long)

    with torch.no_grad():  # ç«¯ä¾§æ¨ç†æ¨¡æ‹Ÿ
        outputs = model(input_ids, task_id=task_id, attention_mask=attention_mask)
    print(f"âœ… å‰å‘è®¡ç®—é€šè¿‡ï¼Logitså½¢çŠ¶: {outputs['logits'].shape}")

    # æµ‹è¯•å…±äº«æƒé‡ï¼ˆ96.6Mï¼‰
    model_shared = TinyTransformer(share_weights=True)
    print(f"âœ… æ¨¡å‹å‚æ•°é‡ï¼ˆå…±äº«æƒé‡ï¼‰: {model_shared.num_parameters(verbose=True) / 1e6:.1f}M")
