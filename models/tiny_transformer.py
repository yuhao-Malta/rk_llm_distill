# models/tiny_transformer.py
import os
import sys
import json
import torch
import torch.nn as nn
from transformers import AutoTokenizer
from torch.quantization import QuantStub, DeQuantStub

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import ModelConfig, MODEL_PATH


class TinyTransformer(nn.Module):
    """
    è½»é‡çº§ Transformer å­¦ç”Ÿæ¨¡å‹ï¼ˆ174M å‚æ•°ï¼Œä¼˜åŒ–åç›®æ ‡ï¼š~40Må‚æ•°ï¼Œé€‚é…1126B NPUï¼‰
    ä¸»è¦æ”¹è¿›ï¼š
    1. âœ… ä¿®å¤å…±äº«æƒé‡çš„æ¢¯åº¦æ›´æ–°é—®é¢˜
    2. âœ… ç»Ÿä¸€ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†å‚æ•°
    3. âœ… æ”¹è¿›å‚æ•°åˆå§‹åŒ–ç­–ç•¥
    4. âœ… å¢å¼ºé”™è¯¯å¤„ç†
    æ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ï¼ˆä¸­â†’è‹±ã€è‹±â†’ä¸­ï¼‰ + attention_maskå¤„ç†åºåˆ—å¡«å……
    æ¨¡å‹ä½¿ç”¨QWen tokenizerï¼ˆä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼Œä»¥æ”¯æŒç¦»çº¿ä½¿ç”¨ï¼‰
    åŸºäºPyTorchçš„nn.TransformerEncoderæ„å»ºç¼–ç å™¨éƒ¨åˆ†
    ç«¯ä¾§ä¼˜åŒ–ï¼šPre-normã€FP16/INT8ã€åºåˆ—é•¿åº¦ 64ã€ä½ batch æ¨ç†
    """

    def __init__(
            self,
            d_model=None,
            nhead=None,
            num_layers=None,
            max_seq_len=None,
            share_weights=None,
            vocab_size=None
    ):
        """
        å‚æ•°ä¼˜å…ˆçº§ï¼šä¼ å…¥å‚æ•° > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼

        :param d_model: éšè—ç»´åº¦
        :param nhead: æ³¨æ„åŠ›å¤´æ•°
        :param num_layers: Transformerå±‚æ•°
        :param max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        :param share_weights: æ˜¯å¦å…±äº«embedå’Œlm_headæƒé‡
        :param vocab_size: è¯æ±‡è¡¨å¤§å°
        """
        super().__init__()

        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å€¼
        config = ModelConfig.CURRENT_CONFIG
        self.d_model = d_model or config["d_model"]
        self.nhead = nhead or config["nhead"]
        self.num_layers = num_layers or config["num_layers"]
        self.max_seq_len = max_seq_len or ModelConfig.MAX_SEQ_LEN
        self.share_weights = share_weights if share_weights is not None else config["share_weights"]
        self.vocab_size = vocab_size or ModelConfig.VOCAB_SIZE

        print(f"âœ… åˆå§‹åŒ– TinyTransformer: d_model={self.d_model}, nhead={self.nhead}, "
              f"num_layers={self.num_layers}, share_weights={self.share_weights}")

        # æ£€æŸ¥è·¯å¾„
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"âŒ Tokenizerè·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")

        # åŠ è½½ tokenizer
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                MODEL_PATH, local_files_only=True, trust_remote_code=True
            )
            print("âœ… æˆåŠŸç¦»çº¿åŠ è½½ Qwen Tokenizer")
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ Tokenizer å¤±è´¥: {e}")

        # éªŒè¯è¯æ±‡è¡¨å¤§å°
        actual_vocab_size = len(self.tokenizer)
        if self.vocab_size != actual_vocab_size:
            print(f"âš ï¸ è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: é…ç½®={self.vocab_size}, å®é™…={actual_vocab_size}")
            self.vocab_size = actual_vocab_size
            ModelConfig.VOCAB_SIZE = actual_vocab_size

        # ==================== æ¨¡å‹å±‚å®šä¹‰ ====================
        # 1. Token åµŒå…¥å±‚
        self.embed = nn.Embedding(self.vocab_size, self.d_model)

        # 2. ä½ç½®ç¼–ç  (å¯å­¦ä¹ )
        self.pos_embed = nn.Parameter(torch.zeros(1, self.max_seq_len, self.d_model))

        # 3. ä»»åŠ¡åµŒå…¥ (0=ä¸­â†’è‹±, 1=è‹±â†’ä¸­)
        self.task_embed = nn.Embedding(2, self.d_model)

        # 4. è¾“å‡ºå±‚ (LM Head)
        if self.share_weights:
            # âœ… ä¿®å¤ï¼šå…±äº«æƒé‡ä½†ä¿æŒç‹¬ç«‹æ¨¡å—
            self.lm_head = nn.Linear(self.d_model, self.vocab_size, bias=False)
            self.lm_head.weight = self.embed.weight  # å…±äº«å‚æ•°å¼•ç”¨
            print("âœ… ä½¿ç”¨æƒé‡å…±äº«æ¨¡å¼ (embed.weight = lm_head.weight)")
        else:
            self.lm_head = nn.Linear(self.d_model, self.vocab_size, bias=False)
            print("âœ… ä½¿ç”¨ç‹¬ç«‹æƒé‡æ¨¡å¼")

        # 5. Transformer ç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=self.nhead,
            dim_feedforward=self.d_model * 2,  # é€šå¸¸æ˜¯ d_model çš„ 2-4 å€
            dropout=0.1,
            batch_first=True,
            norm_first=True  # ç«¯ä¾§ä¼˜åŒ–ï¼šPre-normæé«˜è®­ç»ƒç¨³å®šæ€§å’ŒåŠ é€ŸNPUæ¨ç†ï¼ˆ~10%å»¶è¿Ÿå‡å°‘ï¼‰
        )
        self.encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers,
            enable_nested_tensor=False
        )

        # 6. é‡åŒ–æ”¯æŒ
        self.quant = QuantStub()
        self.dequant = DeQuantStub()

        # 7. åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        """
        æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ç­–ç•¥
        ä½¿ç”¨ Xavier/Kaiming åˆå§‹åŒ– + å°æ ‡å‡†å·®
        """
        # Embedding å±‚
        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)

        # LM Head (å¦‚æœä¸å…±äº«æƒé‡)
        if not self.share_weights:
            nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.02)

        # ä½ç½®ç¼–ç 
        nn.init.normal_(self.pos_embed, mean=0.0, std=0.02)

        # ä»»åŠ¡åµŒå…¥
        nn.init.normal_(self.task_embed.weight, mean=0.0, std=0.02)

        print("âœ… æƒé‡åˆå§‹åŒ–å®Œæˆ (normal, std=0.02)")

    def forward(self, input_ids, task_id=None, attention_mask=None):
        """
        å‰å‘ä¼ æ’­

        :param input_ids: [batch, seq_len] - è¾“å…¥token IDs
        :param task_id: [batch] æˆ– scalar - ä»»åŠ¡ID (0=ä¸­â†’è‹±, 1=è‹±â†’ä¸­)
        :param attention_mask: [batch, seq_len] - attention mask (1=æœ‰æ•ˆ, 0=padding)
        :return: {"logits": [batch, seq_len, vocab_size]}
        """
        # 1. ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
        input_ids = input_ids.to(dtype=torch.long)
        batch_size, seq_len = input_ids.size()

        # 2. åŠ¨æ€æˆªæ–­ (å¦‚æœè¶…è¿‡ max_seq_len)
        if seq_len > self.max_seq_len:
            print(f"âš ï¸ è¾“å…¥åºåˆ—è¿‡é•¿ ({seq_len} > {self.max_seq_len})ï¼Œè‡ªåŠ¨æˆªæ–­")
            input_ids = input_ids[:, :self.max_seq_len]
            if attention_mask is not None:
                attention_mask = attention_mask[:, :self.max_seq_len]  # åŒæ­¥æˆªæ–­ attention_mask
            seq_len = self.max_seq_len

        # 3. Token åµŒå…¥ + ç¼©æ”¾
        x = self.quant(input_ids)  # é‡åŒ–æ”¯æŒ (ä»…æ¨ç†æ—¶ç”Ÿæ•ˆ)
        x = self.embed(x) * (self.d_model ** 0.5)  # ç¼©æ”¾å› å­

        # 4. æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed[:, :seq_len, :]

        # 5. æ·»åŠ ä»»åŠ¡åµŒå…¥ (å¦‚æœæŒ‡å®š)
        if task_id is not None:
            # ç¡®ä¿ task_id æ˜¯ LongTensor å¹¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if not isinstance(task_id, torch.Tensor):
                task_id = torch.tensor(task_id, dtype=torch.long, device=input_ids.device)
            if task_id.dim() == 0:  # scalar
                task_id = task_id.unsqueeze(0).expand(batch_size)
            task_emb = self.task_embed(task_id).unsqueeze(1)  # [batch, 1, d_model]
            x = x + task_emb  # å¹¿æ’­åˆ°æ‰€æœ‰ token

        # 6. ç”Ÿæˆ key_padding_mask (Transformeréœ€è¦ True=mask)
        key_padding_mask = None
        if attention_mask is not None:
            # TransformerEncoder éœ€è¦çš„æ˜¯ key_padding_mask: (batch, seq_len)
            # True è¡¨ç¤ºè¦ mask æ‰çš„ä½ç½®ï¼Œç¬¦åˆTransformerEncoderè¦æ±‚
            key_padding_mask = (attention_mask == 0)

        # 7. Transformer ç¼–ç  (å¼ºåˆ¶ float32ï¼Œå…¼å®¹CPU LayerNorm)
        x = x.to(torch.float32)
        x = self.encoder(x, src_key_padding_mask=key_padding_mask)

        # 8. LM Head ç”Ÿæˆ logits
        x = self.dequant(x)
        logits = self.lm_head(x)
        # è¿”å›å­—å…¸ï¼Œä¾¿äºåç»§æŸå¤±è®¡ç®—ï¼Œå¦‚CrossEntropyLoss
        return {"logits": logits}

    def num_parameters(self, verbose=False):
        """
        å‚æ•°é‡è®¡ç®—ï¼Œæ”¯æŒå…±äº«/ä¸å…±äº«embedå’Œlm_headæƒé‡
        ä¿®å¤Embedæ˜¾ç¤º0.0Mé—®é¢˜
        """
        total = 0
        breakdown = {
            'Embedding': 0,
            'LM Head': 0,
            'Encoder': 0,
            'Task Embed': 0,
            'Pos Embed': 0
        }

        for name, p in self.named_parameters():
            if p.requires_grad:
                param_count = p.numel()
                total += param_count

                # é¿å…é‡å¤è®¡æ•°å…±äº«æƒé‡
                if name.startswith('embed.') and not (self.share_weights and 'lm_head' in breakdown):
                    breakdown['Embedding'] += param_count
                elif name.startswith('lm_head.') and not self.share_weights:
                    breakdown['LM Head'] += param_count
                elif 'encoder' in name:
                    breakdown['Encoder'] += param_count
                elif name.startswith('task_embed.'):
                    breakdown['Task Embed'] += param_count
                elif name.startswith('pos_embed'):
                    breakdown['Pos Embed'] += param_count

        # å…±äº«æƒé‡åªè®¡æ•°ä¸€æ¬¡
        if self.share_weights:
            breakdown['LM Head'] = 0
            breakdown['Embedding (shared with LM Head)'] = breakdown.pop('Embedding')

        if verbose:
            print("\n" + "=" * 50)
            print("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡")
            print("=" * 50)
            for comp, params in breakdown.items():
                if params > 0:
                    print(f"  {comp:<30} {params / 1e6:>8.2f}M")
            print("-" * 50)
            print(f"  {'æ€»è®¡':<30} {total / 1e6:>8.2f}M")
            print("=" * 50)

        return total


# -----------------------------
# æµ‹è¯•ä»£ç ï¼ˆéªŒè¯æ¨¡å‹æ˜¯å¦å¯è¿è¡Œï¼‰
# -----------------------------
if __name__ == "__main__":
    print("\nğŸ§ª æµ‹è¯• TinyTransformer (ä¿®å¤ç‰ˆ)\n")

    # æµ‹è¯•1: ä¸å…±äº«æƒé‡
    print("æµ‹è¯•1: ä¸å…±äº«æƒé‡æ¨¡å¼")
    model1 = TinyTransformer(share_weights=False)
    params1 = model1.num_parameters(verbose=True)

    # æµ‹è¯•2: å…±äº«æƒé‡ (é»˜è®¤)
    print("\næµ‹è¯•2: å…±äº«æƒé‡æ¨¡å¼")
    model2 = TinyTransformer(share_weights=True)
    params2 = model2.num_parameters(verbose=True)

    # æµ‹è¯•3: å‰å‘ä¼ æ’­
    print("\næµ‹è¯•3: å‰å‘ä¼ æ’­")
    batch_size = 2
    seq_len = 32
    input_ids = torch.randint(0, ModelConfig.VOCAB_SIZE, (batch_size, seq_len))
    attention_mask = torch.ones_like(input_ids, dtype=torch.long)
    task_id = torch.tensor([0, 1], dtype=torch.long)

    with torch.no_grad():
        outputs = model2(input_ids, task_id=task_id, attention_mask=attention_mask)

    print(f"âœ… å‰å‘è®¡ç®—æˆåŠŸï¼")
    print(f"  è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    print(f"  è¾“å‡ºå½¢çŠ¶: {outputs['logits'].shape}")
    print(f"  é¢„æœŸå½¢çŠ¶: ({batch_size}, {seq_len}, {ModelConfig.VOCAB_SIZE})")

    assert outputs['logits'].shape == (batch_size, seq_len, ModelConfig.VOCAB_SIZE)
    # assert outputs['logits'].shape == (batch_size, seq_len, 151646)
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
